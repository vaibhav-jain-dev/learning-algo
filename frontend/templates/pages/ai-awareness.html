<style>
    .ai-tabs {
        display: flex;
        flex-wrap: wrap;
        gap: 8px;
        margin-bottom: 24px;
        border-bottom: 2px solid var(--border-color, #e2e8f0);
        padding-bottom: 12px;
    }
    .ai-tab-btn {
        background: var(--bg-secondary, #f8fafc);
        color: var(--text-secondary, #64748b);
        border: 1px solid var(--border-color, #e2e8f0);
        padding: 10px 18px;
        border-radius: 8px 8px 0 0;
        cursor: pointer;
        font-size: 0.9rem;
        font-weight: 500;
        transition: all 0.2s ease;
    }
    .ai-tab-btn:hover {
        background: var(--bg-tertiary, #f1f5f9);
        color: var(--text-primary, #0f172a);
    }
    .ai-tab-btn.active {
        background: var(--accent-primary, #3b82f6);
        color: white;
        border-color: var(--accent-primary, #3b82f6);
    }
    .ai-tab-content {
        display: none;
        animation: fadeIn 0.3s ease;
    }
    .ai-tab-content.active {
        display: block;
    }
    @keyframes fadeIn {
        from { opacity: 0; transform: translateY(10px); }
        to { opacity: 1; transform: translateY(0); }
    }
    .ai-section {
        background: var(--bg-secondary, #f8fafc);
        padding: 24px;
        border-radius: 12px;
        margin-bottom: 24px;
        border: 1px solid var(--border-color, #e2e8f0);
    }
    .ai-section h2 {
        font-size: 1.6rem;
        margin-bottom: 12px;
        color: var(--accent-primary, #3b82f6);
        display: flex;
        align-items: center;
        gap: 10px;
    }
    .ai-section h3 {
        font-size: 1.2rem;
        margin-top: 20px;
        margin-bottom: 12px;
        color: var(--accent-secondary, #8b5cf6);
    }
    .ai-badge {
        display: inline-block;
        padding: 4px 12px;
        border-radius: 20px;
        font-size: 0.8rem;
        font-weight: 600;
        margin-right: 8px;
        margin-bottom: 8px;
    }
    .ai-badge-backend {
        background: rgba(59, 130, 246, 0.15);
        color: #3b82f6;
        border: 1px solid #3b82f6;
    }
    .ai-badge-heavy {
        background: rgba(239, 68, 68, 0.15);
        color: #ef4444;
        border: 1px solid #ef4444;
    }
    .ai-badge-critical {
        background: rgba(245, 158, 11, 0.15);
        color: #f59e0b;
        border: 1px solid #f59e0b;
    }
    .work-item {
        background: var(--bg-tertiary, #f1f5f9);
        padding: 16px;
        border-radius: 8px;
        margin-bottom: 12px;
        border-left: 4px solid var(--accent-primary, #3b82f6);
    }
    .work-item h4 {
        color: var(--text-primary, #0f172a);
        margin-bottom: 8px;
        font-size: 1.1rem;
    }
    .work-item p {
        color: var(--text-secondary, #64748b);
        font-size: 0.95rem;
        margin-bottom: 10px;
    }
    .tech-stack {
        display: flex;
        flex-wrap: wrap;
        gap: 8px;
        margin-top: 10px;
    }
    .tech-tag {
        background: rgba(139, 92, 246, 0.12);
        color: var(--accent-secondary, #8b5cf6);
        padding: 5px 12px;
        border-radius: 6px;
        font-size: 0.85rem;
        border: 1px solid rgba(139, 92, 246, 0.25);
    }
    .ai-highlight {
        background: rgba(59, 130, 246, 0.08);
        padding: 16px;
        border-radius: 8px;
        border-left: 4px solid var(--accent-primary, #3b82f6);
        margin: 16px 0;
    }
    .ai-highlight strong {
        color: var(--accent-primary, #3b82f6);
    }
    .real-world-example {
        background: linear-gradient(135deg, rgba(16, 185, 129, 0.08) 0%, rgba(16, 185, 129, 0.04) 100%);
        padding: 16px;
        border-radius: 8px;
        margin: 16px 0;
        border-left: 4px solid #10b981;
    }
    .real-world-example h5 {
        color: #10b981;
        margin-bottom: 8px;
        font-size: 1rem;
    }
    .real-world-example .company-name {
        font-weight: 600;
        color: var(--text-primary, #0f172a);
    }
    .real-world-example .description {
        font-size: 0.9rem;
        color: var(--text-secondary, #64748b);
        margin-top: 5px;
        line-height: 1.6;
    }
    .collapsible-btn {
        background: var(--bg-tertiary, #f1f5f9);
        color: var(--text-primary, #0f172a);
        cursor: pointer;
        padding: 12px;
        width: 100%;
        border: 1px solid var(--border-color, #e2e8f0);
        text-align: left;
        font-size: 0.95rem;
        border-radius: 8px;
        margin-top: 10px;
        font-weight: 500;
        transition: all 0.2s ease;
        display: flex;
        justify-content: space-between;
        align-items: center;
    }
    .collapsible-btn:hover, .collapsible-btn.active {
        background: var(--accent-primary, #3b82f6);
        color: white;
    }
    .collapsible-btn::after {
        content: '\25BC';
        font-size: 0.75rem;
        transition: transform 0.2s ease;
    }
    .collapsible-btn.active::after {
        transform: rotate(180deg);
    }
    .collapsible-content {
        max-height: 0;
        overflow: hidden;
        transition: max-height 0.3s ease;
        background: var(--bg-secondary, #f8fafc);
        border: 1px solid var(--border-color, #e2e8f0);
        border-top: none;
        border-radius: 0 0 8px 8px;
    }
    .collapsible-content .inner {
        padding: 16px;
    }
    .tool-detail {
        background: white;
        padding: 12px;
        border-radius: 6px;
        margin-bottom: 10px;
        border-left: 3px solid var(--accent-secondary, #8b5cf6);
    }
    .tool-detail h6 {
        color: var(--accent-secondary, #8b5cf6);
        margin-bottom: 5px;
        font-size: 0.95rem;
    }
    .tool-detail p {
        color: var(--text-secondary, #64748b);
        font-size: 0.85rem;
        margin-bottom: 0;
    }
    .diagram-container {
        background: var(--bg-tertiary, #f1f5f9);
        padding: 20px;
        border-radius: 10px;
        margin: 20px 0;
        overflow-x: auto;
        min-height: 300px;
        display: flex;
        align-items: center;
        justify-content: center;
    }
    .diagram-container .mermaid {
        width: 100%;
        min-width: 600px;
    }
    /* Responsive diagram sizes */
    @media (max-width: 768px) {
        .diagram-container {
            padding: 15px;
            min-height: 250px;
        }
        .diagram-container .mermaid {
            min-width: 100%;
        }
    }
    @media (max-width: 480px) {
        .diagram-container {
            padding: 10px;
            overflow-x: scroll;
        }
    }
    .ai-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
        gap: 20px;
        margin-top: 20px;
    }
    @media (max-width: 640px) {
        .ai-grid {
            grid-template-columns: 1fr;
        }
    }
    .ai-card {
        background: var(--bg-tertiary, #f1f5f9);
        padding: 20px;
        border-radius: 10px;
        border: 1px solid var(--border-color, #e2e8f0);
        transition: transform 0.2s ease, box-shadow 0.2s ease;
    }
    .ai-card:hover {
        transform: translateY(-4px);
        box-shadow: 0 8px 24px rgba(59, 130, 246, 0.15);
    }
    .ai-card h4 {
        color: var(--accent-primary, #3b82f6);
        margin-bottom: 10px;
        font-size: 1.1rem;
    }
    .mermaid {
        display: flex;
        justify-content: center;
    }
</style>

<div class="page-layout">
    <div class="page-main">
        <h1>AI Components Awareness</h1>
        <p class="text-muted mb-4">Master AI/ML backend systems - from LLMs to RAG pipelines to production deployment</p>

        <div class="card mb-4">
            <h3 class="panel-header">What You'll Learn</h3>
            <ul>
                <li>How LLMs work internally and how to integrate them</li>
                <li>Building RAG systems, vector search, and embeddings pipelines</li>
                <li>Function calling, AI agents, and orchestration patterns</li>
                <li>Production concerns: observability, guardrails, cost optimization</li>
            </ul>
        </div>

        <div class="ai-tabs">
            <button class="ai-tab-btn active" data-tab="llm">LLM APIs</button>
            <button class="ai-tab-btn" data-tab="llm-deep">LLM Deep Dive</button>
            <button class="ai-tab-btn" data-tab="embeddings">Embeddings</button>
            <button class="ai-tab-btn" data-tab="rag">RAG Systems</button>
            <button class="ai-tab-btn" data-tab="vector">Vector Search</button>
            <button class="ai-tab-btn" data-tab="function-calling">Function Calling</button>
            <button class="ai-tab-btn" data-tab="agents">AI Agents</button>
            <button class="ai-tab-btn" data-tab="multimodal">Multimodal</button>
            <button class="ai-tab-btn" data-tab="observability">Observability</button>
            <button class="ai-tab-btn" data-tab="architecture">Architecture</button>
        </div>

        <!-- LLM APIs Tab -->
        <div class="ai-tab-content active" id="llm">
            <div class="ai-section">
                <h2>Large Language Models (LLMs)</h2>
                <span class="ai-badge ai-badge-backend">Backend Heavy</span>
                <span class="ai-badge ai-badge-critical">Production Critical</span>

                <p>LLMs like GPT-4, Claude, and Llama are the foundation. Your job: build everything around them.</p>

                <h3>Request Flow Diagram</h3>
                <div class="diagram-container">
                    <pre class="mermaid">
sequenceDiagram
    participant Client
    participant Backend
    participant RateLimit as Rate Limiter
    participant Cache
    participant LLM as LLM API

    Client->>Backend: POST /chat (prompt)
    Backend->>RateLimit: Check user quota
    RateLimit-->>Backend: Allowed
    Backend->>Cache: Check cache
    Cache-->>Backend: Cache miss
    Backend->>LLM: API call with prompt
    LLM-->>Backend: Stream tokens
    Backend->>Backend: Count tokens
    Backend->>Cache: Store response
    Backend-->>Client: Stream response
    Backend->>Backend: Log cost & metrics
                    </pre>
                </div>

                <button class="collapsible-btn">üìä Understanding This Diagram - Step by Step</button>
                <div class="collapsible-content">
                    <div class="inner">
                        <div class="tool-detail">
                            <h6>Step 1: Client Request</h6>
                            <p><strong>Client ‚Üí Backend:</strong> User sends a POST request to /chat with their prompt (e.g., "Explain quantum computing"). This is a standard HTTP REST call from your frontend.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 2: Rate Limiting</h6>
                            <p><strong>Backend ‚Üí Rate Limiter:</strong> Before processing, check if user has exceeded their quota (e.g., 100 requests/hour). Uses Redis with token bucket algorithm. Returns 429 if exceeded.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 3: Cache Check</h6>
                            <p><strong>Backend ‚Üí Cache:</strong> Check if identical prompt was recently asked. If cache hit, return cached response instantly (saves $$ and time). Cache key = hash(prompt + model + params).</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 4: LLM API Call</h6>
                            <p><strong>Backend ‚Üí LLM API:</strong> On cache miss, call OpenAI/Anthropic API with the prompt. This is an HTTP POST with API key in headers. LLM starts processing.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 5: Streaming Response</h6>
                            <p><strong>LLM ‚Üí Backend:</strong> LLM streams tokens back (Server-Sent Events). Backend receives "Hello", then " world", then "!". You pipe these to the client in real-time via WebSocket/SSE.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 6: Token Counting & Caching</h6>
                            <p><strong>Backend Processing:</strong> Count input tokens (prompt) + output tokens (response) for billing. Store complete response in cache for future identical queries.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 7: Client Streaming</h6>
                            <p><strong>Backend ‚Üí Client:</strong> Stream tokens to user's browser as they arrive. User sees text appearing word-by-word (better UX than waiting for full response).</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 8: Observability</h6>
                            <p><strong>Backend Logging:</strong> Log cost ($0.002 for 200 tokens), latency (850ms), model used (gpt-4), user_id, timestamp. Send to Prometheus/Datadog for dashboards.</p>
                        </div>
                        <div class="ai-highlight">
                            <strong>üí° Key Insight:</strong> The LLM itself is just one box in this flow. You (backend engineer) own 7 out of 8 steps: rate limiting, caching, streaming, token counting, logging, error handling, and client communication. The model is a commodity API - your infrastructure is the differentiator.
                        </div>
                    </div>
                </div>

                <h3>What Backend Devs Own</h3>

                <div class="work-item">
                    <h4>API Integration & Proxying</h4>
                    <p>You expose the model to your application via backend APIs. The model is just a remote service you integrate with.</p>
                    <ul>
                        <li>Build REST/GraphQL endpoints that accept user prompts</li>
                        <li>Call OpenAI/Anthropic/Bedrock APIs from your backend</li>
                        <li>Handle API errors, timeouts, and retries gracefully</li>
                        <li>Implement circuit breakers for model downtime</li>
                    </ul>
                    <div class="tech-stack">
                        <span class="tech-tag">FastAPI</span>
                        <span class="tech-tag">Express.js</span>
                        <span class="tech-tag">Go net/http</span>
                        <span class="tech-tag">AWS API Gateway</span>
                    </div>

                    <div class="real-world-example">
                        <h5>Real-World Example: Haptik (Mumbai)</h5>
                        <p class="company-name">Intelligent Virtual Assistant API Gateway</p>
                        <p class="description">Haptik's backend team built a unified API gateway that proxies requests to multiple LLM providers (GPT-4, Claude, custom models). Their system handles 10M+ conversations/month with automatic fallback routing. When OpenAI has rate limits, their middleware automatically routes to Anthropic with &lt;200ms additional latency.</p>
                    </div>

                    <button class="collapsible-btn">Tool Stack Details</button>
                    <div class="collapsible-content">
                        <div class="inner">
                            <div class="tool-detail">
                                <h6>FastAPI (Python)</h6>
                                <p>Async web framework for building REST endpoints. Handles streaming responses with async/await, automatic OpenAPI docs generation, built-in request validation with Pydantic.</p>
                            </div>
                            <div class="tool-detail">
                                <h6>Express.js (Node.js)</h6>
                                <p>Lightweight web framework for JavaScript/TypeScript backends. Used for building API middleware that intercepts requests, adds authentication headers, logs usage metrics.</p>
                            </div>
                            <div class="tool-detail">
                                <h6>Circuit Breakers (Resilience4j/Polly)</h6>
                                <p>Libraries that prevent cascading failures when LLM APIs are down. Configure failure thresholds (e.g., 50% errors in 10s) to open circuit and return cached responses.</p>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="work-item">
                    <h4>Streaming Token Responses</h4>
                    <p>LLMs stream responses token-by-token. You need to pipe this to clients in real-time.</p>
                    <ul>
                        <li>Implement Server-Sent Events (SSE) or WebSockets</li>
                        <li>Buffer and flush tokens efficiently</li>
                        <li>Handle client disconnections mid-stream</li>
                        <li>Provide fallback to non-streaming for incompatible clients</li>
                    </ul>
                    <div class="tech-stack">
                        <span class="tech-tag">WebSockets</span>
                        <span class="tech-tag">SSE</span>
                        <span class="tech-tag">Redis Streams</span>
                        <span class="tech-tag">Socket.io</span>
                    </div>
                </div>

                <div class="work-item">
                    <h4>Rate Limiting & Throttling</h4>
                    <p>Protect your backend and control costs by limiting requests per user/API key.</p>
                    <ul>
                        <li>Implement token bucket or sliding window algorithms</li>
                        <li>Track usage per user, team, and organization</li>
                        <li>Return proper 429 status codes with retry headers</li>
                        <li>Queue requests during peak load</li>
                    </ul>
                    <div class="tech-stack">
                        <span class="tech-tag">Redis</span>
                        <span class="tech-tag">Kong</span>
                        <span class="tech-tag">AWS WAF</span>
                        <span class="tech-tag">Nginx</span>
                    </div>
                </div>

                <div class="work-item">
                    <h4>Cost Tracking & Attribution</h4>
                    <p>Every LLM call costs money. You track it per user/request for billing and analytics.</p>
                    <ul>
                        <li>Count input/output tokens for each request</li>
                        <li>Store cost data in time-series database</li>
                        <li>Build dashboards showing spend by user/feature</li>
                        <li>Alert when costs exceed thresholds</li>
                    </ul>
                    <div class="tech-stack">
                        <span class="tech-tag">PostgreSQL</span>
                        <span class="tech-tag">InfluxDB</span>
                        <span class="tech-tag">Prometheus</span>
                        <span class="tech-tag">Grafana</span>
                    </div>
                </div>

                <div class="ai-highlight">
                    <strong>Key Takeaway:</strong> The LLM itself is a black box API. Your backend owns authentication, rate limiting, cost tracking, streaming, retries, and integration. This is 100% backend engineering.
                </div>
            </div>
        </div>

        <!-- LLM Deep Dive Tab -->
        <div class="ai-tab-content" id="llm-deep">
            <div class="ai-section">
                <h2>LLM Deep Dive - How It Actually Works</h2>
                <span class="ai-badge ai-badge-critical">Core Understanding</span>

                <p>Understanding LLMs from the ground up helps you build better AI applications. Let's break it down step by step.</p>

                <h3>1. Tokenization - Text to Numbers</h3>
                <div class="diagram-container">
                    <pre class="mermaid">
flowchart LR
    subgraph Input
        A[Hello world!]
    end
    subgraph Tokenizer
        B[BPE/WordPiece]
    end
    subgraph Tokens
        C["[15496, 995, 0]"]
    end
    subgraph Embeddings
        D["[[0.1, 0.3, ...],<br/>[0.2, 0.1, ...],<br/>[0.5, 0.2, ...]]"]
    end

    A --> B --> C --> D

    style A fill:#3b82f6,color:#fff
    style D fill:#10b981,color:#fff
                    </pre>
                </div>

                <button class="collapsible-btn">üìä Understanding Tokenization - Breaking It Down</button>
                <div class="collapsible-content">
                    <div class="inner">
                        <div class="tool-detail">
                            <h6>Input Text (Blue Box)</h6>
                            <p><strong>"Hello world!"</strong> - This is the raw string from the user. Could be anything: a question, code, or multi-paragraph document.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Tokenizer (BPE/WordPiece)</h6>
                            <p><strong>Byte Pair Encoding (BPE):</strong> Breaks text into subword units. "Hello" might be one token, "world" another, "!" a third. Common words = single tokens. Rare words = multiple tokens. GPT-4 has ~100K tokens in vocabulary.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Token IDs</h6>
                            <p><strong>[15496, 995, 0]:</strong> Each token maps to a unique number. "Hello" = 15496, "world" = 995, "!" = 0 (example). This is how the model sees text - just numbers. You can use tiktoken library to see this mapping.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Embeddings (Green Box)</h6>
                            <p><strong>High-dimensional vectors:</strong> Each token ID becomes a 768D-4096D vector. These vectors encode semantic meaning learned during training. Similar words have similar vectors (cosine similarity).</p>
                        </div>
                        <div class="ai-highlight">
                            <strong>üí∞ Why This Matters for Backend:</strong> Token count = cost. A 1000-word essay might be 1300 tokens. At $0.01/1K tokens, that's $0.013 input. If you generate 2000 token response at $0.03/1K, that's $0.06 output. Total: $0.073 per request. Scale to 100K requests/day = $7,300/day. Optimize prompts to reduce tokens!
                        </div>
                        <div class="tool-detail">
                            <h6>Practical Example</h6>
                            <p><strong>Inefficient:</strong> "Please provide a comprehensive, detailed, and thorough explanation..." (12 tokens)<br>
                            <strong>Efficient:</strong> "Explain in detail..." (4 tokens)<br>
                            Same meaning, 66% fewer tokens, 66% lower cost.</p>
                        </div>
                    </div>
                </div>

                <div class="work-item">
                    <h4>What Happens</h4>
                    <ul>
                        <li><strong>Text Input:</strong> "Hello world!" - raw string</li>
                        <li><strong>Tokenizer:</strong> Breaks text into subword units (BPE algorithm)</li>
                        <li><strong>Token IDs:</strong> Each token maps to a number in vocabulary (50K-100K tokens)</li>
                        <li><strong>Embeddings:</strong> Each ID becomes a high-dimensional vector (768-4096 dims)</li>
                    </ul>
                    <p><strong>Why it matters:</strong> Token count = cost. GPT-4 charges per 1K tokens. Efficient prompts save money.</p>
                </div>

                <h3>2. Transformer Architecture - The Brain</h3>
                <div class="diagram-container">
                    <pre class="mermaid">
flowchart TB
    subgraph Input_Layer[Input Processing]
        A[Token Embeddings] --> B[+ Position Embeddings]
    end

    subgraph Transformer_Block[Transformer Block x N]
        C[Multi-Head Self-Attention]
        D[Add & Normalize]
        E[Feed-Forward Network]
        F[Add & Normalize]
        C --> D --> E --> F
    end

    subgraph Output_Layer[Output Generation]
        G[Linear Layer]
        H[Softmax]
        I[Next Token Probabilities]
    end

    B --> C
    F --> G --> H --> I

    style C fill:#8b5cf6,color:#fff
    style E fill:#f59e0b,color:#fff
    style I fill:#10b981,color:#fff
                    </pre>
                </div>
                <div class="work-item">
                    <h4>Key Components Explained</h4>
                    <ul>
                        <li><strong>Self-Attention:</strong> Each token "looks at" all other tokens to understand context. "Bank" means different things in "river bank" vs "bank account"</li>
                        <li><strong>Multi-Head:</strong> Multiple attention patterns run in parallel (12-96 heads) to capture different relationships</li>
                        <li><strong>Feed-Forward:</strong> Simple neural network that processes each position independently</li>
                        <li><strong>Layer Stacking:</strong> GPT-4 has ~120 layers, each refining the representation</li>
                    </ul>
                </div>

                <h3>3. Attention Mechanism - "What to Focus On"</h3>
                <div class="diagram-container">
                    <pre class="mermaid">
flowchart LR
    subgraph Query["Query (Q)"]
        Q[What am I looking for?]
    end
    subgraph Key["Key (K)"]
        K[What do I contain?]
    end
    subgraph Value["Value (V)"]
        V[What information to return?]
    end
    subgraph Attention["Attention Scores"]
        S["softmax(Q * K^T / sqrt(d))"]
    end
    subgraph Output["Output"]
        O["Weighted sum of V"]
    end

    Q --> S
    K --> S
    S --> O
    V --> O

    style S fill:#8b5cf6,color:#fff
                    </pre>
                </div>
                <div class="work-item">
                    <h4>Attention Formula Breakdown</h4>
                    <pre><code>Attention(Q, K, V) = softmax(Q * K^T / sqrt(d_k)) * V

Example: "The cat sat on the mat because it was tired"
- When processing "it", attention scores might be:
  - "cat": 0.7 (high - "it" refers to cat)
  - "mat": 0.1 (low)
  - "The": 0.05 (very low)</code></pre>
                    <p><strong>Real Impact:</strong> This is why LLMs understand context. The model learned from billions of examples which words relate to which.</p>
                </div>

                <h3>4. Generation Process - Token by Token</h3>
                <div class="diagram-container">
                    <pre class="mermaid">
sequenceDiagram
    participant P as Prompt
    participant M as Model
    participant O as Output

    Note over P: "What is 2+2?"
    P->>M: Process prompt tokens
    M->>M: Generate probabilities
    M->>O: Sample "The"
    Note over O: "The"

    M->>M: Process "The" + context
    M->>O: Sample "answer"
    Note over O: "The answer"

    M->>M: Process all tokens
    M->>O: Sample "is"
    Note over O: "The answer is"

    M->>M: Process all tokens
    M->>O: Sample "4"
    Note over O: "The answer is 4"

    M->>O: Sample EOS token
    Note over O: Generation complete
                    </pre>
                </div>
                <div class="work-item">
                    <h4>Generation Parameters You Control</h4>
                    <ul>
                        <li><strong>Temperature (0-2):</strong> 0 = deterministic, 1 = creative, 2 = random. Use 0 for code, 0.7 for chat.</li>
                        <li><strong>Top-p (nucleus sampling):</strong> Only consider tokens with cumulative probability > p. 0.9 is common.</li>
                        <li><strong>Top-k:</strong> Only consider top k most likely tokens. 50 is common.</li>
                        <li><strong>Max tokens:</strong> Stop after N tokens. Controls cost and response length.</li>
                        <li><strong>Stop sequences:</strong> Custom strings that end generation (e.g., "Human:", "```")</li>
                    </ul>
                </div>

                <h3>5. Context Window & KV Cache</h3>
                <div class="diagram-container">
                    <pre class="mermaid">
flowchart TB
    subgraph Context_Window["Context Window (e.g., 128K tokens)"]
        A["System Prompt<br/>500 tokens"]
        B["Conversation History<br/>5000 tokens"]
        C["Retrieved Context (RAG)<br/>3000 tokens"]
        D["User Query<br/>100 tokens"]
        E["Available for Response<br/>119,400 tokens"]
    end

    subgraph KV_Cache["KV Cache (Optimization)"]
        F["Cached K,V from previous tokens"]
        G["Only compute attention for new token"]
    end

    A --> B --> C --> D --> E
    F --> G

    style A fill:#ef4444,color:#fff
    style E fill:#10b981,color:#fff
                    </pre>
                </div>
                <div class="work-item">
                    <h4>Why This Matters for Backend</h4>
                    <ul>
                        <li><strong>Context Management:</strong> You decide what fits in the window. RAG retrieves relevant chunks.</li>
                        <li><strong>KV Cache:</strong> Backend can maintain cache across requests for faster responses</li>
                        <li><strong>Truncation Strategy:</strong> When context overflows, what do you drop? Recent history? Old context?</li>
                        <li><strong>Cost Optimization:</strong> Longer context = higher cost. Be strategic about what you include.</li>
                    </ul>
                </div>

                <h3>6. Model Sizes & Trade-offs</h3>
                <div class="ai-grid">
                    <div class="ai-card">
                        <h4>Small (7B params)</h4>
                        <ul>
                            <li>Llama 3 7B, Mistral 7B</li>
                            <li>Fast inference (~50ms)</li>
                            <li>Can run on single GPU</li>
                            <li>Good for: classification, extraction</li>
                        </ul>
                    </div>
                    <div class="ai-card">
                        <h4>Medium (70B params)</h4>
                        <ul>
                            <li>Llama 3 70B</li>
                            <li>Moderate latency (~200ms)</li>
                            <li>Needs multi-GPU or quantization</li>
                            <li>Good for: most production use cases</li>
                        </ul>
                    </div>
                    <div class="ai-card">
                        <h4>Large (175B+ params)</h4>
                        <ul>
                            <li>GPT-4, Claude 3 Opus</li>
                            <li>Higher latency (~500ms+)</li>
                            <li>API-only (can't self-host)</li>
                            <li>Good for: complex reasoning, coding</li>
                        </ul>
                    </div>
                </div>

                <div class="ai-highlight">
                    <strong>Backend Engineer's Perspective:</strong> You don't train these models, but understanding how they work helps you:
                    <ul style="margin-top: 8px;">
                        <li>Choose the right model for your use case</li>
                        <li>Optimize prompts for token efficiency</li>
                        <li>Implement proper streaming and caching</li>
                        <li>Debug when outputs are unexpected</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Embeddings Tab -->
        <div class="ai-tab-content" id="embeddings">
            <div class="ai-section">
                <h2>Embedding Models (Text to Vectors)</h2>
                <span class="ai-badge ai-badge-backend">Data Pipeline Heavy</span>

                <p>Embeddings convert text to vectors for similarity search. Powers search, recommendations, and RAG.</p>

                <h3>Pipeline Flow Diagram</h3>
                <div class="diagram-container">
                    <pre class="mermaid">
flowchart LR
    A[Documents] --> B[Text Chunking]
    B --> C[Batch Texts]
    C --> D[Embedding API]
    D --> E["Vectors (768D)"]
    E --> F[Vector DB]

    G[User Query] --> H[Embed Query]
    H --> I[Search Vector DB]
    I --> J[Top K Results]
    J --> K[Re-rank]
    K --> L[Return to User]

    style D fill:#3b82f6,color:#fff
    style F fill:#8b5cf6,color:#fff
    style I fill:#10b981,color:#fff
                    </pre>
                </div>

                <button class="collapsible-btn">üìä Understanding Embeddings Pipeline - Two Phases</button>
                <div class="collapsible-content">
                    <div class="inner">
                        <h5 style="color: #3b82f6; margin-bottom: 10px;">üì• Indexing Phase (Top Row - Offline Process)</h5>
                        <div class="tool-detail">
                            <h6>1. Documents</h6>
                            <p>Your raw content: product descriptions, blog posts, documentation, customer reviews. Stored in database, S3, or filesystem.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>2. Text Chunking</h6>
                            <p>Split documents into 200-500 token chunks. Why? Embedding models have token limits (512-8192). Smaller chunks = more precise retrieval. Example: 5000-word article ‚Üí 20 chunks.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>3. Batch Texts</h6>
                            <p>Group chunks into batches of 50-100 for API efficiency. Single API call for 100 texts vs. 100 separate calls. Reduces latency and cost.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>4. Embedding API (Blue)</h6>
                            <p>Call OpenAI/Cohere API: <code>POST /embeddings</code> with batch of texts. Returns 768D vectors (OpenAI ada-002) or 1536D (text-embedding-3-large). Cost: ~$0.0001/1K tokens.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>5. Vectors (768D)</h6>
                            <p>Each chunk ‚Üí 768-dimensional array: [0.023, -0.15, 0.87, ...]. Semantically similar texts have similar vectors (high cosine similarity ~0.85+).</p>
                        </div>
                        <div class="tool-detail">
                            <h6>6. Vector DB (Purple)</h6>
                            <p>Store vectors in Pinecone/Weaviate/pgvector with metadata (source, date, category). Index with HNSW algorithm for fast similarity search.</p>
                        </div>

                        <h5 style="color: #10b981; margin-top: 20px; margin-bottom: 10px;">üîç Query Phase (Bottom Row - Real-time)</h5>
                        <div class="tool-detail">
                            <h6>7. User Query</h6>
                            <p>User asks: "How do I reset my password?". This is a search query, not exact keyword match.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>8. Embed Query</h6>
                            <p>Convert query to vector using same embedding model. Query embedding: [0.12, -0.08, 0.76, ...]. Must use same model as indexing!</p>
                        </div>
                        <div class="tool-detail">
                            <h6>9. Search Vector DB (Green)</h6>
                            <p>Find top K (e.g., 10) most similar vectors using cosine similarity. Returns chunks with scores: [("Reset password via settings", 0.92), ("Change login credentials", 0.87), ...]</p>
                        </div>
                        <div class="tool-detail">
                            <h6>10. Top K Results</h6>
                            <p>Retrieve associated text chunks and metadata for top results. Filter by metadata if needed (e.g., only docs from last year).</p>
                        </div>
                        <div class="tool-detail">
                            <h6>11. Re-rank</h6>
                            <p>Optional: Use cross-encoder model or business logic to reorder results. Boost recent docs, penalize outdated ones. Improves relevance.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>12. Return to User</h6>
                            <p>Present results to user OR pass to LLM for RAG (feed as context to answer question). Total latency: 50-200ms.</p>
                        </div>

                        <div class="ai-highlight">
                            <strong>üîß Backend Ownership:</strong> You build the entire pipeline: document ingestion cron jobs, chunking logic, batch processing, vector DB schema, search APIs, caching layer, re-ranking algorithms. The embedding API is just one HTTP call - everything else is infrastructure code.
                        </div>
                    </div>
                </div>

                <h3>What You Build</h3>

                <div class="work-item">
                    <h4>Embedding Generation Pipeline</h4>
                    <p>Convert text documents into vector representations at scale.</p>
                    <ul>
                        <li>Call embedding APIs (OpenAI, Cohere, Bedrock)</li>
                        <li>Batch requests to reduce API calls (up to 100 texts/call)</li>
                        <li>Handle rate limits and retries</li>
                        <li>Normalize vectors for cosine similarity</li>
                    </ul>
                    <div class="tech-stack">
                        <span class="tech-tag">OpenAI Embeddings</span>
                        <span class="tech-tag">Sentence Transformers</span>
                        <span class="tech-tag">AWS Bedrock</span>
                        <span class="tech-tag">Cohere</span>
                    </div>
                </div>

                <div class="work-item">
                    <h4>Vector Database Integration</h4>
                    <p>Store and query millions of vectors efficiently.</p>
                    <ul>
                        <li>Design vector schemas with metadata fields</li>
                        <li>Index vectors for fast similarity search (HNSW, IVF)</li>
                        <li>Implement upsert operations for updates</li>
                        <li>Handle pagination for large result sets</li>
                    </ul>
                    <div class="tech-stack">
                        <span class="tech-tag">Pinecone</span>
                        <span class="tech-tag">Weaviate</span>
                        <span class="tech-tag">pgvector</span>
                        <span class="tech-tag">Qdrant</span>
                    </div>
                </div>

                <div class="work-item">
                    <h4>Similarity Search APIs</h4>
                    <p>Expose endpoints for semantic search functionality.</p>
                    <ul>
                        <li>Embed user queries on-the-fly</li>
                        <li>Query vector DB with cosine/euclidean distance</li>
                        <li>Filter by metadata (date, category, user)</li>
                        <li>Re-rank results using business logic</li>
                    </ul>
                    <div class="tech-stack">
                        <span class="tech-tag">FastAPI</span>
                        <span class="tech-tag">Flask</span>
                        <span class="tech-tag">Express</span>
                        <span class="tech-tag">gRPC</span>
                    </div>
                </div>

                <div class="ai-highlight">
                    <strong>Key Takeaway:</strong> Embeddings are a backend data pipeline problem. You generate vectors, store them, query them, and keep them updated. The model is just an API you call.
                </div>
            </div>
        </div>

        <!-- RAG Systems Tab -->
        <div class="ai-tab-content" id="rag">
            <div class="ai-section">
                <h2>RAG (Retrieval-Augmented Generation)</h2>
                <span class="ai-badge ai-badge-heavy">80% Backend Work</span>
                <span class="ai-badge ai-badge-critical">Most Common AI Pattern</span>

                <p>RAG = LLM + your data. This is where backend engineers shine. The model is 20%, your infrastructure is 80%.</p>

                <h3>Complete RAG Pipeline</h3>
                <div class="diagram-container">
                    <pre class="mermaid">
sequenceDiagram
    participant User
    participant Backend
    participant Embed as Embedding API
    participant VectorDB
    participant Cache
    participant LLM

    Note over Backend,VectorDB: Indexing Phase (Offline)
    Backend->>Backend: Ingest documents
    Backend->>Backend: Chunk text (500 tokens)
    Backend->>Embed: Batch embed chunks
    Embed-->>Backend: Vectors
    Backend->>VectorDB: Store vectors + metadata

    Note over User,LLM: Query Phase (Real-time)
    User->>Backend: Ask question
    Backend->>Cache: Check cache
    Cache-->>Backend: Cache miss
    Backend->>Embed: Embed query
    Embed-->>Backend: Query vector
    Backend->>VectorDB: Similarity search
    VectorDB-->>Backend: Top 5 chunks
    Backend->>Backend: Assemble context
    Backend->>LLM: Prompt + context
    LLM-->>Backend: Answer
    Backend->>Cache: Store result
    Backend-->>User: Return answer
                    </pre>
                </div>

                <button class="collapsible-btn">üìä Understanding RAG - The Complete Picture</button>
                <div class="collapsible-content">
                    <div class="inner">
                        <h5 style="color: #8b5cf6; margin-bottom: 10px;">üîß Indexing Phase (Offline - Runs periodically)</h5>
                        <div class="tool-detail">
                            <h6>Step 1: Ingest Documents</h6>
                            <p><strong>Backend cron job:</strong> Connect to data sources (S3 buckets, databases, web scraping, file uploads). Extract text from PDFs, DOCX, HTML. Handle 10K-1M documents. Store in staging table with metadata (source, date, author).</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 2: Chunk Text (500 tokens)</h6>
                            <p><strong>Chunking strategy:</strong> Split each document into 500-token chunks with 50-token overlap (preserves context across boundaries). Use LangChain RecursiveTextSplitter or custom logic. 100-page doc ‚Üí 200 chunks.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 3-4: Batch Embed Chunks</h6>
                            <p><strong>Backend to Embedding API:</strong> Group chunks into batches of 100. Call OpenAI <code>POST /v1/embeddings</code> with <code>model: "text-embedding-3-small"</code>. Parallel processing with worker pool. 1M chunks = 10K API calls (batched).</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 5: Store Vectors + Metadata</h6>
                            <p><strong>Upsert to Vector DB:</strong> Store each chunk: <code>{id, vector: [0.12, ...], metadata: {text, source, date}, namespace}</code>. Pinecone/Weaviate auto-indexes with HNSW. Typically runs nightly or on document update webhooks.</p>
                        </div>

                        <h5 style="color: #10b981; margin-top: 20px; margin-bottom: 10px;">‚ö° Query Phase (Real-time - User-facing)</h5>
                        <div class="tool-detail">
                            <h6>Step 6: User Asks Question</h6>
                            <p><strong>Example:</strong> "What is our refund policy for defective products?" ‚Üí Backend receives this via <code>POST /api/chat</code></p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 7-8: Check Cache</h6>
                            <p><strong>Redis lookup:</strong> Key = hash(query). If identical question asked recently (last hour), return cached answer instantly. Cache hit rate: 15-30% in production. Saves API costs.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 9-10: Embed Query</h6>
                            <p><strong>On cache miss:</strong> Convert user's question to vector using same embedding model. "What is our refund policy..." ‚Üí [0.23, -0.15, 0.76, ...]. Takes ~50ms.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 11-12: Similarity Search</h6>
                            <p><strong>Vector DB query:</strong> <code>query(vector, top_k=5, filter={namespace: "policies"})</code>. Returns 5 most similar chunks with scores: [(chunk_id, 0.89), ...]. Takes ~30ms with HNSW index.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 13: Assemble Context</h6>
                            <p><strong>Backend logic:</strong> Format top 5 chunks into prompt context:<br>
                            <code>"Context:\n1. [Refund policy states...]\n2. [Defective items...]\n\nQuestion: {user_query}\nAnswer:"</code><br>
                            Ensure total tokens &lt; context window (8K-128K).</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 14-15: LLM Generation</h6>
                            <p><strong>Call LLM API:</strong> <code>POST /v1/chat/completions</code> with assembled prompt. LLM reads context and generates answer grounded in your data. Streams response back. Takes ~800ms.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 16: Cache Result</h6>
                            <p><strong>Store in Redis:</strong> <code>SET hash(query) answer EX 3600</code> (1-hour TTL). Next identical query returns instantly from cache.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 17: Return Answer</h6>
                            <p><strong>Backend to User:</strong> Stream answer with source citations: "According to our policy [Source: Refund_Policy_2024.pdf], defective products...". Total latency: 900ms (cache miss) or 20ms (cache hit).</p>
                        </div>

                        <div class="ai-highlight">
                            <strong>üí° RAG Reality Check:</strong> Out of 17 steps, the LLM only handles 1 (generation). Backend engineers own: data ingestion (1), chunking (1), embedding orchestration (2), vector storage (1), caching (3), retrieval (2), context assembly (1), streaming (1), error handling (all steps), monitoring (all steps). RAG is 95% backend infrastructure, 5% LLM API call.
                        </div>

                        <div class="real-world-example">
                            <h5>üè¢ Production Numbers (Mid-Size SaaS)</h5>
                            <p><strong>Indexing:</strong> 500K documents, 10M chunks, embedded in 6 hours (nightly job), $50 embedding cost<br>
                            <strong>Queries:</strong> 100K/day, 25% cache hit rate, 75K vector searches, 75K LLM calls<br>
                            <strong>Cost:</strong> Embeddings $0.50/day, Vector DB $200/month, LLM $150/day, Total: ~$5K/month<br>
                            <strong>Team:</strong> 2 backend engineers own this entire system</p>
                        </div>
                    </div>
                </div>

                <h3>Complete Backend Ownership</h3>

                <div class="work-item">
                    <h4>Data Ingestion Pipeline</h4>
                    <p>Get documents into the system from various sources.</p>
                    <ul>
                        <li>Connect to data sources (S3, databases, APIs, web scraping)</li>
                        <li>Parse multiple formats (PDF, DOCX, HTML, Markdown, CSV)</li>
                        <li>Extract text and metadata (author, date, category)</li>
                        <li>Handle incremental updates and change detection</li>
                    </ul>
                    <div class="tech-stack">
                        <span class="tech-tag">Apache Tika</span>
                        <span class="tech-tag">PyPDF2</span>
                        <span class="tech-tag">BeautifulSoup</span>
                        <span class="tech-tag">AWS Glue</span>
                    </div>

                    <div class="real-world-example">
                        <h5>Real-World Example: RevRag.AI (Bangalore)</h5>
                        <p class="company-name">Multi-format Enterprise Document Ingestion</p>
                        <p class="description">RevRag.AI built modular RAG pipelines handling Hindi/English PDFs, scanned images, and legacy document formats for Indian enterprises. Their ingestion system connects to client databases (MySQL, MongoDB), file systems (S3, NAS), and APIs with change detection via webhooks. Backend engineers handle 100K+ documents/day with parallel processing.</p>
                    </div>
                </div>

                <div class="work-item">
                    <h4>Chunking Strategy</h4>
                    <p>Split documents into optimal-sized chunks for retrieval.</p>
                    <ul>
                        <li>Implement fixed-size, semantic, or recursive chunking</li>
                        <li>Add overlap between chunks to preserve context</li>
                        <li>Respect document structure (paragraphs, sections)</li>
                        <li>Tune chunk size based on retrieval quality metrics</li>
                    </ul>
                    <div class="tech-stack">
                        <span class="tech-tag">LangChain</span>
                        <span class="tech-tag">LlamaIndex</span>
                        <span class="tech-tag">spaCy</span>
                        <span class="tech-tag">NLTK</span>
                    </div>
                </div>

                <div class="work-item">
                    <h4>Context Assembly</h4>
                    <p>Format retrieved chunks into prompt context for the LLM.</p>
                    <ul>
                        <li>Rank chunks by relevance score</li>
                        <li>Format chunks with source citations</li>
                        <li>Truncate to fit within token limits</li>
                        <li>Add instructions on how to use the context</li>
                    </ul>
                    <div class="tech-stack">
                        <span class="tech-tag">Python</span>
                        <span class="tech-tag">Jinja2</span>
                        <span class="tech-tag">LangChain</span>
                        <span class="tech-tag">LlamaIndex</span>
                    </div>
                </div>

                <div class="ai-highlight">
                    <strong>Key Takeaway:</strong> RAG is a massive backend system. Ingestion, chunking, embedding, vector DB, retrieval, caching, re-indexing - all backend work. The LLM just formats the final answer. You build everything around it.
                </div>
            </div>
        </div>

        <!-- Vector Search Tab -->
        <div class="ai-tab-content" id="vector">
            <div class="ai-section">
                <h2>Vector Search</h2>
                <span class="ai-badge ai-badge-backend">Database Engineering</span>

                <p>Vector search powers RAG, recommendations, and semantic search. You manage the vector database infrastructure.</p>

                <h3>Index Types Comparison</h3>
                <div class="ai-grid">
                    <div class="ai-card">
                        <h4>HNSW (Hierarchical Navigable Small World)</h4>
                        <ul>
                            <li>Best for: Most production use cases</li>
                            <li>Speed: Very fast queries</li>
                            <li>Memory: Higher (stores graph)</li>
                            <li>Accuracy: ~95-99% recall</li>
                        </ul>
                    </div>
                    <div class="ai-card">
                        <h4>IVF (Inverted File Index)</h4>
                        <ul>
                            <li>Best for: Large-scale, cost-sensitive</li>
                            <li>Speed: Fast with tuning</li>
                            <li>Memory: Lower</li>
                            <li>Accuracy: ~90-95% recall</li>
                        </ul>
                    </div>
                    <div class="ai-card">
                        <h4>Flat (Brute Force)</h4>
                        <ul>
                            <li>Best for: Small datasets (&lt;100K)</li>
                            <li>Speed: Slow (linear scan)</li>
                            <li>Memory: Minimal overhead</li>
                            <li>Accuracy: 100% (exact)</li>
                        </ul>
                    </div>
                </div>

                <div class="work-item">
                    <h4>Hybrid Search</h4>
                    <p>Combine vector search with keyword search for best results.</p>
                    <ul>
                        <li>Run vector similarity search (semantic)</li>
                        <li>Run BM25/Elasticsearch query (keyword)</li>
                        <li>Merge results using reciprocal rank fusion (RRF)</li>
                        <li>Tune weighting between semantic and keyword scores</li>
                    </ul>
                    <div class="tech-stack">
                        <span class="tech-tag">OpenSearch</span>
                        <span class="tech-tag">ElasticSearch</span>
                        <span class="tech-tag">Weaviate</span>
                        <span class="tech-tag">Meilisearch</span>
                    </div>
                </div>

                <div class="ai-highlight">
                    <strong>Key Takeaway:</strong> Vector search is database engineering. You manage indices, optimize queries, implement re-ranking, combine with keyword search, and handle metadata filtering.
                </div>
            </div>
        </div>

        <!-- Function Calling Tab -->
        <div class="ai-tab-content" id="function-calling">
            <div class="ai-section">
                <h2>Function Calling / Tool Use</h2>
                <span class="ai-badge ai-badge-heavy">100% Backend Work</span>
                <span class="ai-badge ai-badge-critical">Security Critical</span>

                <p>Function calling lets LLMs execute backend functions. This is the most backend-heavy AI feature.</p>

                <h3>Function Calling Flow</h3>
                <div class="diagram-container">
                    <pre class="mermaid">
sequenceDiagram
    participant User
    participant Backend
    participant LLM
    participant Tools as Backend Functions
    participant DB

    User->>Backend: "Book flight to Mumbai"
    Backend->>LLM: Prompt + tool schemas
    LLM-->>Backend: Call search_flights(origin, dest, date)
    Backend->>Backend: Validate params
    Backend->>Tools: Execute search_flights()
    Tools->>DB: Query flights table
    DB-->>Tools: Flight results
    Tools-->>Backend: Return JSON results
    Backend->>LLM: Continue with results
    LLM-->>Backend: Call book_flight(flight_id)
    Backend->>Backend: Check user permissions
    Backend->>Tools: Execute book_flight()
    Tools->>DB: Insert booking
    DB-->>Tools: Booking confirmed
    Tools-->>Backend: Success
    Backend->>LLM: Continue with success
    LLM-->>Backend: Final response
    Backend-->>User: "Booked flight AI123"
                    </pre>
                </div>

                <button class="collapsible-btn">üìä Understanding Function Calling - LLMs Execute Your Code</button>
                <div class="collapsible-content">
                    <div class="inner">
                        <div class="tool-detail">
                            <h6>Step 1: User Request</h6>
                            <p><strong>"Book flight to Mumbai"</strong> - Natural language input. User doesn't write SQL or call APIs. Backend needs to interpret intent.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 2: Send Tool Schemas to LLM</h6>
                            <p><strong>Backend defines functions:</strong> Send JSON schema describing available tools:<br>
                            <code>{
  "name": "search_flights",
  "description": "Search for available flights",
  "parameters": {
    "origin": {"type": "string"},
    "destination": {"type": "string"},
    "date": {"type": "string", "format": "YYYY-MM-DD"}
  }
}</code><br>
                            LLM sees this as "API documentation" - learns what functions exist and how to call them.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 3: LLM Decides to Call Function</h6>
                            <p><strong>LLM responds with function call:</strong> Instead of text, LLM returns:<br>
                            <code>{
  "function_call": {
    "name": "search_flights",
    "arguments": {
      "origin": "DEL",
      "destination": "BOM",
      "date": "2024-03-15"
    }
  }
}</code><br>
                            LLM inferred "Mumbai" = BOM airport code, assumed "today" if no date given.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 4: Backend Validates Parameters</h6>
                            <p><strong>Security check:</strong> Validate origin/dest are valid IATA codes, date is future, user has permission to search flights. Reject if invalid. Never trust LLM output blindly!</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 5-7: Execute Function</h6>
                            <p><strong>Backend calls actual function:</strong> <code>search_flights(origin="DEL", dest="BOM", date="2024-03-15")</code> queries PostgreSQL flights table. Returns JSON: <code>[{id: "AI123", price: 8500, departure: "10:30"}, ...]</code></p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 8: Return Results to LLM</h6>
                            <p><strong>Feed function output back:</strong> Send flight results to LLM as "function result". LLM now sees available flights and decides next action.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 9: LLM Calls Another Function</h6>
                            <p><strong>Multi-step reasoning:</strong> LLM analyzes results and decides to call <code>book_flight(flight_id="AI123")</code>. This is a write operation - destructive action!</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 10: Check User Permissions (CRITICAL)</h6>
                            <p><strong>Authorization:</strong> Verify user has payment method on file, sufficient balance, not exceeding booking limits. Check JWT claims. Log attempt. This is 100% backend security code.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 11-13: Execute Booking</h6>
                            <p><strong>Database transaction:</strong> <code>INSERT INTO bookings ...</code> Deduct payment, send confirmation email, update inventory. Wrap in transaction (rollback on failure).</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 14-15: Final Response</h6>
                            <p><strong>LLM formats result:</strong> Given booking success, LLM generates natural language response: "I've booked flight AI123 from Delhi to Mumbai on March 15th for ‚Çπ8,500. Confirmation sent to your email."</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Step 16: Return to User</h6>
                            <p><strong>Backend sends response:</strong> Stream final answer to user. Also include structured data (booking ID, receipt) for frontend to display.</p>
                        </div>

                        <div class="ai-highlight">
                            <strong>üîê Security Reality:</strong> The LLM is a prompt interpreter, NOT a security boundary. YOU must:<br>
                            - Validate all function parameters (type, format, range)<br>
                            - Enforce authentication & authorization on every function<br>
                            - Rate limit function calls per user<br>
                            - Require confirmation for destructive actions (delete, payment)<br>
                            - Audit log all function executions with user_id, timestamp<br>
                            - Implement circuit breakers for cascading failures<br>
                            The LLM just suggests function calls - you decide whether to execute them.
                        </div>

                        <div class="real-world-example">
                            <h5>‚ö†Ô∏è Common Security Mistakes</h5>
                            <p><strong>‚ùå Bad:</strong> Execute any function LLM suggests without validation<br>
                            <strong>‚úÖ Good:</strong> Allowlist of safe functions per user role (customer can search, only admin can refund)<br><br>
                            <strong>‚ùå Bad:</strong> Trust LLM parameter extraction (it might hallucinate values)<br>
                            <strong>‚úÖ Good:</strong> Validate params with JSON schema, regex, database lookup<br><br>
                            <strong>‚ùå Bad:</strong> Let LLM call <code>delete_user()</code> directly<br>
                            <strong>‚úÖ Good:</strong> Require explicit user confirmation for destructive actions</p>
                        </div>
                    </div>
                </div>

                <h3>What You Own (Everything)</h3>

                <div class="work-item">
                    <h4>Define Tool Schemas</h4>
                    <p>Describe your functions in JSON Schema for the model to understand.</p>
                    <ul>
                        <li>Write OpenAPI/JSON Schema specs for each function</li>
                        <li>Include parameter types, constraints, descriptions</li>
                        <li>Version schemas as APIs evolve</li>
                        <li>Register schemas with model API</li>
                    </ul>
                    <div class="tech-stack">
                        <span class="tech-tag">JSON Schema</span>
                        <span class="tech-tag">OpenAPI 3.0</span>
                        <span class="tech-tag">Pydantic</span>
                        <span class="tech-tag">TypeScript</span>
                    </div>
                </div>

                <div class="work-item">
                    <h4>Secure Side Effects</h4>
                    <p>Functions often modify state (DB writes, API calls). You protect them.</p>
                    <ul>
                        <li>Implement role-based access control (RBAC)</li>
                        <li>Require user confirmation for destructive actions</li>
                        <li>Rate limit function calls per user</li>
                        <li>Audit log all function executions</li>
                    </ul>
                    <div class="tech-stack">
                        <span class="tech-tag">IAM</span>
                        <span class="tech-tag">OAuth</span>
                        <span class="tech-tag">JWT</span>
                        <span class="tech-tag">CloudWatch</span>
                    </div>
                </div>

                <div class="ai-highlight">
                    <strong>Key Takeaway:</strong> Function calling is backend engineering disguised as AI. You define APIs, validate inputs, execute logic, handle errors, enforce security. The model just decides which function to call - you do everything else.
                </div>
            </div>
        </div>

        <!-- AI Agents Tab -->
        <div class="ai-tab-content" id="agents">
            <div class="ai-section">
                <h2>AI Agents (LLM + Memory + Tools)</h2>
                <span class="ai-badge ai-badge-heavy">Backend State Machines</span>
                <span class="ai-badge ai-badge-critical">Complex Backend System</span>

                <p>Agents are autonomous systems that combine LLMs with tools and memory. This is sophisticated backend engineering.</p>

                <h3>Agent Architecture</h3>
                <div class="diagram-container">
                    <pre class="mermaid">
flowchart TB
    User[User Input] --> Agent[Agent Controller]
    Agent --> Memory[Memory Store]
    Memory --> ShortTerm[Short-term: Redis]
    Memory --> LongTerm[Long-term: Vector DB]

    Agent --> LLM[LLM Reasoning]
    LLM --> Decision{Decision}
    Decision -->|Use Tool| Tools[Tool Executor]
    Decision -->|Respond| Response[Generate Response]

    Tools --> DB[(Database)]
    Tools --> API[External APIs]
    Tools --> Code[Code Runner]

    Tools --> Result[Tool Result]
    Result --> LLM
    Response --> User

    Agent --> Guard[Safety Rails]
    Guard --> RateLimit[Rate Limits]
    Guard --> Permissions[Permissions]
    Guard --> Audit[Audit Log]

    style Agent fill:#3b82f6,color:#fff
    style LLM fill:#8b5cf6,color:#fff
    style Tools fill:#10b981,color:#fff
    style Memory fill:#f59e0b,color:#fff
                    </pre>
                </div>

                <button class="collapsible-btn">üìä Understanding AI Agents - Autonomous Backend Systems</button>
                <div class="collapsible-content">
                    <div class="inner">
                        <div class="tool-detail">
                            <h6>User Input ‚Üí Agent Controller (Blue)</h6>
                            <p><strong>Entry point:</strong> User sends task: "Analyze last month's sales and email report to team". Agent Controller is your orchestration layer - a state machine that manages multi-step workflows. Implemented as a backend service (Python/Node.js).</p>
                        </div>

                        <h5 style="color: #f59e0b; margin-top: 15px;">üß† Memory Store (Orange) - Two Types</h5>
                        <div class="tool-detail">
                            <h6>Short-term Memory: Redis</h6>
                            <p><strong>Conversation context:</strong> Store last 10-20 messages in Redis with session_id as key. TTL = 1 hour. Includes user messages, tool calls, results. Used for immediate context within conversation.<br>
                            <code>redis.set(f"session:{session_id}", json.dumps(messages), ex=3600)</code></p>
                        </div>
                        <div class="tool-detail">
                            <h6>Long-term Memory: Vector DB</h6>
                            <p><strong>Semantic memory:</strong> Store facts, preferences, past interactions as embeddings in Pinecone/Weaviate. Retrieve relevant memories based on current query.<br>
                            Example: User mentioned "I prefer CSV reports" 3 months ago ‚Üí Agent retrieves this preference when generating report today.</p>
                        </div>

                        <h5 style="color: #8b5cf6; margin-top: 15px;">ü§ñ LLM Reasoning (Purple) - Decision Maker</h5>
                        <div class="tool-detail">
                            <h6>Decision Diamond</h6>
                            <p><strong>LLM evaluates options:</strong> Given current state + memories + available tools, LLM decides:<br>
                            <strong>Option 1:</strong> "I need to use a tool" ‚Üí Goes to Tool Executor (green)<br>
                            <strong>Option 2:</strong> "I have enough info to respond" ‚Üí Generate Response<br>
                            This is a loop - agent can call multiple tools in sequence.</p>
                        </div>

                        <h5 style="color: #10b981; margin-top: 15px;">üîß Tool Executor (Green) - Action Layer</h5>
                        <div class="tool-detail">
                            <h6>Database Tools</h6>
                            <p><strong>SQL query executor:</strong> Agent calls <code>query_sales_data(start_date, end_date)</code> ‚Üí Backend executes <code>SELECT SUM(revenue) FROM sales WHERE ...</code> Returns structured data to LLM.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>External API Tools</h6>
                            <p><strong>Integration layer:</strong> Call Stripe API for payments, SendGrid for emails, Slack for notifications. Backend wraps these in tool functions with error handling, retries, authentication.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Code Runner (Advanced)</h6>
                            <p><strong>Sandboxed execution:</strong> For complex tasks, agent writes Python/JavaScript code, backend executes in isolated Docker container (e.g., using E2B, Modal). Security-critical: proper sandboxing prevents arbitrary code execution.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Tool Result Feedback Loop</h6>
                            <p><strong>Results go back to LLM:</strong> Agent executed <code>query_sales_data()</code> ‚Üí Got results: <code>{"revenue": 125000, "orders": 450}</code> ‚Üí LLM sees this and decides next action (maybe call <code>generate_chart()</code> or <code>send_email()</code>)</p>
                        </div>

                        <h5 style="color: #ef4444; margin-top: 15px;">üõ°Ô∏è Safety Rails (Red Border) - Production Requirements</h5>
                        <div class="tool-detail">
                            <h6>Rate Limits</h6>
                            <p><strong>Prevent runaway agents:</strong> Max 50 tool calls per session, max 10 LLM calls per minute. Use Redis counters. If exceeded, pause agent and ask for user confirmation to continue.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Permissions</h6>
                            <p><strong>RBAC enforcement:</strong> Before executing any tool, check <code>user.role</code>. Regular users can read data, only admins can delete/modify. Check permissions at tool execution time, not just schema definition time.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Audit Log</h6>
                            <p><strong>Compliance:</strong> Log every tool call: <code>{timestamp, user_id, tool_name, params, result, status}</code>. Store in PostgreSQL or CloudWatch. Required for debugging ("Why did agent send that email?") and compliance.</p>
                        </div>

                        <div class="ai-highlight">
                            <strong>üèóÔ∏è What You Build as Backend Engineer:</strong><br>
                            <strong>1. Agent Controller:</strong> State machine orchestrating LLM + tools (Python: LangGraph, JavaScript: LangChain.js)<br>
                            <strong>2. Memory Systems:</strong> Redis for short-term, Vector DB for long-term, retrieval logic<br>
                            <strong>3. Tool Registry:</strong> Define 10-50 tool functions, JSON schemas, validators<br>
                            <strong>4. Tool Implementations:</strong> Write actual functions (DB queries, API calls, file operations)<br>
                            <strong>5. Safety Infrastructure:</strong> Rate limiters, permission checkers, audit loggers<br>
                            <strong>6. Error Handling:</strong> Retry logic, fallbacks, graceful degradation<br>
                            <strong>7. Observability:</strong> Trace agent execution, measure latency per step, cost tracking<br>
                            The LLM is just the "brain" that decides what to do next - you build the entire body (tools, memory, safety, infrastructure).
                        </div>

                        <div class="real-world-example">
                            <h5>üìà Agent Execution Example: "Analyze sales and email report"</h5>
                            <p><strong>Turn 1:</strong> LLM decides to call <code>query_sales_data("2024-01", "2024-01")</code><br>
                            <strong>Turn 2:</strong> Got data. LLM decides to call <code>calculate_growth_rate(current, previous)</code><br>
                            <strong>Turn 3:</strong> LLM decides to call <code>generate_chart(data, type="bar")</code><br>
                            <strong>Turn 4:</strong> LLM decides to call <code>send_email(to="team@company.com", subject="Sales Report", body=..., attachments=[chart])</code><br>
                            <strong>Turn 5:</strong> All done. LLM generates response: "Report sent! Revenue grew 12% to $125K across 450 orders."<br><br>
                            <strong>Backend managed:</strong> 5 tool executions, 5 LLM calls, memory updates, permission checks, audit logs, error handling. Total: ~4 seconds, $0.05 in API costs.</p>
                        </div>
                    </div>
                </div>

                <h3>What You Build</h3>

                <div class="work-item">
                    <h4>Agent State Storage</h4>
                    <p>Track agent state across multi-turn interactions.</p>
                    <ul>
                        <li>Store conversation history (messages, tool calls, results)</li>
                        <li>Track current task and sub-tasks</li>
                        <li>Maintain agent memory (short-term and long-term)</li>
                        <li>Handle state serialization for resuming later</li>
                    </ul>
                    <div class="tech-stack">
                        <span class="tech-tag">Redis</span>
                        <span class="tech-tag">DynamoDB</span>
                        <span class="tech-tag">PostgreSQL</span>
                        <span class="tech-tag">MongoDB</span>
                    </div>
                </div>

                <div class="work-item">
                    <h4>Memory Management</h4>
                    <p>Give agents both short-term and long-term memory.</p>
                    <ul>
                        <li><strong>Short-term:</strong> Recent conversation in Redis (last 10-20 turns)</li>
                        <li><strong>Long-term:</strong> Facts, preferences in vector DB (semantic retrieval)</li>
                        <li>Implement memory summarization to compress history</li>
                        <li>Apply TTL policies for privacy compliance</li>
                    </ul>
                    <div class="tech-stack">
                        <span class="tech-tag">Redis</span>
                        <span class="tech-tag">Pinecone</span>
                        <span class="tech-tag">PostgreSQL</span>
                        <span class="tech-tag">MemGPT</span>
                    </div>
                </div>

                <div class="ai-highlight">
                    <strong>Key Takeaway:</strong> Agents are backend systems that happen to use LLMs. You build state machines, tool executors, memory stores, safety systems, and orchestration logic. The LLM is just the decision-making component - you build everything else.
                </div>
            </div>
        </div>

        <!-- Multimodal Tab -->
        <div class="ai-tab-content" id="multimodal">
            <div class="ai-section">
                <h2>Multimodal Models (Text + Image + Audio)</h2>
                <span class="ai-badge ai-badge-backend">File Handling Heavy</span>

                <p>Models like GPT-4o and Gemini accept multiple input types. You handle the file uploads and preprocessing.</p>

                <h3>Request Flow</h3>
                <div class="diagram-container">
                    <pre class="mermaid">
sequenceDiagram
    participant Client
    participant Backend
    participant S3
    participant Queue as Job Queue
    participant Worker
    participant Model as Multimodal API

    Client->>Backend: Upload image/audio
    Backend->>Backend: Validate file type/size
    Backend->>S3: Store original
    S3-->>Backend: File URL
    Backend->>Queue: Enqueue job
    Backend-->>Client: Job ID + polling URL
    Worker->>Queue: Pull job
    Worker->>S3: Fetch file
    Worker->>Worker: Preprocess (resize/transcode)
    Worker->>Model: Send to API
    Model-->>Worker: Results
    Worker->>Backend: Update job status
    Client->>Backend: Poll for results
    Backend-->>Client: Return results
                    </pre>
                </div>

                <div class="work-item">
                    <h4>File Upload Infrastructure</h4>
                    <p>Users upload images, audio, PDFs. You need robust upload handling.</p>
                    <ul>
                        <li>Implement multipart/form-data endpoints</li>
                        <li>Validate file types, sizes, and formats</li>
                        <li>Scan for malware and malicious content</li>
                        <li>Generate presigned URLs for direct S3 uploads</li>
                    </ul>
                    <div class="tech-stack">
                        <span class="tech-tag">Multer</span>
                        <span class="tech-tag">boto3</span>
                        <span class="tech-tag">S3</span>
                        <span class="tech-tag">CloudFlare R2</span>
                    </div>
                </div>

                <div class="work-item">
                    <h4>Media Preprocessing</h4>
                    <p>Images need resizing, audio needs transcoding. You build these pipelines.</p>
                    <ul>
                        <li>Resize/compress images to reduce costs</li>
                        <li>Convert audio formats (MP3, WAV, FLAC)</li>
                        <li>Extract frames from video</li>
                        <li>OCR preprocessing for document images</li>
                    </ul>
                    <div class="tech-stack">
                        <span class="tech-tag">FFmpeg</span>
                        <span class="tech-tag">Pillow</span>
                        <span class="tech-tag">Sharp</span>
                        <span class="tech-tag">Lambda</span>
                    </div>
                </div>

                <div class="ai-highlight">
                    <strong>Key Takeaway:</strong> Multimodal = file handling at scale. You own uploads, storage, preprocessing, async jobs, and metadata management. Classic backend work.
                </div>
            </div>
        </div>

        <!-- Observability Tab -->
        <div class="ai-tab-content" id="observability">
            <div class="ai-section">
                <h2>AI Observability & Guardrails</h2>
                <span class="ai-badge ai-badge-backend">Monitoring & Security</span>

                <p>Production AI systems need monitoring, evaluation, and safety controls. Backend engineers own this.</p>

                <h3>Input Validation Pipeline</h3>
                <div class="diagram-container">
                    <pre class="mermaid">
flowchart TD
    A[User Input] --> B{Length Check}
    B -->|> 10K chars| C[Reject: 413 Too Large]
    B -->|Valid| D{Format Validation}
    D -->|Invalid| E[Reject: 400 Bad Request]
    D -->|Valid| F{Prompt Injection Scan}
    F -->|Detected| G[Reject: 400 Malicious]
    F -->|Clean| H{Rate Limit Check}
    H -->|Exceeded| I[Reject: 429 Rate Limited]
    H -->|Within Limits| J{Content Filter}
    J -->|Toxic/NSFW| K[Reject: 400 Policy Violation]
    J -->|Safe| L[Send to LLM]

    style C fill:#ef4444,color:#fff
    style E fill:#ef4444,color:#fff
    style G fill:#ef4444,color:#fff
    style I fill:#f59e0b,color:#fff
    style K fill:#ef4444,color:#fff
    style L fill:#10b981,color:#fff
                    </pre>
                </div>

                <div class="work-item">
                    <h4>Latency Metrics</h4>
                    <p>Track end-to-end and per-component latencies.</p>
                    <ul>
                        <li>Instrument API endpoints with timing middleware</li>
                        <li>Measure embedding time, retrieval time, LLM time separately</li>
                        <li>Track p50, p95, p99 latencies</li>
                        <li>Alert on latency spikes</li>
                    </ul>
                    <div class="tech-stack">
                        <span class="tech-tag">Prometheus</span>
                        <span class="tech-tag">Grafana</span>
                        <span class="tech-tag">DataDog</span>
                        <span class="tech-tag">New Relic</span>
                    </div>
                </div>

                <div class="work-item">
                    <h4>Cost Tracking</h4>
                    <p>Monitor spending on API calls and infrastructure.</p>
                    <ul>
                        <li>Track token usage per request (input + output)</li>
                        <li>Calculate cost per user, per feature</li>
                        <li>Build dashboards showing daily/monthly spend</li>
                        <li>Set budget alerts and automatic shutoffs</li>
                    </ul>
                    <div class="tech-stack">
                        <span class="tech-tag">CloudWatch</span>
                        <span class="tech-tag">InfluxDB</span>
                        <span class="tech-tag">PostgreSQL</span>
                        <span class="tech-tag">Metabase</span>
                    </div>
                </div>

                <div class="ai-highlight">
                    <strong>Key Takeaway:</strong> Observability and guardrails are backend concerns. You instrument, monitor, log, validate, filter, and protect. These are standard backend practices applied to AI systems.
                </div>
            </div>
        </div>

        <!-- Architecture Tab -->
        <div class="ai-tab-content" id="architecture">
            <div class="ai-section">
                <h2>AI System Architecture</h2>
                <span class="ai-badge ai-badge-critical">Mental Model</span>

                <p>Here's how all the pieces fit together in a production AI system.</p>

                <h3>Full System Architecture</h3>
                <div class="diagram-container">
                    <pre class="mermaid">
flowchart TB
    User[User] --> API[API Gateway]
    API --> Auth[Auth & Rate Limit]
    Auth --> Router[Model Router]

    Router --> LLM1[LLM API GPT-4]
    Router --> LLM2[LLM API Claude]
    Router --> LLM3[LLM API Llama]

    Router --> RAG[RAG Pipeline]
    RAG --> Embed[Embedding API]
    RAG --> VectorDB[Vector DB]
    RAG --> Cache[Redis Cache]

    Router --> Tools[Function Calling]
    Tools --> DB[(PostgreSQL)]
    Tools --> External[External APIs]
    Tools --> Lambda[AWS Lambda]

    Router --> Agent[AI Agent]
    Agent --> Memory[Memory Store]
    Agent --> State[State Machine]
    Agent --> Orchestrator[Orchestration]

    Router --> Guard[Guardrails]
    Guard --> InputVal[Input Validation]
    Guard --> OutputFilter[Output Filtering]
    Guard --> PIIRedact[PII Redaction]

    LLM1 --> Metrics[Observability]
    LLM2 --> Metrics
    LLM3 --> Metrics
    RAG --> Metrics
    Tools --> Metrics
    Agent --> Metrics

    Metrics --> Prom[Prometheus]
    Metrics --> Logs[CloudWatch Logs]
    Metrics --> Cost[Cost Tracking]

    style User fill:#3b82f6,color:#fff
    style API fill:#8b5cf6,color:#fff
    style Router fill:#10b981,color:#fff
    style RAG fill:#f59e0b,color:#fff
    style Tools fill:#ef4444,color:#fff
    style Agent fill:#06b6d4,color:#fff
                    </pre>
                </div>

                <button class="collapsible-btn">üìä Understanding Full Production Architecture - All Pieces Together</button>
                <div class="collapsible-content">
                    <div class="inner">
                        <h5 style="color: #3b82f6;">üö™ Entry Layer (Blue/Purple)</h5>
                        <div class="tool-detail">
                            <h6>User ‚Üí API Gateway</h6>
                            <p><strong>Entry point:</strong> User hits your REST API, GraphQL endpoint, or WebSocket. AWS API Gateway, Kong, or custom Express/FastAPI server. Handles HTTPS, CORS, DDoS protection.</p>
                        </div>
                        <div class="tool-detail">
                            <h6>Auth & Rate Limit (Purple)</h6>
                            <p><strong>Security layer:</strong> Validate JWT token, check API key, verify user identity. Rate limit: 100 req/min per user (Redis counters). Return 401 if unauthenticated, 429 if rate limited.</p>
                        </div>

                        <h5 style="color: #10b981; margin-top: 15px;">üéØ Model Router (Green) - Traffic Controller</h5>
                        <div class="tool-detail">
                            <h6>Intelligent Routing Logic</h6>
                            <p><strong>Decision logic:</strong> Based on request type, route to:<br>
                            - <strong>Simple queries:</strong> GPT-3.5-turbo (fast, cheap)<br>
                            - <strong>Complex reasoning:</strong> GPT-4 or Claude Opus<br>
                            - <strong>Code generation:</strong> GPT-4 or Llama 70B<br>
                            - <strong>Fallback:</strong> If OpenAI down, route to Anthropic<br>
                            Implemented as backend service with routing rules.</p>
                        </div>

                        <h5 style="color: #3b82f6; margin-top: 15px;">ü§ñ LLM API Layer</h5>
                        <div class="tool-detail">
                            <h6>Multi-Provider Strategy</h6>
                            <p><strong>LLM1: GPT-4</strong> - High quality, $0.03/1K output tokens, 128K context<br>
                            <strong>LLM2: Claude</strong> - Long context (200K), better safety, $0.015/1K<br>
                            <strong>LLM3: Llama (self-hosted)</strong> - Fixed cost, full control, requires GPU infra<br>
                            Router maintains health checks, circuit breakers, automatic failover.</p>
                        </div>

                        <h5 style="color: #f59e0b; margin-top: 15px;">üìö RAG Pipeline (Orange) - Knowledge Layer</h5>
                        <div class="tool-detail">
                            <h6>RAG Components</h6>
                            <p><strong>Embedding API:</strong> Convert queries/docs to vectors<br>
                            <strong>Vector DB:</strong> Pinecone/Weaviate storing 10M+ vectors<br>
                            <strong>Redis Cache:</strong> Cache embedded queries (1hr TTL), API responses (5min TTL)<br>
                            When user asks about your data, Router sends request through RAG pipeline first.</p>
                        </div>

                        <h5 style="color: #ef4444; margin-top: 15px;">üîß Function Calling (Red) - Action Layer</h5>
                        <div class="tool-detail">
                            <h6>Tool Execution Infrastructure</h6>
                            <p><strong>PostgreSQL:</strong> Query/modify your app database (orders, users, products)<br>
                            <strong>External APIs:</strong> Stripe, SendGrid, Slack, Google Maps<br>
                            <strong>AWS Lambda:</strong> Serverless functions for heavy computation<br>
                            Router uses this when user request needs actions (book, send, create, delete).</p>
                        </div>

                        <h5 style="color: #06b6d4; margin-top: 15px;">ü§ñ AI Agent (Cyan) - Autonomous Layer</h5>
                        <div class="tool-detail">
                            <h6>Agent System</h6>
                            <p><strong>Memory Store:</strong> Redis + Vector DB for conversation state<br>
                            <strong>State Machine:</strong> Track multi-step workflows (task planning, execution, verification)<br>
                            <strong>Orchestrator:</strong> Manages agent lifecycle, schedules background tasks<br>
                            Router delegates complex, multi-step tasks to Agent system.</p>
                        </div>

                        <h5 style="color: #8b5cf6; margin-top: 15px;">üõ°Ô∏è Guardrails - Safety & Compliance</h5>
                        <div class="tool-detail">
                            <h6>Protection Layers</h6>
                            <p><strong>Input Validation:</strong> Block prompt injection attempts, malicious payloads, oversized inputs<br>
                            <strong>Output Filtering:</strong> Scan for toxic content, profanity, sensitive data leaks<br>
                            <strong>PII Redaction:</strong> Detect & mask email, phone, SSN, credit cards before logging<br>
                            Every request flows through guardrails before reaching LLM.</p>
                        </div>

                        <h5 style="color: #10b981; margin-top: 15px;">üìä Observability - Production Monitoring</h5>
                        <div class="tool-detail">
                            <h6>Metrics Collection</h6>
                            <p><strong>Prometheus:</strong> Metrics (request rate, latency p50/p95/p99, error rate, token count)<br>
                            <strong>CloudWatch Logs:</strong> Full request/response logs, error traces, audit trail<br>
                            <strong>Cost Tracking:</strong> Per-user, per-feature cost breakdown (LLM calls, embeddings, vector searches)<br>
                            All components send metrics ‚Üí Observability layer ‚Üí Grafana dashboards.</p>
                        </div>

                        <div class="ai-highlight">
                            <strong>üèóÔ∏è Backend Engineering Breakdown by Component:</strong><br>
                            <strong>YOU Own:</strong> API Gateway, Auth, Rate Limiting, Router (100% yours)<br>
                            <strong>YOU Integrate:</strong> LLM APIs (10% yours - just HTTP calls)<br>
                            <strong>YOU Build:</strong> RAG Pipeline (95% yours - only embedding call is external)<br>
                            <strong>YOU Build:</strong> Function Calling (100% yours - LLM just suggests, you execute)<br>
                            <strong>YOU Build:</strong> AI Agents (95% yours - orchestration, memory, tools all yours)<br>
                            <strong>YOU Build:</strong> Guardrails (100% yours - validation, filtering, security)<br>
                            <strong>YOU Build:</strong> Observability (100% yours - instrumentation, logging, dashboards)<br><br>
                            <strong>Total:</strong> LLMs are ~5% of the system (3 boxes out of 60+ components). Everything else is backend engineering: APIs, databases, caching, queues, orchestration, security, monitoring.
                        </div>

                        <div class="real-world-example">
                            <h5>üí∞ Production Cost Breakdown (100K users, 1M requests/day)</h5>
                            <p><strong>Infrastructure:</strong> AWS/GCP compute, databases, Redis - $3K/month<br>
                            <strong>Vector DB:</strong> Pinecone/Weaviate - $500/month<br>
                            <strong>LLM API calls:</strong> OpenAI/Anthropic - $8K/month (varies with usage)<br>
                            <strong>Embeddings:</strong> OpenAI embeddings API - $200/month<br>
                            <strong>Monitoring:</strong> Datadog/New Relic - $500/month<br>
                            <strong>Total:</strong> ~$12K/month, $144K/year<br><br>
                            <strong>Team:</strong> 3-4 backend engineers manage this entire system. Senior backend engineers with AI/ML awareness are building the next generation of software.</p>
                        </div>

                        <div class="real-world-example">
                            <h5>üéØ Request Flow Example: "Show me sales for last month and email to CEO"</h5>
                            <p><strong>1.</strong> User ‚Üí API Gateway ‚Üí Auth (validate JWT) ‚Üí Rate Limit (check quota)<br>
                            <strong>2.</strong> Router analyzes: "Complex task needs Agent"<br>
                            <strong>3.</strong> Agent receives task ‚Üí Checks Memory (any preferences?)<br>
                            <strong>4.</strong> Agent ‚Üí LLM: "What steps needed?" ‚Üí LLM: "Query DB, generate chart, send email"<br>
                            <strong>5.</strong> Agent calls Tool: <code>query_sales(last_month)</code> ‚Üí PostgreSQL ‚Üí Returns data<br>
                            <strong>6.</strong> Agent calls Tool: <code>generate_chart(data)</code> ‚Üí Lambda generates PNG<br>
                            <strong>7.</strong> Guardrails scan email content (no PII leaks?)<br>
                            <strong>8.</strong> Agent calls Tool: <code>send_email(to="ceo@...", attachments=[chart])</code><br>
                            <strong>9.</strong> Metrics logged: 3 tools, 2 LLM calls, 850ms latency, $0.04 cost<br>
                            <strong>10.</strong> Response to User: "Sales report sent! Revenue: $125K (+12% vs last month)"<br><br>
                            <strong>Components used:</strong> 8 different systems, 15+ services, all orchestrated by your backend code.</p>
                        </div>
                    </div>
                </div>

                <h3>Component Summary</h3>
                <div class="ai-grid">
                    <div class="ai-card">
                        <h4>LLMs</h4>
                        <p>Black box APIs you integrate with. GPT-4, Claude, Llama accessed via REST APIs with streaming.</p>
                    </div>
                    <div class="ai-card">
                        <h4>Embeddings</h4>
                        <p>Convert text to vectors for similarity. OpenAI, Cohere APIs. Store in vector DB.</p>
                    </div>
                    <div class="ai-card">
                        <h4>RAG</h4>
                        <p>LLM + external knowledge. 80% backend data pipeline: ingest, chunk, embed, search.</p>
                    </div>
                    <div class="ai-card">
                        <h4>Vector Search</h4>
                        <p>Find similar content by embeddings. Pinecone, Weaviate, pgvector with hybrid search.</p>
                    </div>
                    <div class="ai-card">
                        <h4>Tool Calling</h4>
                        <p>LLMs execute your backend functions. 100% backend work: schema, validation, execution.</p>
                    </div>
                    <div class="ai-card">
                        <h4>Agents</h4>
                        <p>Autonomous systems with tools + memory. State machines, memory management, safety rails.</p>
                    </div>
                </div>

                <div class="ai-highlight">
                    <strong>Final Takeaway:</strong> LLMs are small black boxes you integrate via APIs. Everything else - RAG pipelines, vector search, function calling, agents, routing, guardrails, observability, orchestration - is backend engineering. Your existing skills in APIs, databases, caching, queues, and infrastructure directly apply. The AI era doesn't replace backend engineers - it needs them more than ever.
                </div>
            </div>
        </div>
    </div>

    <aside class="page-sidebar">
        <div class="panel">
            <div class="panel-header">Key Concepts</div>
            <div class="quick-ref-item"><span class="quick-ref-key">LLM</span><span class="quick-ref-value">Large Language Model</span></div>
            <div class="quick-ref-item"><span class="quick-ref-key">RAG</span><span class="quick-ref-value">Retrieval-Augmented Generation</span></div>
            <div class="quick-ref-item"><span class="quick-ref-key">Embeddings</span><span class="quick-ref-value">Text to Vectors</span></div>
            <div class="quick-ref-item"><span class="quick-ref-key">Vector DB</span><span class="quick-ref-value">Similarity Search</span></div>
        </div>
        <div class="panel mt-4">
            <div class="panel-header">Popular Tools</div>
            <div class="quick-ref-item"><span class="quick-ref-key">Pinecone</span><span class="quick-ref-value">Vector DB</span></div>
            <div class="quick-ref-item"><span class="quick-ref-key">LangChain</span><span class="quick-ref-value">LLM Framework</span></div>
            <div class="quick-ref-item"><span class="quick-ref-key">OpenAI</span><span class="quick-ref-value">LLM Provider</span></div>
            <div class="quick-ref-item"><span class="quick-ref-key">Redis</span><span class="quick-ref-value">Cache/Memory</span></div>
        </div>
        <div class="panel mt-4">
            <div class="panel-header">Cost Factors</div>
            <div class="quick-ref-item"><span class="quick-ref-key">Input Tokens</span><span class="quick-ref-value">$0.01/1K</span></div>
            <div class="quick-ref-item"><span class="quick-ref-key">Output Tokens</span><span class="quick-ref-value">$0.03/1K</span></div>
            <div class="quick-ref-item"><span class="quick-ref-key">Embeddings</span><span class="quick-ref-value">$0.0001/1K</span></div>
        </div>
    </aside>
</div>

<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
    // Initialize Mermaid with responsive sizing
    const isMobile = window.innerWidth < 768;
    const isTablet = window.innerWidth >= 768 && window.innerWidth < 1024;

    mermaid.initialize({
        startOnLoad: true,
        theme: 'base',
        themeVariables: {
            primaryColor: '#3b82f6',
            primaryTextColor: '#1e293b',
            primaryBorderColor: '#3b82f6',
            lineColor: '#475569',
            secondaryColor: '#8b5cf6',
            tertiaryColor: '#10b981',
            fontFamily: '-apple-system, BlinkMacSystemFont, Segoe UI, Roboto, sans-serif',
            fontSize: isMobile ? '12px' : isTablet ? '15px' : '16px'
        },
        flowchart: {
            useMaxWidth: true,
            htmlLabels: true,
            curve: 'basis'
        },
        sequence: {
            useMaxWidth: true,
            wrap: true,
            width: isMobile ? 300 : isTablet ? 500 : 800
        }
    });

    // Tab switching with Mermaid re-initialization
    document.querySelectorAll('.ai-tab-btn').forEach(btn => {
        btn.addEventListener('click', () => {
            const tabId = btn.getAttribute('data-tab');

            document.querySelectorAll('.ai-tab-btn').forEach(b => b.classList.remove('active'));
            document.querySelectorAll('.ai-tab-content').forEach(c => c.classList.remove('active'));

            btn.classList.add('active');
            const activeTab = document.getElementById(tabId);
            activeTab.classList.add('active');

            // Re-render Mermaid diagrams in the newly active tab
            setTimeout(() => {
                const mermaidDiagrams = activeTab.querySelectorAll('.mermaid');
                mermaidDiagrams.forEach((diagram, index) => {
                    // Only re-render if diagram hasn't been rendered yet
                    if (!diagram.getAttribute('data-processed')) {
                        mermaid.run({
                            nodes: [diagram]
                        });
                    }
                });
            }, 100);
        });
    });

    // Collapsible functionality
    document.querySelectorAll('.collapsible-btn').forEach(btn => {
        btn.addEventListener('click', function() {
            this.classList.toggle('active');
            const content = this.nextElementSibling;
            if (content.style.maxHeight) {
                content.style.maxHeight = null;
            } else {
                content.style.maxHeight = content.scrollHeight + 'px';
            }
        });
    });
</script>
