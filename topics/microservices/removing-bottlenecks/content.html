<h1 id="removing-bottlenecks-in-distributed-systems">Removing Bottlenecks in Distributed Systems</h1>
<h2 id="overview">Overview</h2>
<p>Bottleneck removal is the systematic practice of identifying and eliminating constraints that limit system throughput. Unlike superficial optimizations, effective bottleneck removal requires understanding the internal mechanics of your entire stack - from CPU cache behavior to database lock contention, from network buffer sizes to garbage collector pauses.</p>
<p><strong>Tags:</strong> Performance, Bottlenecks, Profiling, Scaling, Caching, Async</p>
<hr />
<h2 id="section-1-profiling-techniques-and-bottleneck-identification">Section 1: Profiling Techniques and Bottleneck Identification</h2>
<p>Understanding where time is spent before optimizing is fundamental. Premature optimization without profiling leads to wasted effort on non-critical paths while real bottlenecks persist.</p>
<div>
<h3>PROFILING HIERARCHY</h3>
<div>
<div>
<div>L1: Distributed Tracing</div>
<div>
  Cross-service latency, request flow visualization, span analysis
</div>
<div>
<span>Tools: Jaeger, Zipkin, AWS X-Ray</span>
</div>
</div>
<div>
<div>L2: Application Profiling</div>
<div>
  CPU flame graphs, memory allocation, GC pressure, lock contention
</div>
<div>
<span>Tools: pprof, async-profiler, perf</span>
</div>
</div>
<div>
<div>L3: Database Profiling</div>
<div>
  Query execution plans, lock wait analysis, I/O statistics
</div>
<div>
<span>Tools: EXPLAIN ANALYZE, pg_stat_statements</span>
</div>
</div>
<div>
<div>L4: System Profiling</div>
<div>
  Kernel syscalls, network stack, disk I/O, CPU cache misses
</div>
<div>
<span>Tools: eBPF, strace, iostat, vmstat</span>
</div>
</div>
</div>
</div>
<h3 id="internal-mechanism-cpu-flame-graphs">Internal Mechanism: CPU Flame Graphs</h3>
<p>Flame graphs visualize stack traces where the x-axis represents the proportion of time spent and the y-axis shows call stack depth. The <strong>key insight</strong> is that wide plateaus indicate hot paths - functions where the program spends significant time.</p>
<pre><code class="language-go">// Enable continuous profiling in production
import _ &quot;net/http/pprof&quot;

func main() {
    // pprof endpoints available at /debug/pprof/
    // CPU profile: /debug/pprof/profile?seconds=30
    // Heap profile: /debug/pprof/heap
    // Goroutine profile: /debug/pprof/goroutine
    // Mutex contention: /debug/pprof/mutex
    // Block profile: /debug/pprof/block

    go func() {
        log.Println(http.ListenAndServe(&quot;localhost:6060&quot;, nil))
    }()
}
</code></pre>
<div>
<div>CRITICAL INSIGHT: The USE Method</div>
<div>
For every resource, check: <strong>Utilization</strong> (percentage of time busy), <strong>Saturation</strong> (queue depth when busy), and <strong>Errors</strong> (count of error events). This systematic approach prevents overlooking subtle bottlenecks.
</div>
<div>
<div>
<div>Utilization</div>
<div>CPU at 95% = obvious bottleneck</div>
</div>
<div>
<div>Saturation</div>
<div>Run queue> CPU cores = hidden bottleneck</div>
</div>
<div>
<div>Errors</div>
<div>Timeout errors = downstream bottleneck</div>
</div>
</div>
</div>
<h3 id="edge-case-coordinated-omission">Edge Case: Coordinated Omission</h3>
<p>Most load testing tools suffer from <strong>coordinated omission</strong> - when a slow response delays the next request, the tool under-reports latency. If your server takes 1 second to respond but you're sending requests every 100ms, the tool misses the 9 requests that would have been waiting.</p>
<pre><code class="language-python"># BAD: Naive load testing (suffers from coordinated omission)
for i in range(1000):
    start = time.time()
    response = requests.get(url)
    latency = time.time() - start  # WRONG: doesn't account for waiting time

# GOOD: Corrected measurement
scheduled_time = time.time()
for i in range(1000):
    actual_start = time.time()
    wait_time = actual_start - scheduled_time  # Time spent waiting
    response = requests.get(url)
    service_time = time.time() - actual_start
    total_latency = wait_time + service_time  # TRUE latency
    scheduled_time += 0.001  # Next request should have started 1ms later
</code></pre>
<h3 id="interview-deep-dive-profiling-techniques">Interview Deep-Dive: Profiling Techniques</h3>
<div>
<h4>Level 1: How would you identify which microservice is the bottleneck in a request that spans 10 services?</h4>
<div>
<div>Answer:</div>
<div>
  Use distributed tracing with span analysis. Each service creates a span with start/end timestamps. The bottleneck is the span with the longest duration that's on the critical path. Look for spans where self-time (total time minus child spans) is high - this isolates actual processing time from waiting on downstream services.
</div>
</div>
<h4>Level 2: The tracing shows Service B takes 500ms, but CPU utilization is only 5%. What's happening and how do you diagnose further?</h4>
<div>
<div>Answer:</div>
<div>
Low CPU with high latency indicates the service is waiting, not computing. Possible causes: (1) I/O wait - database queries, network calls, disk reads; (2) Lock contention - threads blocked on mutexes; (3) Thread pool exhaustion - all workers busy, requests queued. Diagnose with: <code>pprof block profile</code> for lock contention, <code>pprof mutex profile</code> for mutex waits, and trace child spans to identify slow downstream calls. Also check connection pool metrics - if at max connections, requests queue waiting for available connections.
</div>
</div>
<h4>Level 3: The block profile shows threads waiting on a channel receive. The channel is fed by a goroutine making database calls. How do you determine if the database is the root cause or if there's a producer-consumer imbalance?</h4>
<div>
<div>Answer:</div>
<div>
Measure both independently. For the database: check <code>pg_stat_activity</code> for active queries, <code>pg_stat_statements</code> for query timing, and connection wait events. For producer-consumer imbalance: add metrics for channel buffer utilization over time. If the channel is frequently empty (consumers starving), the producer (DB goroutine) is too slow. If the channel is frequently full (producers blocking), consumers are too slow. The fix differs: slow DB requires query optimization or read replicas; slow consumers need more worker goroutines. A true database bottleneck will show in DB-side metrics (high active_time, lock waits, I/O waits).
</div>
</div>
</div>
<hr />
<h2 id="section-2-database-bottlenecks">Section 2: Database Bottlenecks</h2>
<p>Databases are the most common bottleneck in microservices because they're stateful, shared, and bound by fundamental I/O constraints. Understanding database internals is essential for effective optimization.</p>
<div>
<h3>DATABASE BOTTLENECK TAXONOMY</h3>
<div>
<div>
<div>Connection Exhaustion</div>
<div>
  Each connection consumes ~10MB RAM (PostgreSQL) for work_mem, sort buffers, and connection state. With 100 microservice instances each opening 20 connections, you need 2000 database connections.
</div>
<div>
<div><strong>Solution:</strong> Connection poolers like PgBouncer in transaction mode multiplex many client connections over fewer database connections.</div>
</div>
</div>
<div>
<div>Lock Contention</div>
<div>
  Row-level locks during UPDATE cause transactions to wait. Hot rows (popular products, counters) become serialization points. Even SELECT can block if it needs a share lock conflicting with an exclusive lock.
</div>
<div>
<div><strong>Solution:</strong> Optimistic locking with version columns, or sharding hot data across multiple rows.</div>
</div>
</div>
<div>
<div>Query Plan Degradation</div>
<div>
  Statistics become stale as data distribution changes. The planner chooses sequential scan over index scan because it believes the table is smaller than it actually is. ANALYZE frequency matters.
</div>
<div>
<div><strong>Solution:</strong> Automated ANALYZE schedules, query plan monitoring, plan baselines (Oracle) or pg_hint_plan.</div>
</div>
</div>
<div>
<div>Write Amplification</div>
<div>
  B-tree index updates during INSERT cause page splits. A single row insert might trigger multiple page writes across multiple indexes. Wide rows with many indexes are particularly expensive.
</div>
<div>
<div><strong>Solution:</strong> Careful index design, partial indexes, periodic REINDEX to reclaim space.</div>
</div>
</div>
</div>
</div>
<h3 id="internal-mechanism-the-n1-query-problem-at-scale">Internal Mechanism: The N+1 Query Problem at Scale</h3>
<p>The N+1 problem occurs when code fetches a parent record, then makes N separate queries for related records. At small scale (N=10), this adds 50-100ms. At production scale (N=1000), this becomes 5-10 seconds and can saturate connection pools.</p>
<pre><code class="language-go">// PROBLEMATIC: N+1 queries
func GetOrdersWithItems(userID string) ([]Order, error) {
    orders, _ := db.Query(&quot;SELECT * FROM orders WHERE user_id = $1&quot;, userID)

    for _, order := range orders {
        // This executes once per order - N additional queries!
        items, _ := db.Query(&quot;SELECT * FROM order_items WHERE order_id = $1&quot;, order.ID)
        order.Items = items
    }
    return orders, nil
}

// OPTIMIZED: Single query with JOIN or IN clause
func GetOrdersWithItemsOptimized(userID string) ([]Order, error) {
    rows, _ := db.Query(`
        SELECT o.*, oi.*
        FROM orders o
        LEFT JOIN order_items oi ON o.id = oi.order_id
        WHERE o.user_id = $1
    `, userID)

    // Or use DataLoader pattern for batching
    orderIDs := extractOrderIDs(orders)
    items, _ := db.Query(&quot;SELECT * FROM order_items WHERE order_id = ANY($1)&quot;, orderIDs)
    // Group items by order_id and attach
}
</code></pre>
<div>
<div>ASSUMPTION CHECK: Read Replicas Solve All Read Bottlenecks</div>
<div>
<strong>False.</strong> Read replicas introduce replication lag (typically 10ms-1s, but can spike to minutes during load). This creates read-after-write inconsistency: a user updates their profile, immediately reads it back, and sees stale data because their read went to a replica that hasn't received the write yet.
</div>
<div>
<div>Mitigation Strategies:</div>
<ul>
<li><strong>Session consistency:</strong> Route user's reads to primary for X seconds after their write</li>
<li><strong>Causal consistency:</strong> Track LSN (Log Sequence Number) of last write, wait for replica to catch up</li>
<li><strong>Monotonic reads:</strong> Sticky sessions to same replica within a request</li>
</ul>
</div>
</div>
<h3 id="connection-pooling-deep-dive">Connection Pooling Deep Dive</h3>
<p>Connection pools prevent the overhead of establishing new database connections (TCP handshake, TLS negotiation, authentication - typically 20-50ms). However, pool configuration is nuanced.</p>
<pre><code class="language-go">// Production-grade connection pool configuration
db, err := sql.Open(&quot;postgres&quot;, connStr)
db.SetMaxOpenConns(25)          // Maximum connections in pool
db.SetMaxIdleConns(10)          // Connections kept idle for reuse
db.SetConnMaxLifetime(5 * time.Minute)  // Recycle connections (important for load balancers)
db.SetConnMaxIdleTime(1 * time.Minute)  // Close idle connections

// Formula for MaxOpenConns:
// connections = (core_count * 2) + effective_spindle_count
// For SSD: effective_spindle_count is typically 1 (but very fast)
// For cloud: start with 10-25, load test to find saturation point
</code></pre>
<div>
<div>TRADE-OFF: Pool Size vs. Queue Wait Time</div>
<div>
<div>
<div>Too Few Connections</div>
<div>Requests queue waiting for available connection. Latency spikes under load. Pool utilization hits 100%.</div>
</div>
<div>
<div>Too Many Connections</div>
<div>Database memory exhaustion. Context switching overhead. Diminishing returns past 50-100 connections per core.</div>
</div>
</div>
</div>
<h3 id="interview-deep-dive-database-bottlenecks">Interview Deep-Dive: Database Bottlenecks</h3>
<div>
<h4>Level 1: Your application's p99 latency spikes to 2 seconds during peak hours. Database CPU is at 30%. What's your diagnosis approach?</h4>
<div>
<div>Answer:</div>
<div>
Low CPU with high latency suggests the database is waiting, not computing. Check in order: (1) <code>pg_stat_activity</code> for wait_event_type - look for "Lock", "IO", "Client" waits; (2) Connection pool metrics - are connections exhausted, causing application-level queuing?; (3) <code>pg_locks</code> for lock contention between transactions; (4) I/O wait times with <code>pg_stat_io</code>. The p99 spike (not p50) suggests specific queries or lock patterns, not general overload.
</div>
</div>
<h4>Level 2: You find heavy lock waits on a "balances" table. Multiple services update user balances concurrently. How do you reduce contention without changing the schema?</h4>
<div>
<div>Answer:</div>
<div>
Several approaches: (1) <strong>SELECT FOR UPDATE SKIP LOCKED</strong> - workers grab unlocked rows, skip contested ones; (2) <strong>Optimistic locking</strong> with version column - read version, update with WHERE version = X, retry on conflict; (3) <strong>Advisory locks</strong> on user_id to serialize at application level with shorter hold time; (4) <strong>Batch updates</strong> - aggregate balance changes in Redis, flush to database periodically; (5) <strong>NOWAIT option</strong> - fail fast rather than wait, let application retry with backoff. For balance-specific case, consider event sourcing - append balance change events, compute current balance on read (see [[event-sourcing]](/system-design/event-sourcing)).
</div>
</div>
<h4>Level 3: You implement optimistic locking but now see high retry rates (30%) under load. The retries are causing more contention. How do you break this cycle?</h4>
<div>
<div>Answer:</div>
<div>
High retry rates indicate fundamental contention that optimistic locking can't solve - you're just moving the serialization point. Solutions: (1) <strong>Exponential backoff with jitter</strong> - prevents retry storms by spreading retries over time; (2) <strong>Pre-partitioned locks</strong> - hash user_id to one of N lock buckets, reducing collision probability from 100% to 1/N; (3) <strong>Fundamentally change the model</strong> - for balances, use credit/debit ledger entries (append-only, no contention) and compute balance as SUM(). Reads are slower but writes never conflict; (4) <strong>CQRS pattern</strong> - separate write model (event log) from read model (materialized balance). The retry spiral indicates the data model doesn't match the access pattern.
</div>
</div>
</div>
<hr />
<h2 id="section-3-asynchronous-processing">Section 3: Asynchronous Processing</h2>
<p>Converting synchronous operations to asynchronous decouples producers from consumers, enabling independent scaling and improved response times. However, async introduces complexity: delivery guarantees, ordering, idempotency, and observability challenges.</p>
<div>
<h3>SYNC vs ASYNC PROCESSING FLOW</h3>
<div>
<div>
<div>Synchronous (Blocking)</div>
<div>
<div>Client Request</div>
<div>|</div>
<div>Order Service</div>
<div>|</div>
<div>Payment Service (500ms)</div>
<div>|</div>
<div>Email Service (1000ms)</div>
<div>|</div>
<div>Analytics (200ms)</div>
</div>
<div>
<div>Total: 1700ms blocking</div>
<div>Client waits for everything</div>
</div>
</div>
<div>
<div>Asynchronous (Non-blocking)</div>
<div>
<div>Client Request</div>
<div>|</div>
<div>Order Service</div>
<div>
<div>|</div>
<div>Publish Event</div>
</div>
<div>
<div>Message Queue</div>
</div>
<div>
<div>Payment</div>
<div>Email</div>
<div>Analytics</div>
</div>
<div>(process independently)</div>
</div>
<div>
<div>Response: 50ms</div>
<div>Background processing continues</div>
</div>
</div>
</div>
</div>
<h3 id="internal-mechanism-message-queue-delivery-guarantees">Internal Mechanism: Message Queue Delivery Guarantees</h3>
<p>Understanding delivery semantics is crucial for correctness:</p>
<pre><code>At-Most-Once:  Fire and forget. Fast but messages may be lost.
               Use for: metrics, logs, non-critical notifications

At-Least-Once: Retry until acknowledged. Messages may be duplicated.
               Use for: most business events (with idempotent consumers)

Exactly-Once:  Each message processed exactly once. Complex and expensive.
               Use for: financial transactions (but consider idempotency instead)
</code></pre>
<pre><code class="language-go">// At-least-once consumer with idempotency
func ProcessOrderEvent(ctx context.Context, event OrderEvent) error {
    // Idempotency key prevents duplicate processing
    processed, err := redis.SetNX(ctx,
        fmt.Sprintf(&quot;processed:%s&quot;, event.EventID),
        &quot;1&quot;,
        24*time.Hour,
    )
    if err != nil {
        return err
    }
    if !processed {
        // Already processed this event, skip
        log.Info(&quot;Skipping duplicate event&quot;, &quot;eventID&quot;, event.EventID)
        return nil
    }

    // Process the event
    if err := processOrder(event); err != nil {
        // Delete idempotency key so retry can occur
        redis.Del(ctx, fmt.Sprintf(&quot;processed:%s&quot;, event.EventID))
        return err
    }

    return nil
}
</code></pre>
<div>
<div>DESIGN CHOICE: Transactional Outbox Pattern</div>
<div>
    Publishing to a message queue after database commit creates a dual-write problem: the database write succeeds but queue publish fails, leaving the system inconsistent. The Transactional Outbox solves this by writing events to an outbox table in the same database transaction, then a separate process polls and publishes.
</div>
<div>
<pre>
  BEGIN TRANSACTION;
  INSERT INTO orders (id, user_id, total) VALUES (...);
  INSERT INTO outbox (aggregate_id, event_type, payload)
  VALUES (order_id, 'OrderCreated', '{"orderId": "..."}');
  COMMIT;
-- Separate CDC process or poller reads outbox and publishes to Kafka</pre>
</div>
</div>
<h3 id="edge-case-consumer-lag-and-backpressure">Edge Case: Consumer Lag and Backpressure</h3>
<p>When consumers can't keep up with producers, queue depth grows unbounded. This causes: increased end-to-end latency, memory pressure on brokers, potential message expiration.</p>
<pre><code class="language-go">// Implementing backpressure with bounded concurrency
func StartConsumer(ctx context.Context, queue Queue) {
    // Semaphore limits concurrent processing
    sem := make(chan struct{}, 100) // Max 100 concurrent

    for {
        select {
        case &lt;-ctx.Done():
            return
        case sem &lt;- struct{}{}: // Acquire semaphore
            msg, err := queue.Receive(ctx)
            if err != nil {
                &lt;-sem // Release on error
                continue
            }

            go func(m Message) {
                defer func() { &lt;-sem }() // Release when done
                processMessage(m)
            }(msg)
        }
    }
}

// Monitoring consumer lag
// Kafka: consumer_group_lag = log_end_offset - consumer_offset
// Action: Alert when lag &gt; threshold, scale consumers
</code></pre>
<h3 id="interview-deep-dive-async-processing">Interview Deep-Dive: Async Processing</h3>
<div>
<h4>Level 1: When should you NOT use async processing even though it would improve response time?</h4>
<div>
<div>Answer:</div>
<div>
Avoid async when: (1) <strong>User expects immediate feedback</strong> - payment confirmation, real-time validation; (2) <strong>Ordering is critical and complex</strong> - async makes ordering guarantees harder; (3) <strong>Debugging/observability is paramount</strong> - distributed traces across async boundaries are harder to follow; (4) <strong>System is already simple</strong> - async adds operational complexity (DLQs, retry logic, idempotency); (5) <strong>Strong consistency required</strong> - read-after-write must see the update. The complexity cost must be justified by the latency or decoupling benefits.
</div>
</div>
<h4>Level 2: Your async email service processes messages from Kafka but sometimes sends duplicate emails. The idempotency check uses Redis but still fails. Why?</h4>
<div>
<div>Answer:</div>
<div>
Several failure modes: (1) <strong>Check-then-act race condition</strong> - if using GET then SET instead of atomic SETNX, two consumers can pass the check simultaneously; (2) <strong>Redis key expiration</strong> - if TTL expires before Kafka consumer commits offset, reprocessing occurs after TTL; (3) <strong>Consumer crash after email sent, before commit</strong> - Kafka redelivers message, but Redis key exists so... wait, actually this should work. Unless: (4) <strong>Redis is separate from email send</strong> - if Redis SET succeeds, email send fails, consumer crashes, Redis key remains but email never sent. On retry, idempotency check blocks the retry. Need: atomic "check, process, mark" or compensation. Use Redis transaction: check key, if not exists mark as "processing", send email, mark as "complete".
</div>
</div>
<h4>Level 3: You fix the idempotency issue but now have a new problem: some emails are never sent because the "processing" state gets stuck when workers crash. How do you implement reliable exactly-once email delivery?</h4>
<div>
<div>Answer:</div>
<div>
  Implement a state machine with timeout-based recovery: (1) States: PENDING -> PROCESSING -> SENT or FAILED; (2) When claiming a message, set state to PROCESSING with a claim_until timestamp (now + 5 minutes); (3) Background reaper queries for messages WHERE state = 'PROCESSING' AND claim_until < NOW(), resets them to PENDING for retry; (4) After successful send, atomically set state to SENT; (5) For the email service specifically, check with the email provider's API if the message was actually sent (using a client-generated message ID) before retrying - this handles "send succeeded but ack failed" cases. This is essentially implementing a transactional outbox (see [[transactional-outbox]](/microservices/patterns)) with lease-based claiming.
</div>
</div>
</div>
<hr />
<h2 id="section-4-caching-layers">Section 4: Caching Layers</h2>
<p>Caching reduces latency and backend load by storing computed results closer to the requester. However, caches introduce consistency challenges, cache stampedes, and memory pressure. Effective caching requires understanding invalidation strategies, eviction policies, and failure modes.</p>
<div>
<h3>MULTI-TIER CACHE ARCHITECTURE</h3>
<div>
<div>
<div>
<div>Client</div>
<div>Browser/App</div>
</div>
<div>-></div>
<div>
<div>L1: CDN</div>
<div>Edge Cache</div>
<div>~10ms latency</div>
</div>
<div>-></div>
<div>
<div>L2: Gateway</div>
<div>API Cache</div>
<div>~25ms latency</div>
</div>
</div>
<div>
<div>-></div>
<div>
<div>L3: App</div>
<div>In-Process</div>
<div>~1ms latency</div>
</div>
<div>-></div>
<div>
<div>L4: Redis</div>
<div>Distributed</div>
<div>~5ms latency</div>
</div>
<div>-></div>
<div>
<div>Database</div>
<div>Source of Truth</div>
<div>~50ms latency</div>
</div>
</div>
</div>
<div>
<div>
<div>CDN</div>
<div>Static assets, public API responses</div>
</div>
<div>
<div>Gateway</div>
<div>Auth tokens, rate limit state</div>
</div>
<div>
<div>In-Process</div>
<div>Hot config, compiled regexes</div>
</div>
<div>
<div>Redis</div>
<div>Session, user data, query results</div>
</div>
</div>
</div>
<h3 id="internal-mechanism-cache-stampede-prevention">Internal Mechanism: Cache Stampede Prevention</h3>
<p>When a popular cache entry expires, multiple concurrent requests hit the database simultaneously - a cache stampede. This can overload the database exactly when the cache is supposed to protect it.</p>
<pre><code class="language-go">// PROBLEMATIC: Simple cache-aside allows stampede
func GetProduct(ctx context.Context, id string) (*Product, error) {
    cached, err := cache.Get(ctx, &quot;product:&quot;+id)
    if err == nil {
        return cached, nil
    }

    // Cache miss - ALL concurrent requests hit database
    product, err := db.GetProduct(ctx, id)
    if err != nil {
        return nil, err
    }

    cache.Set(ctx, &quot;product:&quot;+id, product, 5*time.Minute)
    return product, nil
}

// SOLUTION 1: Singleflight - coalesce concurrent requests
var group singleflight.Group

func GetProductWithSingleflight(ctx context.Context, id string) (*Product, error) {
    cached, err := cache.Get(ctx, &quot;product:&quot;+id)
    if err == nil {
        return cached, nil
    }

    // Only ONE request hits database, others wait and share result
    result, err, _ := group.Do(&quot;product:&quot;+id, func() (interface{}, error) {
        product, err := db.GetProduct(ctx, id)
        if err != nil {
            return nil, err
        }
        cache.Set(ctx, &quot;product:&quot;+id, product, 5*time.Minute)
        return product, nil
    })

    if err != nil {
        return nil, err
    }
    return result.(*Product), nil
}

// SOLUTION 2: Probabilistic early expiration
func GetProductWithEarlyExpiry(ctx context.Context, id string) (*Product, error) {
    cached, ttl, err := cache.GetWithTTL(ctx, &quot;product:&quot;+id)
    if err == nil {
        // Probabilistically refresh before expiry
        // As TTL approaches 0, probability of refresh increases
        // Formula: probability = 1 - (ttl / original_ttl)
        if rand.Float64() &gt; float64(ttl)/float64(5*time.Minute) {
            go refreshCache(ctx, id) // Background refresh
        }
        return cached, nil
    }
    // ... fallback to database
}
</code></pre>
<div>
<div>TRADE-OFF: Cache Invalidation Strategies</div>
<div>
<div>
<div>Time-Based (TTL)</div>
<div>Simple, eventual consistency. Data stale until expiry.</div>
<div>Use when: Staleness is acceptable (product catalog, user profiles)</div>
</div>
<div>
<div>Event-Based Invalidation</div>
<div>Near-real-time. Complex dependency tracking required.</div>
<div>Use when: Updates are infrequent but must be visible quickly</div>
</div>
<div>
<div>Write-Through</div>
<div>Cache always consistent. Write latency increased.</div>
<div>Use when: Read-heavy with occasional writes, consistency critical</div>
</div>
<div>
<div>Cache-Aside + Version</div>
<div>Key includes version: "user:123:v5". Increment version on update.</div>
<div>Use when: Avoiding explicit invalidation, tolerating orphaned keys</div>
</div>
</div>
</div>
<h3 id="edge-case-cache-and-database-consistency">Edge Case: Cache and Database Consistency</h3>
<p>The cache-aside pattern has a subtle race condition:</p>
<pre><code>Timeline:
T1: Request A reads value=100 from database
T2: Request B updates value=200 in database
T3: Request B invalidates cache (cache now empty)
T4: Request A writes value=100 to cache (STALE!)

Result: Cache has 100, database has 200 - indefinite inconsistency until TTL
</code></pre>
<pre><code class="language-go">// MITIGATION: Use versioning or compare-and-set
func SetCacheIfFresh(ctx context.Context, key string, value interface{}, version int64) error {
    // Only set if our version is current
    script := `
        local current_version = redis.call('HGET', KEYS[1], 'version')
        if current_version == false or tonumber(current_version) &lt; tonumber(ARGV[2]) then
            redis.call('HSET', KEYS[1], 'data', ARGV[1], 'version', ARGV[2])
            redis.call('EXPIRE', KEYS[1], ARGV[3])
            return 1
        end
        return 0
    `
    return redis.Eval(ctx, script, []string{key}, value, version, ttlSeconds)
}
</code></pre>
<h3 id="interview-deep-dive-caching-layers">Interview Deep-Dive: Caching Layers</h3>
<div>
<h4>Level 1: Your cache hit rate is 95% but p99 latency is still high. How is this possible?</h4>
<div>
<div>Answer:</div>
<div>
P99 is dominated by the 5% of cache misses, not the 95% hits. If cache hits take 5ms and misses take 500ms, p99 is closer to 500ms because 1 in 20 requests is a miss. Additionally: (1) <strong>Hot key problem</strong> - specific keys are accessed so frequently that Redis itself becomes bottleneck; (2) <strong>Large values</strong> - serialization/deserialization time for large cached objects; (3) <strong>Network latency variance</strong> - even to Redis, network can have p99 spikes; (4) <strong>GC pauses</strong> - in-process caches can cause GC pressure. Check if misses correlate with specific keys or patterns.
</div>
</div>
<h4>Level 2: You identify a hot key (celebrity user profile viewed 100K times/second). How do you cache this effectively?</h4>
<div>
<div>Answer:</div>
<div>
Multiple strategies: (1) <strong>Local caching</strong> - each app instance caches hot keys in-memory (Caffeine, Guava). 100 instances each handling 1K/s is easier than Redis handling 100K/s; (2) <strong>Key replication</strong> - spread the key across multiple Redis keys (user:123:shard:0 through user:123:shard:9), randomly select on read. 10x the memory but 10x the throughput; (3) <strong>Read replicas</strong> - distribute reads across multiple Redis replicas; (4) <strong>CDN caching</strong> - if the data is public, push to edge. Combine: local cache (1ms, 80% hit) -> Redis (5ms, 19% hit) -> database (1% miss). For a celebrity profile that changes rarely, aggressive TTL (5min) with event-based invalidation works well.
</div>
</div>
<h4>Level 3: You implement local caching but now profile updates take minutes to propagate to all instances. Business requires updates visible within 5 seconds. How do you solve this without losing the local cache benefit?</h4>
<div>
<div>Answer:</div>
<div>
  Implement pub/sub invalidation: (1) When profile updates, publish invalidation event to Redis Pub/Sub channel; (2) Each app instance subscribes to this channel; (3) On receiving invalidation, evict from local cache; (4) Next request fetches fresh data, repopulates local cache. This gives you local cache speed with near-real-time invalidation. For 5-second guarantee: (1) Short local TTL (30s) as backup; (2) Monitor pub/sub lag; (3) Consider using Redis Streams instead of Pub/Sub for persistence (handles instance restarts). Alternative: use a two-tier TTL - local cache 30s, Redis 5min. Local always checks Redis version before using cached value. If version changed, invalidate. This is more network calls but simpler operationally than pub/sub.
</div>
</div>
</div>
<hr />
<h2 id="section-5-horizontal-scaling">Section 5: Horizontal Scaling</h2>
<p>Horizontal scaling adds more instances to handle increased load, as opposed to vertical scaling (bigger machines). It's the foundation of cloud-native architectures but requires stateless design and careful consideration of data consistency.</p>
<div>
<h3>HORIZONTAL SCALING PREREQUISITES</h3>
<div>
<div>
<div>Stateless Services</div>
<div>
  No request should depend on state from a previous request being on the same instance. Session, cache, and file storage must be externalized.
</div>
<div>
<div><strong>Move to:</strong> Redis for sessions, S3 for files, external cache</div>
</div>
</div>
<div>
<div>Idempotent Operations</div>
<div>
  Requests may be retried due to timeouts or load balancer failover. The same request executed twice must produce the same result.
</div>
<div>
<div><strong>Implement:</strong> Idempotency keys, upserts instead of inserts</div>
</div>
</div>
<div>
<div>Graceful Shutdown</div>
<div>
  Instances are added/removed dynamically. In-flight requests must complete before termination. Health checks must accurately reflect readiness.
</div>
<div>
<div><strong>Handle:</strong> SIGTERM, drain connections, fail health checks</div>
</div>
</div>
</div>
</div>
<h3 id="internal-mechanism-auto-scaling-decisions">Internal Mechanism: Auto-Scaling Decisions</h3>
<p>Auto-scalers must balance responsiveness (scaling up before users experience degradation) with stability (avoiding oscillation from rapid scale up/down cycles).</p>
<pre><code class="language-yaml"># Kubernetes HPA with multiple metrics and behavior controls
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: order-service
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: order-service
  minReplicas: 3
  maxReplicas: 100
  metrics:
  # Primary: CPU utilization
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  # Secondary: Request latency from Prometheus
  - type: External
    external:
      metric:
        name: http_request_duration_seconds
        selector:
          matchLabels:
            service: order-service
            quantile: &quot;0.99&quot;
      target:
        type: Value
        value: 500m  # 500ms p99 target
  # Custom: Queue depth
  - type: External
    external:
      metric:
        name: kafka_consumer_lag
        selector:
          matchLabels:
            consumer_group: order-processor
      target:
        type: AverageValue
        averageValue: &quot;1000&quot;
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # Wait 5 min before scaling down
      policies:
      - type: Percent
        value: 10  # Remove max 10% of pods per period
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0  # Scale up immediately
      policies:
      - type: Percent
        value: 100  # Can double pods per period
        periodSeconds: 15
      - type: Pods
        value: 4  # Or add 4 pods, whichever is greater
        periodSeconds: 15
      selectPolicy: Max
</code></pre>
<div>
<div>DESIGN CHOICE: Scaling Metrics Selection</div>
<div>
    CPU utilization is a lagging indicator - by the time CPU is high, users are already experiencing latency. Better metrics are leading indicators of saturation.
</div>
<div>
<div>
<div>Lagging Indicators</div>
<ul>
<li>CPU/Memory utilization</li>
<li>Error rate spikes</li>
<li>P99 latency exceeded</li>
</ul>
</div>
<div>
<div>Leading Indicators</div>
<ul>
<li>Queue depth / consumer lag</li>
<li>Connection pool utilization</li>
<li>Request rate trend (derivative)</li>
</ul>
</div>
</div>
</div>
<h3 id="edge-case-cold-start-and-connection-pool-exhaustion">Edge Case: Cold Start and Connection Pool Exhaustion</h3>
<p>When new instances start, they need time to warm up: JIT compilation, cache population, connection pool establishment. During this period, they're slower than existing instances.</p>
<pre><code class="language-go">// Implement readiness probe that checks actual readiness
func readinessHandler(w http.ResponseWriter, r *http.Request) {
    // Check database connection pool is healthy
    if err := db.Ping(); err != nil {
        http.Error(w, &quot;Database not ready&quot;, http.StatusServiceUnavailable)
        return
    }

    // Check cache connection
    if err := cache.Ping(); err != nil {
        http.Error(w, &quot;Cache not ready&quot;, http.StatusServiceUnavailable)
        return
    }

    // Check if JIT warmup is complete (for JVM)
    if !isWarmedUp() {
        http.Error(w, &quot;Not warmed up&quot;, http.StatusServiceUnavailable)
        return
    }

    w.WriteHeader(http.StatusOK)
}

// Pre-warm critical paths
func warmUp() {
    // Execute common queries to populate caches
    // Make requests to frequently-called endpoints
    // Load configuration and compile templates

    warmedUp = true
}
</code></pre>
<p>Database connection pool exhaustion during scale-out is common: 10 instances each wanting 20 connections = 200 connections. Suddenly adding 10 more instances means 400 connections.</p>
<pre><code class="language-go">// Calculate safe pool sizes based on expected scale
// Rule: (instances * max_conns_per_instance) &lt; database_max_connections * 0.8

// For PostgreSQL with max_connections = 500
// With max 50 instances: 50 * max_per_instance &lt; 400
// Therefore: max_per_instance = 8

// Use PgBouncer for connection multiplexing
// 100 app connections -&gt; PgBouncer -&gt; 20 database connections
</code></pre>
<h3 id="load-balancer-algorithms">Load Balancer Algorithms</h3>
<p>The choice of load balancing algorithm affects latency distribution and is critical for horizontal scaling effectiveness.</p>
<div>
<h3>LOAD BALANCING ALGORITHMS</h3>
<div>
<div>
<div>Round Robin</div>
<div>Simple, equal distribution. Ignores instance health and load.</div>
<div>Issue: Slow instance receives equal traffic, becomes bottleneck.</div>
</div>
<div>
<div>Least Connections</div>
<div>Routes to instance with fewest active connections.</div>
<div>Better: Slow instances naturally receive less traffic.</div>
</div>
<div>
<div>Weighted Round Robin</div>
<div>Assign weights based on instance capacity.</div>
<div>Use for: Heterogeneous instances, canary deployments.</div>
</div>
<div>
<div>Power of Two Choices</div>
<div>Pick 2 random instances, route to one with fewer connections.</div>
<div>Best: Combines randomness with load awareness, O(1) decision.</div>
</div>
</div>
</div>
<h3 id="interview-deep-dive-horizontal-scaling">Interview Deep-Dive: Horizontal Scaling</h3>
<div>
<h4>Level 1: You scale from 10 to 50 instances but latency doesn't improve proportionally. What might be happening?</h4>
<div>
<div>Answer:</div>
<div>
The bottleneck has moved. Common causes: (1) <strong>Shared database</strong> - 50 instances all hitting the same database, now database is the bottleneck; (2) <strong>Shared cache</strong> - Redis or Memcached becomes saturated; (3) <strong>Network bandwidth</strong> - aggregate bandwidth exceeds network capacity; (4) <strong>Lock contention</strong> - more instances mean more concurrent access to locked resources; (5) <strong>Connection limits</strong> - downstream services can't handle 5x the connections; (6) <strong>Cold start impact</strong> - new instances slower during warmup, dragging down overall latency. Check: database metrics, cache hit rates, connection pool wait times.
</div>
</div>
<h4>Level 2: You've identified the database as the new bottleneck after scaling. Adding read replicas helped but write performance is still poor. What are your options?</h4>
<div>
<div>Answer:</div>
<div>
Writes are harder to scale than reads. Options: (1) <strong>Write batching</strong> - buffer writes in memory/Redis, flush periodically (trades latency for throughput); (2) <strong>Sharding</strong> - partition data by user_id or tenant_id across multiple database instances (see [[database-sharding]](/system-design/database-sharding)); (3) <strong>CQRS</strong> - separate write model (optimized for writes) from read model (optimized for reads), sync via events; (4) <strong>Async writes</strong> - queue writes for background processing, return immediately; (5) <strong>Reduce write frequency</strong> - aggregate updates instead of individual writes (e.g., view counts). For immediate relief: analyze slow write queries, optimize indexes for write patterns, consider SSDs for I/O-bound writes.
</div>
</div>
<h4>Level 3: You implement sharding by user_id. Now you have cross-shard queries for reports that aggregate data across all users. These queries are extremely slow. How do you handle this?</h4>
<div>
<div>Answer:</div>
<div>
Cross-shard queries are the fundamental challenge of sharding. Strategies: (1) <strong>Scatter-gather</strong> - query all shards in parallel, aggregate results in application layer. Works for simple aggregations but limited for complex joins; (2) <strong>Dedicated analytics database</strong> - stream changes to a columnar database (ClickHouse, BigQuery) optimized for analytical queries. Operational queries stay on shards, reports go to analytics DB; (3) <strong>Pre-computed aggregates</strong> - maintain running totals updated on each write. Trade write cost for read speed; (4) <strong>Lambda/Kappa architecture</strong> - real-time stream processing (Flink, Spark Streaming) maintains aggregates, batch jobs correct any drift; (5) <strong>Change the query</strong> - if reports are "per-tenant", route to the tenant's shard. Avoid true cross-shard queries by designing data locality. The pattern: OLTP on shards, OLAP on separate infrastructure.
</div>
</div>
</div>
<hr />
<h2 id="cross-cutting-concerns">Cross-Cutting Concerns</h2>
<h3 id="observability-for-bottleneck-detection">Observability for Bottleneck Detection</h3>
<p>Effective bottleneck removal requires comprehensive observability. The three pillars work together:</p>
<div>
<h3>OBSERVABILITY PILLARS FOR BOTTLENECK DETECTION</h3>
<div>
<div>
<div>Metrics</div>
<div>
<strong>RED Method:</strong> Rate, Errors, Duration for services.<br/>
<strong>USE Method:</strong> Utilization, Saturation, Errors for resources.<br/>
<strong>Key:</strong> Histogram percentiles (p50, p95, p99), not averages.
</div>
</div>
<div>
<div>Traces</div>
<div>
  End-to-end request flow across services.<br/>
  Identify slow spans and service dependencies.<br/>
<strong>Key:</strong> Sample high-latency requests for detailed analysis.
</div>
</div>
<div>
<div>Logs</div>
<div>
  Structured logs with trace IDs for correlation.<br/>
  Error details and context for debugging.<br/>
<strong>Key:</strong> Log slow queries, cache misses, retry events.
</div>
</div>
</div>
</div>
<h3 id="related-concepts">Related Concepts</h3>
<ul>
<li><a href="/microservices/circuit-breakers">[circuit-breakers]</a> - Fail fast when dependencies are slow</li>
<li><a href="/system-design/rate-limiting">[rate-limiting]</a> - Protect services from overload</li>
<li><a href="/system-design/database-sharding">[database-sharding]</a> - Scale writes across multiple databases</li>
<li><a href="/system-design/event-sourcing">[event-sourcing]</a> - Eliminate write contention with append-only logs</li>
<li><a href="/system-design/cqrs">[cqrs]</a> - Separate read and write models for independent scaling</li>
<li><a href="/microservices/connection-pooling">[connection-pooling]</a> - Efficient resource reuse</li>
<li><a href="/system-design/load-balancing">[load-balancing]</a> - Distribute traffic effectively</li>
</ul>
<hr />
<h2 id="key-takeaways">Key Takeaways</h2>
<div>
<div>
<div>
<div>1. Profile Before Optimizing</div>
<div>Use distributed tracing and the USE method to identify real bottlenecks. Premature optimization wastes effort on non-critical paths.</div>
</div>
<div>
<div>2. Bottlenecks Migrate</div>
<div>Fixing one bottleneck reveals the next. Plan for this: scaling the app layer will expose database limits, adding replicas will expose write bottlenecks.</div>
</div>
<div>
<div>3. Caching is Not Free</div>
<div>Caches introduce consistency challenges, stampede risks, and operational complexity. Design invalidation strategy upfront.</div>
</div>
<div>
<div>4. Async Requires Idempotency</div>
<div>Asynchronous processing with at-least-once delivery means messages may be duplicated. Design consumers to handle this gracefully.</div>
</div>
<div>
<div>5. Scale Stateless First</div>
<div>Horizontal scaling requires stateless services. Move sessions, files, and caches to external stores before adding instances.</div>
</div>
<div>
<div>6. Connection Pools Have Limits</div>
<div>More app instances mean more database connections. Plan connection limits across your entire fleet, not per-instance.</div>
</div>
</div>
</div>
