<style>
/* Mobile-specific styles for iPhone 15 and similar devices */
@media screen and (max-width: 480px) {
    /* Force all grid layouts to single column */
    [style*="grid-template-columns"] {
        display: block !important;
    }
    [style*="grid-template-columns"] > div {
        margin-bottom: 16px !important;
    }
    /* Adjust padding for mobile */
    [style*="padding: 32px"],
    [style*="padding: 24px"] {
        padding: 16px !important;
    }
    /* Smaller headings */
    h4[style*="font-size: 18px"],
    h4[style*="font-size: 16px"] {
        font-size: 15px !important;
    }
    /* Readable font sizes */
    [style*="font-size: 13px"],
    [style*="font-size: 12px"],
    [style*="font-size: 11px"],
    [style*="font-size: 10px"] {
        font-size: 13px !important;
        line-height: 1.6 !important;
    }
    /* Flex containers stack vertically */
    [style*="display: flex"][style*="gap"] {
        flex-direction: column !important;
    }
    /* Better spacing for nested content */
    [style*="padding-left: 64px"],
    [style*="padding-left: 48px"],
    [style*="padding-left: 40px"] {
        padding-left: 16px !important;
    }
    /* Code blocks */
    pre {
        font-size: 12px !important;
        padding: 12px !important;
        overflow-x: auto !important;
    }
    pre code {
        font-size: 12px !important;
    }
    /* Tables */
    table {
        font-size: 12px !important;
        display: block !important;
        overflow-x: auto !important;
    }
    th, td {
        padding: 8px !important;
        font-size: 12px !important;
    }
}
</style>
<h1 id="rate-limiter">Rate Limiter</h1>
<h2 id="problem-statement">Problem Statement</h2>
<p>Design a rate limiter that controls the rate of requests a client can make to an API. Support configurable limits like &quot;100 requests per minute per user&quot; with distributed deployment, multiple rate limiting strategies, and comprehensive observability.</p>
<h2 id="requirements">Requirements</h2>
<h3 id="functional-requirements">Functional Requirements</h3>
<ul>
<li>Limit requests per time window (per-user, per-IP, per-API-key, global)</li>
<li>Support multiple algorithms (token bucket, sliding window, leaky bucket)</li>
<li>Return remaining quota and reset time in response headers</li>
<li>Handle concurrent requests safely</li>
<li>Support tiered rate limits (free/premium/enterprise)</li>
</ul>
<h3 id="non-functional-requirements">Non-Functional Requirements</h3>
<ul>
<li>Sub-millisecond latency for rate limit decisions</li>
<li>Horizontally scalable across multiple servers</li>
<li>Graceful degradation when rate limit storage is unavailable</li>
<li>Support for burst traffic patterns</li>
<li>Accurate tracking across distributed nodes</li>
</ul>
<hr />
<h2 id="core-concepts-deep-dive">Core Concepts Deep Dive</h2>
<h3 id="why-rate-limiting-exists">Why Rate Limiting Exists</h3>
<div style="background: #f8fafc; border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Protection Layers:</strong></p>
<table>
<thead>
<tr>
<th>Threat Vector</th>
<th>Rate Limiting Defense</th>
<th>Alternative Mitigations</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>DoS/DDoS Attacks</strong></td>
<td>Per-IP limits, global limits</td>
<td>WAF, CDN, IP blocklists</td>
</tr>
<tr>
<td><strong>Credential Stuffing</strong></td>
<td>Per-IP on login endpoints</td>
<td>CAPTCHA, account lockout</td>
</tr>
<tr>
<td><strong>API Abuse</strong></td>
<td>Per-API-key limits</td>
<td>Usage quotas, billing</td>
</tr>
<tr>
<td><strong>Resource Exhaustion</strong></td>
<td>Per-resource limits</td>
<td>Circuit breakers, queuing</td>
</tr>
<tr>
<td><strong>Scraping</strong></td>
<td>Per-user agent patterns</td>
<td>Bot detection, fingerprinting</td>
</tr>
</tbody>
</table>
<p><span style="background: linear-gradient(90deg, rgba(46,204,113,0.3) 0%, rgba(46,204,113,0.1) 100%); padding: 2px 8px; border-radius: 4px"><strong>Key Insight</strong>: Rate limiting is a defense-in-depth measure, not a complete security solution. It buys time and reduces blast radius.</span></p>
</div>
<h4 id="interview-questions-rate-limiting-fundamentals">Interview Questions: Rate Limiting Fundamentals</h4>
<div style="background: #f0fdf4; border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Level 1: Where should rate limiting be implemented in a microservices architecture?</strong></p>
<p>Rate limiting can be implemented at multiple layers:</p>
<ol>
<li>
<p><strong>API Gateway Layer</strong> (Recommended primary location)</p>
<ul>
<li>Centralized enforcement before requests reach services</li>
<li>Single point of policy management</li>
<li>Can terminate bad requests early, saving downstream resources</li>
<li>Examples: Kong, AWS API Gateway, Nginx</li>
</ul>
</li>
<li>
<p><strong>Service Mesh Sidecar</strong></p>
<ul>
<li>Per-service rate limiting with <a href="/topics/system-design/service-mesh">[service-mesh]</a> sidecars</li>
<li>Envoy proxy with rate limit service</li>
<li>Service-to-service rate limiting</li>
</ul>
</li>
<li>
<p><strong>Application Layer</strong></p>
<ul>
<li>Business logic-aware rate limiting</li>
<li>Can consider request cost (expensive queries vs cheap)</li>
<li>Last line of defense</li>
</ul>
</li>
</ol>
<p><span style="background: linear-gradient(90deg, rgba(46,204,113,0.3) 0%, rgba(46,204,113,0.1) 100%); padding: 2px 8px; border-radius: 4px"><strong>Trade-off</strong>: Gateway rate limiting is efficient but coarse-grained. Application-level is precise but adds latency to every service.</span></p>
<hr />
<p><strong>Level 2: How do you handle rate limiting when the rate limiter storage (Redis) becomes unavailable?</strong></p>
<p>This is a critical availability vs. security trade-off:</p>
<p><strong>Option 1: Fail Open (Allow all requests)</strong></p>
<pre><code class="language-python">def check_rate_limit(key: str) -&gt; bool:
    try:
        return redis_client.check_limit(key)
    except RedisConnectionError:
        logger.warning(f&quot;Rate limiter unavailable, allowing request for {key}&quot;)
        return True  # Fail open
</code></pre>
<ul>
<li><strong>Pro</strong>: Service remains available</li>
<li><strong>Con</strong>: Vulnerable to abuse during outage</li>
</ul>
<p><strong>Option 2: Fail Closed (Deny all requests)</strong></p>
<pre><code class="language-python">def check_rate_limit(key: str) -&gt; bool:
    try:
        return redis_client.check_limit(key)
    except RedisConnectionError:
        logger.error(f&quot;Rate limiter unavailable, denying request for {key}&quot;)
        return False  # Fail closed
</code></pre>
<ul>
<li><strong>Pro</strong>: Maintains security posture</li>
<li><strong>Con</strong>: Complete service outage</li>
</ul>
<p><strong>Option 3: Local Fallback with Degraded Limits (Recommended)</strong></p>
<pre><code class="language-python">class RateLimiterWithFallback:
    def __init__(self):
        self.redis_limiter = RedisRateLimiter()
        self.local_limiter = LocalTokenBucket(
            rate=10,  # Conservative local limit
            capacity=20
        )
        self.local_mode_start = None

    def check_rate_limit(self, key: str) -&gt; RateLimitResult:
        try:
            result = self.redis_limiter.check(key)
            self.local_mode_start = None  # Reset fallback
            return result
        except RedisConnectionError:
            self._enter_local_mode()
            # Use local rate limiter with stricter limits
            local_result = self.local_limiter.allow(key)
            return RateLimitResult(
                allowed=local_result,
                degraded=True,
                message=&quot;Operating in degraded mode&quot;
            )

    def _enter_local_mode(self):
        if self.local_mode_start is None:
            self.local_mode_start = time.time()
            alert_ops_team(&quot;Rate limiter in local fallback mode&quot;)
</code></pre>
<p><span style="background: linear-gradient(90deg, rgba(46,204,113,0.3) 0%, rgba(46,204,113,0.1) 100%); padding: 2px 8px; border-radius: 4px"><strong>Design Choice</strong>: Local fallback provides availability with reduced but consistent protection. The stricter local limits prevent complete abuse while the distributed system recovers.</span></p>
<hr />
<p><strong>Level 3: How would you implement request cost-based rate limiting where different endpoints consume different amounts of quota?</strong></p>
<p>This requires weighted token consumption where expensive operations cost more:</p>
<pre><code class="language-python">from dataclasses import dataclass
from typing import Dict
from enum import Enum

class EndpointCost(Enum):
    CHEAP = 1       # GET /users/{id}
    STANDARD = 5    # POST /users
    EXPENSIVE = 20  # POST /reports/generate
    VERY_EXPENSIVE = 100  # POST /batch-import

@dataclass
class RequestContext:
    user_id: str
    endpoint: str
    method: str
    payload_size: int

class CostBasedRateLimiter:
    &quot;&quot;&quot;
    Rate limiter that considers request cost.

    Design considerations:
    1. Different endpoints have different computational costs
    2. Large payloads should cost more
    3. Batch operations should be weighted by batch size
    4. Read vs write operations have different costs
    &quot;&quot;&quot;

    def __init__(self, tokens_per_minute: int, bucket_capacity: int):
        self.tokens_per_minute = tokens_per_minute
        self.bucket_capacity = bucket_capacity
        self.endpoint_costs: Dict[str, int] = {}
        self.token_bucket = TokenBucket(
            rate=tokens_per_minute / 60,
            capacity=bucket_capacity
        )

    def register_endpoint_cost(self, pattern: str, cost: EndpointCost):
        &quot;&quot;&quot;Register cost for endpoint pattern.&quot;&quot;&quot;
        self.endpoint_costs[pattern] = cost.value

    def calculate_request_cost(self, ctx: RequestContext) -&gt; int:
        &quot;&quot;&quot;
        Calculate total cost for a request.

        Cost factors:
        1. Base endpoint cost
        2. Payload size multiplier
        3. Batch size multiplier (if applicable)
        &quot;&quot;&quot;
        # Get base cost from endpoint pattern matching
        base_cost = self._match_endpoint_cost(ctx.endpoint, ctx.method)

        # Add payload size penalty (1 extra token per 10KB)
        payload_penalty = ctx.payload_size // 10240

        # Calculate final cost
        total_cost = base_cost + payload_penalty

        return max(1, total_cost)  # Minimum cost is 1

    def _match_endpoint_cost(self, endpoint: str, method: str) -&gt; int:
        &quot;&quot;&quot;Match endpoint to cost using pattern matching.&quot;&quot;&quot;
        # Priority: exact match &gt; pattern match &gt; default
        key = f&quot;{method}:{endpoint}&quot;

        if key in self.endpoint_costs:
            return self.endpoint_costs[key]

        # Pattern matching for path parameters
        for pattern, cost in self.endpoint_costs.items():
            if self._matches_pattern(key, pattern):
                return cost

        # Default costs by method
        default_costs = {
            'GET': 1,
            'POST': 5,
            'PUT': 5,
            'DELETE': 3,
            'PATCH': 3
        }
        return default_costs.get(method, 5)

    def check_rate_limit(self, ctx: RequestContext) -&gt; RateLimitResult:
        &quot;&quot;&quot;Check if request is allowed and consume appropriate tokens.&quot;&quot;&quot;
        cost = self.calculate_request_cost(ctx)

        result = self.token_bucket.allow(ctx.user_id, tokens=cost)

        return RateLimitResult(
            allowed=result.allowed,
            remaining=result.remaining,
            cost_charged=cost if result.allowed else 0,
            reset_at=result.reset_at
        )

# Usage example
limiter = CostBasedRateLimiter(tokens_per_minute=1000, bucket_capacity=200)
limiter.register_endpoint_cost(&quot;GET:/users/*&quot;, EndpointCost.CHEAP)
limiter.register_endpoint_cost(&quot;POST:/reports/generate&quot;, EndpointCost.EXPENSIVE)
limiter.register_endpoint_cost(&quot;POST:/batch/*&quot;, EndpointCost.VERY_EXPENSIVE)
</code></pre>
<p><span style="background: linear-gradient(90deg, rgba(46,204,113,0.3) 0%, rgba(46,204,113,0.1) 100%); padding: 2px 8px; border-radius: 4px"><strong>Assumption</strong>: Request cost can be determined before execution. For operations where cost is only known after execution (like database queries), consider post-execution accounting with rollback capabilities.</span></p>
</div>
<hr />
<h2 id="token-bucket-algorithm">Token Bucket Algorithm</h2>
<h3 id="internal-mechanism">Internal Mechanism</h3>
<div style="background: #eff6ff; border-radius: 12px; padding: 24px; margin: 20px 0">
<div style="text-align: center; margin-bottom: 20px">
<div style="display: inline-block; background: linear-gradient(135deg, #1f6feb 0%, #388bfd 100%); border-radius: 12px; padding: 20px 40px">
<div style="color: #fff; font-weight: bold; font-size: 16px; margin-bottom: 16px">Token Bucket State Machine</div>
<div style="display: flex; align-items: center; justify-content: center; gap: 30px">
<div style="text-align: center">
<div style="background: #eff6ff; border-radius: 8px; padding: 16px; margin-bottom: 8px">
<div style="font-size: 12px; color: #8b949e; margin-bottom: 8px">Bucket State</div>
<div style="display: flex; gap: 4px; justify-content: center; flex-wrap: wrap; max-width: 100px">
<div style="background: #238636; width: 14px; height: 14px; border-radius: 50%"></div>
<div style="background: #238636; width: 14px; height: 14px; border-radius: 50%"></div>
<div style="background: #238636; width: 14px; height: 14px; border-radius: 50%"></div>
<div style="background: #238636; width: 14px; height: 14px; border-radius: 50%"></div>
<div style="background: #f8fafc; width: 14px; height: 14px; border-radius: 50%"></div>
<div style="background: #f8fafc; width: 14px; height: 14px; border-radius: 50%"></div>
</div>
<div style="color: #7ee787; font-size: 11px; margin-top: 8px">4/6 tokens</div>
</div>
<div style="color: #a5d6ff; font-size: 11px">capacity = 6</div>
</div>
<div style="color: #8b949e; font-size: 24px">+</div>
<div style="text-align: center">
<div style="background: #eff6ff; border-radius: 8px; padding: 16px; margin-bottom: 8px">
<div style="font-size: 12px; color: #8b949e; margin-bottom: 8px">Refill Rate</div>
<div style="color: #58a6ff; font-size: 18px; font-weight: bold">2/sec</div>
</div>
<div style="color: #a5d6ff; font-size: 11px">rate = 2.0</div>
</div>
<div style="color: #8b949e; font-size: 24px">+</div>
<div style="text-align: center">
<div style="background: #eff6ff; border-radius: 8px; padding: 16px; margin-bottom: 8px">
<div style="font-size: 12px; color: #8b949e; margin-bottom: 8px">Last Update</div>
<div style="color: #f0883e; font-size: 14px; font-weight: bold">1706012345.123</div>
</div>
<div style="color: #a5d6ff; font-size: 11px">Unix timestamp</div>
</div>
</div>
</div>
</div>
<p><strong>Core State Variables:</strong></p>
<pre><code>struct TokenBucket {
    tokens: f64,        // Current token count (can be fractional during calculation)
    capacity: f64,      // Maximum tokens the bucket can hold
    rate: f64,          // Tokens added per second
    last_update: f64,   // Unix timestamp of last token calculation
}
</code></pre>
<p><strong>The Fundamental Equation:</strong></p>
<pre><code>new_tokens = min(capacity, old_tokens + (current_time - last_update) * rate)
</code></pre>
<p><span style="background: linear-gradient(90deg, rgba(46,204,113,0.3) 0%, rgba(46,204,113,0.1) 100%); padding: 2px 8px; border-radius: 4px"><strong>Critical Implementation Detail</strong>: Tokens are calculated lazily on each request, not continuously refilled. This is essential for efficiency - we don't need background threads or timers.</span></p>
</div>
<h3 id="token-bucket-step-by-step-execution">Token Bucket: Step-by-Step Execution</h3>
<div style="background: #f5f3ff; border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Scenario</strong>: Rate = 10 tokens/sec, Capacity = 20 tokens</p>
<div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 16px; margin: 20px 0">
<div style="background: rgba(31, 111, 235, 0.2); border-radius: 8px; padding: 16px">
<div style="color: #58a6ff; font-weight: bold; margin-bottom: 8px">t=0.000s: Initial State</div>
<div style="color: #c9d1d9; font-size: 13px">
tokens = 20 (full bucket)<br>
last_update = 0.000
</div>
</div>
<div style="background: rgba(31, 111, 235, 0.2); border-radius: 8px; padding: 16px">
<div style="color: #58a6ff; font-weight: bold; margin-bottom: 8px">t=0.001s: Burst of 15 requests</div>
<div style="color: #c9d1d9; font-size: 13px">
tokens = 20 - 15 = 5<br>
All 15 requests ALLOWED
</div>
</div>
<div style="background: rgba(35, 134, 54, 0.2); border-radius: 8px; padding: 16px">
<div style="color: #7ee787; font-weight: bold; margin-bottom: 8px">t=0.500s: Request arrives</div>
<div style="color: #c9d1d9; font-size: 13px">
elapsed = 0.499s<br>
refill = 0.499 * 10 = 4.99<br>
tokens = min(20, 5 + 4.99) = 9.99<br>
After request: 8.99 tokens
</div>
</div>
<div style="background: rgba(248, 81, 73, 0.2); border-radius: 8px; padding: 16px">
<div style="color: #ff7b72; font-weight: bold; margin-bottom: 8px">t=0.501s: 10 more requests</div>
<div style="color: #c9d1d9; font-size: 13px">
tokens = 8.99<br>
Only 8 requests ALLOWED<br>
2 requests DENIED (429)
</div>
</div>
</div>
<p><strong>Why Token Bucket Handles Bursts Well:</strong></p>
<p>The bucket &quot;saves up&quot; unused capacity. If a user has been quiet for a while, they can legitimately burst up to capacity. This matches real-world usage patterns where users might be idle, then suddenly need several quick operations.</p>
<p><span style="background: linear-gradient(90deg, rgba(46,204,113,0.3) 0%, rgba(46,204,113,0.1) 100%); padding: 2px 8px; border-radius: 4px"><strong>Trade-off</strong>: Burst allowance improves user experience but can overwhelm downstream services if all users burst simultaneously. Consider combining with a global rate limiter.</span></p>
</div>
<h3 id="token-bucket-implementation-with-full-edge-case-handling">Token Bucket Implementation with Full Edge Case Handling</h3>
<pre><code class="language-python">import time
import threading
from dataclasses import dataclass
from typing import Optional, Dict
from contextlib import contextmanager
import logging

logger = logging.getLogger(__name__)

@dataclass(frozen=True)
class RateLimitResult:
    &quot;&quot;&quot;Immutable result of a rate limit check.&quot;&quot;&quot;
    allowed: bool
    remaining: int
    reset_at: float  # Unix timestamp when bucket will be full
    retry_after: Optional[float]  # Seconds until request might succeed
    limit: int  # Total limit for context

    def to_headers(self) -&gt; Dict[str, str]:
        &quot;&quot;&quot;Convert to standard rate limit headers.&quot;&quot;&quot;
        headers = {
            'X-RateLimit-Limit': str(self.limit),
            'X-RateLimit-Remaining': str(self.remaining),
            'X-RateLimit-Reset': str(int(self.reset_at)),
        }
        if self.retry_after is not None:
            headers['Retry-After'] = str(int(self.retry_after) + 1)
        return headers


class TokenBucket:
    &quot;&quot;&quot;
    Thread-safe token bucket implementation.

    Key design decisions:
    1. Lazy refill: Tokens calculated on demand, not continuously
    2. Float tokens: Internal precision, integer for external API
    3. Monotonic time: Use monotonic clock to avoid issues with clock adjustments
    4. Lock granularity: Per-bucket lock for maximum concurrency
    &quot;&quot;&quot;

    __slots__ = ('rate', 'capacity', '_tokens', '_last_update', '_lock', '_created_at')

    def __init__(self, rate: float, capacity: int):
        &quot;&quot;&quot;
        Initialize token bucket.

        Args:
            rate: Tokens added per second
            capacity: Maximum tokens bucket can hold

        Raises:
            ValueError: If rate &lt;= 0 or capacity &lt;= 0
        &quot;&quot;&quot;
        if rate &lt;= 0:
            raise ValueError(f&quot;Rate must be positive, got {rate}&quot;)
        if capacity &lt;= 0:
            raise ValueError(f&quot;Capacity must be positive, got {capacity}&quot;)

        self.rate = float(rate)
        self.capacity = int(capacity)
        self._tokens = float(capacity)  # Start full
        self._last_update = time.monotonic()
        self._lock = threading.Lock()
        self._created_at = time.time()

    def _refill(self, now: float) -&gt; None:
        &quot;&quot;&quot;
        Refill tokens based on elapsed time.

        IMPORTANT: Must be called while holding _lock.
        &quot;&quot;&quot;
        elapsed = now - self._last_update
        if elapsed &gt; 0:
            self._tokens = min(
                self.capacity,
                self._tokens + elapsed * self.rate
            )
            self._last_update = now

    def allow(self, tokens: int = 1) -&gt; RateLimitResult:
        &quot;&quot;&quot;
        Check if request is allowed and consume tokens if so.

        Args:
            tokens: Number of tokens to consume (default 1)

        Returns:
            RateLimitResult with allowed status and metadata
        &quot;&quot;&quot;
        if tokens &lt;= 0:
            raise ValueError(f&quot;Tokens must be positive, got {tokens}&quot;)
        if tokens &gt; self.capacity:
            # Request can never succeed - it exceeds bucket capacity
            return RateLimitResult(
                allowed=False,
                remaining=0,
                reset_at=time.time() + self.capacity / self.rate,
                retry_after=None,  # Will never succeed
                limit=self.capacity
            )

        with self._lock:
            now = time.monotonic()
            self._refill(now)

            current_time = time.time()

            if self._tokens &gt;= tokens:
                self._tokens -= tokens

                # Calculate when bucket will be full again
                tokens_needed = self.capacity - self._tokens
                reset_at = current_time + (tokens_needed / self.rate)

                return RateLimitResult(
                    allowed=True,
                    remaining=int(self._tokens),
                    reset_at=reset_at,
                    retry_after=None,
                    limit=self.capacity
                )

            # Request denied - calculate retry time
            tokens_needed = tokens - self._tokens
            retry_after = tokens_needed / self.rate

            return RateLimitResult(
                allowed=False,
                remaining=0,
                reset_at=current_time + (self.capacity / self.rate),
                retry_after=retry_after,
                limit=self.capacity
            )

    def peek(self) -&gt; int:
        &quot;&quot;&quot;Get current token count without modifying state.&quot;&quot;&quot;
        with self._lock:
            now = time.monotonic()
            elapsed = now - self._last_update
            tokens = min(self.capacity, self._tokens + elapsed * self.rate)
            return int(tokens)

    def __repr__(self) -&gt; str:
        return f&quot;TokenBucket(rate={self.rate}, capacity={self.capacity}, tokens={self.peek()})&quot;


class RateLimiter:
    &quot;&quot;&quot;
    Multi-key rate limiter with automatic cleanup.

    Manages a collection of token buckets, one per key (user, IP, etc).
    Includes background cleanup of stale buckets to prevent memory leaks.
    &quot;&quot;&quot;

    def __init__(
        self,
        rate: float,
        capacity: int,
        cleanup_interval: float = 60.0,
        bucket_ttl: float = 3600.0
    ):
        self.rate = rate
        self.capacity = capacity
        self.bucket_ttl = bucket_ttl

        self._buckets: Dict[str, TokenBucket] = {}
        self._bucket_last_access: Dict[str, float] = {}
        self._lock = threading.RLock()

        # Start cleanup thread
        self._cleanup_interval = cleanup_interval
        self._shutdown = threading.Event()
        self._cleanup_thread = threading.Thread(
            target=self._cleanup_loop,
            daemon=True,
            name=&quot;RateLimiter-Cleanup&quot;
        )
        self._cleanup_thread.start()

    def _get_bucket(self, key: str) -&gt; TokenBucket:
        &quot;&quot;&quot;Get or create bucket for key.&quot;&quot;&quot;
        with self._lock:
            now = time.time()
            self._bucket_last_access[key] = now

            if key not in self._buckets:
                self._buckets[key] = TokenBucket(self.rate, self.capacity)

            return self._buckets[key]

    def allow(self, key: str, tokens: int = 1) -&gt; RateLimitResult:
        &quot;&quot;&quot;Check rate limit for given key.&quot;&quot;&quot;
        bucket = self._get_bucket(key)
        return bucket.allow(tokens)

    def _cleanup_loop(self) -&gt; None:
        &quot;&quot;&quot;Background thread to clean up stale buckets.&quot;&quot;&quot;
        while not self._shutdown.wait(self._cleanup_interval):
            self._cleanup_stale_buckets()

    def _cleanup_stale_buckets(self) -&gt; None:
        &quot;&quot;&quot;Remove buckets that haven't been accessed recently.&quot;&quot;&quot;
        now = time.time()
        cutoff = now - self.bucket_ttl

        with self._lock:
            stale_keys = [
                key for key, last_access in self._bucket_last_access.items()
                if last_access &lt; cutoff
            ]

            for key in stale_keys:
                del self._buckets[key]
                del self._bucket_last_access[key]

            if stale_keys:
                logger.debug(f&quot;Cleaned up {len(stale_keys)} stale rate limit buckets&quot;)

    def shutdown(self) -&gt; None:
        &quot;&quot;&quot;Gracefully shutdown cleanup thread.&quot;&quot;&quot;
        self._shutdown.set()
        self._cleanup_thread.join(timeout=5.0)

    def stats(self) -&gt; Dict:
        &quot;&quot;&quot;Get current rate limiter statistics.&quot;&quot;&quot;
        with self._lock:
            return {
                'active_buckets': len(self._buckets),
                'rate': self.rate,
                'capacity': self.capacity
            }
</code></pre>
<h4 id="interview-questions-token-bucket">Interview Questions: Token Bucket</h4>
<div style="background: linear-gradient(135deg, #2d1f3d 0%, #4a3a5d 100%); border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Level 1: Why do we use monotonic time instead of wall clock time in the token bucket implementation?</strong></p>
<p>Wall clock time (<code>time.time()</code>) can jump forward or backward due to:</p>
<ul>
<li>NTP synchronization adjusting the clock</li>
<li>Daylight saving time changes</li>
<li>Manual clock adjustments</li>
<li>VM migrations or suspend/resume</li>
</ul>
<p><strong>Problems with wall clock:</strong></p>
<pre><code class="language-python"># Scenario: Clock jumps backward by 5 seconds during NTP sync
last_update = 1706012345.0  # 10:00:00
current_time = 1706012340.0  # Clock jumped back to 09:59:55

elapsed = current_time - last_update  # -5 seconds!
# This would result in NEGATIVE token refill
tokens = tokens + elapsed * rate  # tokens decrease!
</code></pre>
<p><strong>Solution using monotonic time:</strong></p>
<pre><code class="language-python"># Monotonic time only moves forward, regardless of wall clock
last_update = 12345.678  # Arbitrary monotonic value
current_time = 12350.123  # Always &gt;= last_update

elapsed = current_time - last_update  # Always &gt;= 0
</code></pre>
<p><span style="background: linear-gradient(90deg, rgba(46,204,113,0.3) 0%, rgba(46,204,113,0.1) 100%); padding: 2px 8px; border-radius: 4px"><strong>Trade-off</strong>: Monotonic time is only valid within a single process. For distributed rate limiting, you must use wall clock time with tolerance for small errors.</span></p>
<hr />
<p><strong>Level 2: How do you handle the &quot;thundering herd&quot; problem when many requests arrive exactly when the bucket refills?</strong></p>
<p>The thundering herd occurs when many blocked clients retry simultaneously after their rate limits reset:</p>
<div style="background: #eff6ff; border-radius: 8px; padding: 16px; margin: 16px 0">
<div style="display: flex; align-items: center; gap: 20px">
<div style="text-align: center; flex: 1">
<div style="color: #f85149; font-size: 12px; margin-bottom: 8px">All clients blocked</div>
<div style="background: #f85149; height: 8px; border-radius: 4px"></div>
<div style="color: #8b949e; font-size: 11px; margin-top: 4px">t=0</div>
</div>
<div style="color: #8b949e">...</div>
<div style="text-align: center; flex: 1">
<div style="color: #ffa657; font-size: 12px; margin-bottom: 8px">All retry at reset</div>
<div style="background: linear-gradient(90deg, #ffa657 80%, #238636 80%); height: 8px; border-radius: 4px"></div>
<div style="color: #8b949e; font-size: 11px; margin-top: 4px">t=reset</div>
</div>
<div style="color: #8b949e">...</div>
<div style="text-align: center; flex: 1">
<div style="color: #f85149; font-size: 12px; margin-bottom: 8px">Spike overloads system</div>
<div style="background: #f85149; height: 8px; border-radius: 4px"></div>
<div style="color: #8b949e; font-size: 11px; margin-top: 4px">t=reset+1ms</div>
</div>
</div>
</div>
<p><strong>Mitigation Strategies:</strong></p>
<ol>
<li><strong>Jittered Retry-After Header:</strong></li>
</ol>
<pre><code class="language-python">import random

def calculate_retry_after(base_retry: float) -&gt; float:
    &quot;&quot;&quot;Add jitter to spread out retries.&quot;&quot;&quot;
    # Add 0-25% random jitter
    jitter = random.uniform(0, base_retry * 0.25)
    return base_retry + jitter
</code></pre>
<ol start="2">
<li><strong>Exponential Backoff with Jitter (Client-side):</strong></li>
</ol>
<pre><code class="language-python">def retry_with_backoff(attempt: int, base: float = 1.0, cap: float = 60.0) -&gt; float:
    &quot;&quot;&quot;Calculate backoff with full jitter.&quot;&quot;&quot;
    temp = min(cap, base * (2 ** attempt))
    return random.uniform(0, temp)
</code></pre>
<ol start="3">
<li><strong>Token Drip Instead of Reset:</strong></li>
</ol>
<pre><code class="language-python"># Instead of resetting to full capacity at reset time,
# tokens drip in continuously. This naturally spreads load.
# Token bucket already does this - it's a feature!
</code></pre>
<p><span style="background: linear-gradient(90deg, rgba(46,204,113,0.3) 0%, rgba(46,204,113,0.1) 100%); padding: 2px 8px; border-radius: 4px"><strong>Key Insight</strong>: Token bucket's continuous refill naturally mitigates thundering herd compared to fixed window. Returning a jittered Retry-After header helps further.</span></p>
<hr />
<p><strong>Level 3: How would you implement a token bucket that supports &quot;borrowing&quot; from future tokens for premium users while maintaining rate guarantees?</strong></p>
<p>This is a sophisticated requirement for premium users who need occasional burst beyond capacity with guaranteed future repayment:</p>
<pre><code class="language-python">from dataclasses import dataclass
from typing import Optional
import time
import threading

@dataclass
class BorrowingResult:
    allowed: bool
    tokens_consumed: int
    tokens_borrowed: int
    debt_remaining: int
    repayment_complete_at: Optional[float]

class BorrowingTokenBucket:
    &quot;&quot;&quot;
    Token bucket that allows borrowing from future allocation.

    Design:
    - Users can borrow up to `max_debt` tokens beyond current balance
    - Debt is repaid automatically as tokens refill
    - No new tokens available until debt is fully repaid
    - Premium feature with strict limits to prevent abuse

    Use case: User needs to upload 50 files but only has 20 tokens.
    They can borrow 30 tokens and will be rate-limited until debt is repaid.
    &quot;&quot;&quot;

    def __init__(
        self,
        rate: float,
        capacity: int,
        max_debt: int,
        debt_interest_rate: float = 0.0
    ):
        &quot;&quot;&quot;
        Args:
            rate: Tokens per second
            capacity: Maximum tokens
            max_debt: Maximum tokens that can be borrowed
            debt_interest_rate: Extra cost per borrowed token (0.1 = 10% extra)
        &quot;&quot;&quot;
        self.rate = rate
        self.capacity = capacity
        self.max_debt = max_debt
        self.debt_interest_rate = debt_interest_rate

        self._tokens = float(capacity)
        self._debt = 0.0  # Current debt (positive = owes tokens)
        self._last_update = time.monotonic()
        self._lock = threading.Lock()

    def _refill(self, now: float) -&gt; None:
        &quot;&quot;&quot;Refill tokens, paying debt first.&quot;&quot;&quot;
        elapsed = now - self._last_update
        if elapsed &lt;= 0:
            return

        new_tokens = elapsed * self.rate
        self._last_update = now

        # Pay debt first before adding to available tokens
        if self._debt &gt; 0:
            debt_payment = min(new_tokens, self._debt)
            self._debt -= debt_payment
            new_tokens -= debt_payment

        # Add remaining to available tokens
        self._tokens = min(self.capacity, self._tokens + new_tokens)

    def allow(
        self,
        tokens: int = 1,
        allow_borrowing: bool = False
    ) -&gt; BorrowingResult:
        &quot;&quot;&quot;
        Check rate limit with optional borrowing.

        Args:
            tokens: Tokens needed
            allow_borrowing: Whether to allow borrowing if insufficient
        &quot;&quot;&quot;
        with self._lock:
            now = time.monotonic()
            self._refill(now)

            # Check if in debt - no requests allowed while in debt
            if self._debt &gt; 0 and not allow_borrowing:
                return BorrowingResult(
                    allowed=False,
                    tokens_consumed=0,
                    tokens_borrowed=0,
                    debt_remaining=int(self._debt),
                    repayment_complete_at=time.time() + (self._debt / self.rate)
                )

            # Normal case: enough tokens available
            if self._tokens &gt;= tokens:
                self._tokens -= tokens
                return BorrowingResult(
                    allowed=True,
                    tokens_consumed=tokens,
                    tokens_borrowed=0,
                    debt_remaining=int(self._debt),
                    repayment_complete_at=None
                )

            # Borrowing case
            if allow_borrowing:
                available = self._tokens
                needed_from_debt = tokens - available

                # Apply interest to borrowed amount
                debt_with_interest = needed_from_debt * (1 + self.debt_interest_rate)

                # Check if borrowing is allowed
                potential_total_debt = self._debt + debt_with_interest
                if potential_total_debt &lt;= self.max_debt:
                    self._tokens = 0
                    self._debt = potential_total_debt

                    repayment_time = time.time() + (self._debt / self.rate)

                    return BorrowingResult(
                        allowed=True,
                        tokens_consumed=int(available),
                        tokens_borrowed=int(needed_from_debt),
                        debt_remaining=int(self._debt),
                        repayment_complete_at=repayment_time
                    )

            # Cannot fulfill request
            return BorrowingResult(
                allowed=False,
                tokens_consumed=0,
                tokens_borrowed=0,
                debt_remaining=int(self._debt),
                repayment_complete_at=time.time() + (self._debt / self.rate) if self._debt &gt; 0 else None
            )

# Usage example
bucket = BorrowingTokenBucket(
    rate=10,           # 10 tokens/sec
    capacity=50,       # Max 50 tokens
    max_debt=100,      # Can borrow up to 100 tokens
    debt_interest_rate=0.2  # 20% interest on borrowed tokens
)

# User needs 80 tokens but only has 50
result = bucket.allow(tokens=80, allow_borrowing=True)
# Result: allowed=True, consumed=50, borrowed=30, debt=36 (30 * 1.2)
# User will be rate-limited for 3.6 seconds while debt is repaid
</code></pre>
<p><span style="background: linear-gradient(90deg, rgba(46,204,113,0.3) 0%, rgba(46,204,113,0.1) 100%); padding: 2px 8px; border-radius: 4px"><strong>Design Choice</strong>: Interest on borrowed tokens creates economic incentive to avoid borrowing except when necessary. This self-regulates the feature usage without hard blocking premium users.</span></p>
</div>
<hr />
<h2 id="sliding-window-algorithm">Sliding Window Algorithm</h2>
<h3 id="why-sliding-window-exists">Why Sliding Window Exists</h3>
<div style="background: linear-gradient(135deg, #4a1a1a 0%, #6b2d2d 100%); border-radius: 12px; padding: 20px; margin: 16px 0">
<p><strong>The Fixed Window Boundary Problem:</strong></p>
<div style="background: #eff6ff; border-radius: 8px; padding: 16px; margin: 16px 0">
<div style="display: flex; flex-direction: column; gap: 8px">
<div style="display: flex; align-items: center; gap: 8px">
<div style="width: 100px; color: #8b949e; font-size: 12px">Limit: 100/min</div>
<div style="flex: 1; display: flex">
<div style="flex: 1; background: linear-gradient(90deg, transparent 90%, #f85149 90%); height: 24px;display: flex; align-items: center; justify-content: flex-end; padding-right: 4px">
<span style="color: #f85149; font-size: 10px">100 req</span>
</div>
<div style="flex: 1; background: linear-gradient(90deg, #f85149 10%, transparent 10%); height: 24px;display: flex; align-items: center; padding-left: 4px">
<span style="color: #f85149; font-size: 10px">100 req</span>
</div>
</div>
</div>
<div style="display: flex; align-items: center; gap: 8px">
<div style="width: 100px"></div>
<div style="flex: 1; display: flex; justify-content: center">
<div style="background: #f85149; color: #fff; padding: 4px 12px; border-radius: 4px; font-size: 11px; font-weight: bold">
200 requests in ~2 seconds!
</div>
</div>
</div>
<div style="color: #8b949e; font-size: 11px; text-align: center">
Window 1 (00:00-01:00) | Window 2 (01:00-02:00)
</div>
</div>
</div>
<p><strong>Result</strong>: User sends 100 requests at 00:59, then 100 more at 01:01. Both are allowed because they're in different windows, but effectively they've sent 200 requests in 2 seconds - double the intended rate!</p>
</div>
<h3 id="sliding-window-counter-the-weighted-average-solution">Sliding Window Counter: The Weighted Average Solution</h3>
<div style="background: #f5f3ff; border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>The Key Insight</strong>: Instead of storing every request timestamp, approximate the count using a weighted average of the current and previous window.</p>
<div style="background: #eff6ff; border-radius: 8px; padding: 20px; margin: 16px 0">
<p><strong>Visual: Sliding Window at 25% into Current Window</strong></p>
<div style="display: flex; align-items: center; margin: 16px 0">
<div style="width: 120px; color: #8b949e; font-size: 12px">Previous Window</div>
<div style="flex: 1; position: relative">
<div style="display: flex; height: 40px">
<div style="flex: 3; background: linear-gradient(90deg, rgba(139, 148, 158, 0.1) 0%, rgba(139, 148, 158, 0.3) 100%);display: flex; align-items: center; justify-content: center">
<span style="color: #d2a8ff; font-size: 12px">84 requests (75% weight)</span>
</div>
<div style="flex: 1; background: rgba(136, 87, 229, 0.3);display: flex; align-items: center; justify-content: center">
<span style="color: #d2a8ff; font-size: 12px">36 req</span>
</div>
</div>
<div style="position: absolute; left: 75%; top: 50%; transform: translate(-50%, -50%); background: #8957e5; color: #fff; padding: 2px 8px; border-radius: 4px; font-size: 10px">
NOW
</div>
</div>
</div>
<p><strong>Calculation:</strong></p>
<pre><code>window_size = 60 seconds
current_position = 15 seconds into current window
progress = 15 / 60 = 0.25 (25%)

previous_window_count = 84
current_window_count = 36

weighted_count = previous × (1 - progress) + current × 1.0
               = 84 × 0.75 + 36 × 1.0
               = 63 + 36
               = 99

Limit = 100, weighted_count = 99 → ALLOWED
</code></pre>
</div>
<p><span style="background: linear-gradient(90deg, rgba(46,204,113,0.3) 0%, rgba(46,204,113,0.1) 100%); padding: 2px 8px; border-radius: 4px"><strong>Trade-off</strong>: Sliding window counter is an approximation. In worst case, it can allow up to ~6% more requests than the limit. If you need exact counting, use sliding window log (stores all timestamps).</span></p>
</div>
<h3 id="sliding-window-implementation">Sliding Window Implementation</h3>
<pre><code class="language-python">import time
import threading
from collections import defaultdict
from dataclasses import dataclass
from typing import Dict, Tuple, Optional

@dataclass
class SlidingWindowResult:
    allowed: bool
    remaining: int
    reset_at: float
    current_count: int
    window_size: int

class SlidingWindowCounter:
    &quot;&quot;&quot;
    Sliding window counter rate limiter.

    Memory efficient: Only stores 2 counters per key (current + previous window).
    Time efficient: O(1) per operation.

    Trade-off: Approximate counting, can allow ~6% over limit in worst case.
    &quot;&quot;&quot;

    def __init__(self, limit: int, window_seconds: int):
        &quot;&quot;&quot;
        Args:
            limit: Maximum requests per window
            window_seconds: Window size in seconds
        &quot;&quot;&quot;
        if limit &lt;= 0:
            raise ValueError(f&quot;Limit must be positive, got {limit}&quot;)
        if window_seconds &lt;= 0:
            raise ValueError(f&quot;Window must be positive, got {window_seconds}&quot;)

        self.limit = limit
        self.window_seconds = window_seconds

        # counters[key][window_number] = count
        self._counters: Dict[str, Dict[int, int]] = defaultdict(lambda: defaultdict(int))
        self._lock = threading.Lock()

    def _get_window_number(self, timestamp: float) -&gt; int:
        &quot;&quot;&quot;Get window number for a given timestamp.&quot;&quot;&quot;
        return int(timestamp // self.window_seconds)

    def _get_window_progress(self, timestamp: float) -&gt; float:
        &quot;&quot;&quot;Get progress through current window (0.0 to 1.0).&quot;&quot;&quot;
        return (timestamp % self.window_seconds) / self.window_seconds

    def _calculate_weighted_count(
        self,
        key: str,
        current_window: int,
        progress: float
    ) -&gt; float:
        &quot;&quot;&quot;Calculate weighted request count across windows.&quot;&quot;&quot;
        previous_window = current_window - 1

        prev_count = self._counters[key].get(previous_window, 0)
        curr_count = self._counters[key].get(current_window, 0)

        # Weight previous window by remaining time, current by full weight
        weighted = prev_count * (1 - progress) + curr_count

        return weighted

    def allow(self, key: str) -&gt; SlidingWindowResult:
        &quot;&quot;&quot;
        Check if request is allowed and increment counter if so.

        Args:
            key: Identifier for rate limit bucket (user ID, IP, etc.)
        &quot;&quot;&quot;
        now = time.time()
        current_window = self._get_window_number(now)
        progress = self._get_window_progress(now)

        with self._lock:
            # Calculate current weighted count
            weighted_count = self._calculate_weighted_count(key, current_window, progress)

            # Check if over limit
            if weighted_count &gt;= self.limit:
                # Calculate when window will reset
                reset_at = (current_window + 1) * self.window_seconds

                return SlidingWindowResult(
                    allowed=False,
                    remaining=0,
                    reset_at=reset_at,
                    current_count=int(weighted_count),
                    window_size=self.limit
                )

            # Increment counter for current window
            self._counters[key][current_window] += 1

            # Cleanup old windows (keep only current and previous)
            self._cleanup_old_windows(key, current_window)

            # Calculate remaining (approximate)
            remaining = max(0, int(self.limit - weighted_count - 1))
            reset_at = (current_window + 1) * self.window_seconds

            return SlidingWindowResult(
                allowed=True,
                remaining=remaining,
                reset_at=reset_at,
                current_count=int(weighted_count) + 1,
                window_size=self.limit
            )

    def _cleanup_old_windows(self, key: str, current_window: int) -&gt; None:
        &quot;&quot;&quot;Remove windows older than previous window.&quot;&quot;&quot;
        previous_window = current_window - 1

        stale_windows = [
            w for w in self._counters[key].keys()
            if w &lt; previous_window
        ]

        for window in stale_windows:
            del self._counters[key][window]

    def get_current_count(self, key: str) -&gt; int:
        &quot;&quot;&quot;Get approximate current request count for a key.&quot;&quot;&quot;
        now = time.time()
        current_window = self._get_window_number(now)
        progress = self._get_window_progress(now)

        with self._lock:
            return int(self._calculate_weighted_count(key, current_window, progress))
</code></pre>
<h3 id="sliding-window-log-exact-counting">Sliding Window Log: Exact Counting</h3>
<pre><code class="language-python">import time
import threading
from collections import defaultdict, deque
from dataclasses import dataclass
from typing import Dict, Deque

@dataclass
class SlidingLogResult:
    allowed: bool
    remaining: int
    reset_at: float
    requests_in_window: int

class SlidingWindowLog:
    &quot;&quot;&quot;
    Exact request counting using timestamp log.

    Stores every request timestamp within the window.
    Most accurate but O(n) space where n = requests per window per key.

    Use when:
    - Accuracy is critical (compliance, billing)
    - Request volume is low to moderate
    - Memory is not a concern

    Avoid when:
    - High request volume (millions/minute)
    - Memory constrained environment
    - Sub-millisecond latency required
    &quot;&quot;&quot;

    def __init__(self, limit: int, window_seconds: int):
        self.limit = limit
        self.window_seconds = window_seconds

        # logs[key] = deque of timestamps
        self._logs: Dict[str, Deque[float]] = defaultdict(deque)
        self._lock = threading.Lock()

    def allow(self, key: str) -&gt; SlidingLogResult:
        &quot;&quot;&quot;Check rate limit with exact counting.&quot;&quot;&quot;
        now = time.time()
        cutoff = now - self.window_seconds

        with self._lock:
            log = self._logs[key]

            # Remove expired timestamps (O(k) where k = expired entries)
            while log and log[0] &lt;= cutoff:
                log.popleft()

            count = len(log)

            if count &gt;= self.limit:
                # Find when oldest request will expire
                oldest = log[0] if log else now
                reset_at = oldest + self.window_seconds

                return SlidingLogResult(
                    allowed=False,
                    remaining=0,
                    reset_at=reset_at,
                    requests_in_window=count
                )

            # Add new request timestamp
            log.append(now)

            # Calculate reset time (when first request in window expires)
            reset_at = log[0] + self.window_seconds

            return SlidingLogResult(
                allowed=True,
                remaining=self.limit - count - 1,
                reset_at=reset_at,
                requests_in_window=count + 1
            )

    def get_request_count(self, key: str) -&gt; int:
        &quot;&quot;&quot;Get exact count of requests in current window.&quot;&quot;&quot;
        now = time.time()
        cutoff = now - self.window_seconds

        with self._lock:
            log = self._logs[key]
            # Count non-expired timestamps
            return sum(1 for ts in log if ts &gt; cutoff)
</code></pre>
<h4 id="interview-questions-sliding-window">Interview Questions: Sliding Window</h4>
<div style="background: #f0fdf4; border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Level 1: What's the maximum error rate of sliding window counter compared to sliding window log?</strong></p>
<p>The sliding window counter approximates the true count. In the worst case:</p>
<p><strong>Worst Case Scenario:</strong></p>
<ul>
<li>Window size: 60 seconds, Limit: 100</li>
<li>At t=0: Previous window had 100 requests all at t=-1 second</li>
<li>At t=0.01 (1% into current window): weighted = 100 × 0.99 + 0 = 99</li>
<li>User makes request #100 at t=0.01: weighted = 99, allowed!</li>
<li>At t=0.02: weighted = 100 × 0.98 + 1 = 99, another request allowed!</li>
</ul>
<p><strong>Maximum Over-limit:</strong></p>
<pre><code>In the absolute worst case, sliding window counter can allow approximately:
- Up to 2× - 1 requests in a window of size W
- For limit=100, worst case ≈ 106 requests
- Error rate ≈ 6%
</code></pre>
<p><strong>Why this happens:</strong><br />
The counter assumes requests are uniformly distributed within each window. When requests cluster at window boundaries, the approximation breaks down.</p>
<p><span style="background: linear-gradient(90deg, rgba(46,204,113,0.3) 0%, rgba(46,204,113,0.1) 100%); padding: 2px 8px; border-radius: 4px"><strong>Assumption</strong>: In real-world traffic, requests are typically well-distributed, making the approximation very accurate (usually &lt;1% error). The 6% worst case requires adversarial request patterns.</span></p>
<hr />
<p><strong>Level 2: How would you implement sliding window with sub-second precision without storing every timestamp?</strong></p>
<p>Use <strong>sub-windows</strong> (also called micro-batching) to get better precision with bounded memory:</p>
<pre><code class="language-python">class PreciseSlidingWindow:
    &quot;&quot;&quot;
    Sliding window with configurable precision.

    Instead of 1 counter per window, use N sub-windows.
    More sub-windows = better precision but more memory.

    Example: 60-second window with 60 sub-windows (1-second precision)
    Only stores 60 integers instead of potentially millions of timestamps.
    &quot;&quot;&quot;

    def __init__(
        self,
        limit: int,
        window_seconds: int,
        precision_seconds: float = 1.0
    ):
        self.limit = limit
        self.window_seconds = window_seconds
        self.precision_seconds = precision_seconds

        # Number of sub-windows
        self.num_subwindows = int(window_seconds / precision_seconds)

        # Store count per sub-window
        # subwindows[key] = {subwindow_number: count}
        self._subwindows: Dict[str, Dict[int, int]] = defaultdict(dict)
        self._lock = threading.Lock()

    def _get_subwindow(self, timestamp: float) -&gt; int:
        &quot;&quot;&quot;Get sub-window number for timestamp.&quot;&quot;&quot;
        return int(timestamp / self.precision_seconds)

    def _calculate_count(self, key: str, now: float) -&gt; float:
        &quot;&quot;&quot;
        Calculate weighted count across all relevant sub-windows.

        For each sub-window that overlaps with [now - window_seconds, now]:
        - Calculate what fraction of that sub-window is within our window
        - Multiply count by that fraction
        &quot;&quot;&quot;
        window_start = now - self.window_seconds
        current_subwindow = self._get_subwindow(now)
        start_subwindow = self._get_subwindow(window_start)

        total = 0.0
        subwindows = self._subwindows[key]

        for sw in range(start_subwindow, current_subwindow + 1):
            count = subwindows.get(sw, 0)
            if count == 0:
                continue

            # Calculate weight for this sub-window
            sw_start = sw * self.precision_seconds
            sw_end = sw_start + self.precision_seconds

            # Clamp to our window
            overlap_start = max(sw_start, window_start)
            overlap_end = min(sw_end, now)

            # Weight is fraction of sub-window that overlaps
            weight = (overlap_end - overlap_start) / self.precision_seconds
            total += count * weight

        return total

    def allow(self, key: str) -&gt; bool:
        now = time.time()
        current_subwindow = self._get_subwindow(now)

        with self._lock:
            count = self._calculate_count(key, now)

            if count &gt;= self.limit:
                return False

            # Increment current sub-window
            if current_subwindow not in self._subwindows[key]:
                self._subwindows[key][current_subwindow] = 0
            self._subwindows[key][current_subwindow] += 1

            # Cleanup old sub-windows
            self._cleanup(key, current_subwindow)

            return True

    def _cleanup(self, key: str, current_subwindow: int) -&gt; None:
        &quot;&quot;&quot;Remove sub-windows outside our window.&quot;&quot;&quot;
        cutoff = current_subwindow - self.num_subwindows - 1

        stale = [sw for sw in self._subwindows[key] if sw &lt; cutoff]
        for sw in stale:
            del self._subwindows[key][sw]
</code></pre>
<p><span style="background: linear-gradient(90deg, rgba(46,204,113,0.3) 0%, rgba(46,204,113,0.1) 100%); padding: 2px 8px; border-radius: 4px"><strong>Trade-off</strong>: With 1-second precision for a 60-second window, you store 60 integers (~240 bytes) per key instead of potentially thousands of timestamps. Error is bounded by precision_seconds / window_seconds.</span></p>
<hr />
<p><strong>Level 3: How would you implement rate limiting that considers request &quot;weight&quot; where some requests consume more quota than others, while still using sliding window?</strong></p>
<p>This requires tracking total weight consumed rather than just request count:</p>
<pre><code class="language-python">from dataclasses import dataclass
from typing import List, Tuple
from collections import deque

@dataclass
class WeightedRequest:
    timestamp: float
    weight: int

class WeightedSlidingWindow:
    &quot;&quot;&quot;
    Sliding window that tracks request weights.

    Instead of counting requests, we sum their weights.
    Useful for:
    - Different API endpoints having different costs
    - Batch operations consuming proportional quota
    - Resource-based rate limiting (CPU time, memory, etc.)
    &quot;&quot;&quot;

    def __init__(self, max_weight: int, window_seconds: int):
        &quot;&quot;&quot;
        Args:
            max_weight: Maximum total weight allowed per window
            window_seconds: Window size
        &quot;&quot;&quot;
        self.max_weight = max_weight
        self.window_seconds = window_seconds

        # Store (timestamp, weight) tuples
        self._requests: Dict[str, deque] = defaultdict(deque)
        self._current_weight: Dict[str, int] = defaultdict(int)
        self._lock = threading.Lock()

    def _cleanup_expired(self, key: str, now: float) -&gt; None:
        &quot;&quot;&quot;Remove expired requests and update current weight.&quot;&quot;&quot;
        cutoff = now - self.window_seconds
        requests = self._requests[key]

        while requests and requests[0].timestamp &lt;= cutoff:
            expired = requests.popleft()
            self._current_weight[key] -= expired.weight

    def allow(self, key: str, weight: int = 1) -&gt; Tuple[bool, int, float]:
        &quot;&quot;&quot;
        Check if request with given weight is allowed.

        Args:
            key: Rate limit key
            weight: Cost of this request

        Returns:
            (allowed, remaining_weight, reset_at)
        &quot;&quot;&quot;
        if weight &lt;= 0:
            raise ValueError(f&quot;Weight must be positive, got {weight}&quot;)
        if weight &gt; self.max_weight:
            # Single request exceeds limit - will never succeed
            return (False, 0, float('inf'))

        now = time.time()

        with self._lock:
            self._cleanup_expired(key, now)

            current = self._current_weight[key]

            if current + weight &gt; self.max_weight:
                # Find when enough weight will expire
                requests = self._requests[key]
                weight_needed = current + weight - self.max_weight

                accumulated = 0
                reset_at = now + self.window_seconds

                for req in requests:
                    accumulated += req.weight
                    if accumulated &gt;= weight_needed:
                        reset_at = req.timestamp + self.window_seconds
                        break

                return (False, max(0, self.max_weight - current), reset_at)

            # Add request
            self._requests[key].append(WeightedRequest(now, weight))
            self._current_weight[key] += weight

            remaining = self.max_weight - self._current_weight[key]

            # Reset at is when first request expires
            reset_at = now + self.window_seconds
            if self._requests[key]:
                reset_at = self._requests[key][0].timestamp + self.window_seconds

            return (True, remaining, reset_at)

    def get_available_weight(self, key: str) -&gt; int:
        &quot;&quot;&quot;Get currently available weight for a key.&quot;&quot;&quot;
        now = time.time()

        with self._lock:
            self._cleanup_expired(key, now)
            return max(0, self.max_weight - self._current_weight[key])

# Usage example
limiter = WeightedSlidingWindow(max_weight=1000, window_seconds=60)

# Cheap read operation (weight 1)
allowed, remaining, reset = limiter.allow(&quot;user:123&quot;, weight=1)

# Expensive report generation (weight 50)
allowed, remaining, reset = limiter.allow(&quot;user:123&quot;, weight=50)

# Very expensive batch import (weight 200)
allowed, remaining, reset = limiter.allow(&quot;user:123&quot;, weight=200)
</code></pre>
<p><span style="background: linear-gradient(90deg, rgba(46,204,113,0.3) 0%, rgba(46,204,113,0.1) 100%); padding: 2px 8px; border-radius: 4px"><strong>Design Choice</strong>: Storing individual weighted requests gives exact accounting but uses O(n) memory. For approximate weighted rate limiting with bounded memory, use the sub-window approach where each sub-window stores total weight instead of count.</span></p>
</div>
<hr />
<h2 id="distributed-rate-limiting">Distributed Rate Limiting</h2>
<h3 id="the-distributed-challenge">The Distributed Challenge</h3>
<div style="background: #f8fafc; border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Problem</strong>: With multiple application servers, each server has its own rate limiter state. A user could send requests to different servers and effectively multiply their rate limit.</p>
<div style="background: #eff6ff; border-radius: 8px; padding: 20px; margin: 16px 0">
<div style="text-align: center; color: #8b949e; font-size: 14px; margin-bottom: 16px">User with 100/min limit sends requests round-robin</div>
<div style="display: flex; justify-content: center; gap: 40px; margin-bottom: 16px">
<div style="text-align: center">
<div style="background: linear-gradient(135deg, #238636 0%, #2ea043 100%); width: 60px; height: 60px; border-radius: 8px; display: flex; align-items: center; justify-content: center; margin: 0 auto 8px">
<span style="color: #fff; font-size: 12px; font-weight: bold">Server 1</span>
</div>
<div style="color: #7ee787; font-size: 12px">50/100</div>
</div>
<div style="text-align: center">
<div style="background: linear-gradient(135deg, #238636 0%, #2ea043 100%); width: 60px; height: 60px; border-radius: 8px; display: flex; align-items: center; justify-content: center; margin: 0 auto 8px">
<span style="color: #fff; font-size: 12px; font-weight: bold">Server 2</span>
</div>
<div style="color: #7ee787; font-size: 12px">50/100</div>
</div>
<div style="text-align: center">
<div style="background: linear-gradient(135deg, #238636 0%, #2ea043 100%); width: 60px; height: 60px; border-radius: 8px; display: flex; align-items: center; justify-content: center; margin: 0 auto 8px">
<span style="color: #fff; font-size: 12px; font-weight: bold">Server 3</span>
</div>
<div style="color: #7ee787; font-size: 12px">50/100</div>
</div>
</div>
<div style="text-align: center; background: #f85149; color: #fff; padding: 8px 16px; border-radius: 4px; display: inline-block">
Result: 150 requests/min with 100/min limit!
</div>
</div>
<p><strong>Solutions:</strong></p>
<table>
<thead>
<tr>
<th>Approach</th>
<th>Pros</th>
<th>Cons</th>
<th>Use When</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Centralized (Redis)</strong></td>
<td>Accurate, simple</td>
<td>Single point of failure, latency</td>
<td>Most cases</td>
</tr>
<tr>
<td><strong>Sticky Sessions</strong></td>
<td>No shared state</td>
<td>Uneven load, failover issues</td>
<td>Simple setups</td>
</tr>
<tr>
<td><strong>Gossip Protocol</strong></td>
<td>Highly available</td>
<td>Eventually consistent</td>
<td>Very high scale</td>
</tr>
<tr>
<td><strong>Hybrid</strong></td>
<td>Best of both</td>
<td>Complex</td>
<td>Large distributed systems</td>
</tr>
</tbody>
</table>
</div>
<h3 id="redis-implementation-atomic-token-bucket">Redis Implementation: Atomic Token Bucket</h3>
<div style="background: #f0fdf4; border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Why Lua Scripts?</strong></p>
<p>Redis operations are atomic individually, but rate limiting requires multiple operations:</p>
<ol>
<li>Read current token count</li>
<li>Calculate refill</li>
<li>Check if enough tokens</li>
<li>Decrement if allowed</li>
</ol>
<p>Without atomicity, race conditions occur:</p>
<pre><code>Thread A: Read tokens = 1
Thread B: Read tokens = 1
Thread A: Decrement to 0, allow request
Thread B: Decrement to 0, allow request  // BOTH allowed with 1 token!
</code></pre>
<p><strong>Lua Script for Atomic Token Bucket:</strong></p>
<pre><code class="language-lua">-- KEYS[1] = rate limit key (e.g., &quot;ratelimit:user:123&quot;)
-- ARGV[1] = rate (tokens per second)
-- ARGV[2] = capacity (max tokens)
-- ARGV[3] = current timestamp (with millisecond precision)
-- ARGV[4] = tokens requested
-- ARGV[5] = TTL for the key in seconds

local key = KEYS[1]
local rate = tonumber(ARGV[1])
local capacity = tonumber(ARGV[2])
local now = tonumber(ARGV[3])
local requested = tonumber(ARGV[4])
local ttl = tonumber(ARGV[5])

-- Get current state
local data = redis.call('HMGET', key, 'tokens', 'last_update')
local tokens = tonumber(data[1])
local last_update = tonumber(data[2])

-- Initialize if new key
if tokens == nil then
    tokens = capacity
    last_update = now
end

-- Calculate token refill
local elapsed = math.max(0, now - last_update)
local refill = elapsed * rate
tokens = math.min(capacity, tokens + refill)

-- Prepare response
local allowed = 0
local remaining = math.floor(tokens)
local retry_after = -1

if tokens &gt;= requested then
    -- Allow request
    allowed = 1
    tokens = tokens - requested
    remaining = math.floor(tokens)
else
    -- Deny request - calculate retry time
    local tokens_needed = requested - tokens
    retry_after = tokens_needed / rate
end

-- Update state
redis.call('HMSET', key, 'tokens', tokens, 'last_update', now)
redis.call('EXPIRE', key, ttl)

-- Return: allowed (0/1), remaining tokens, retry_after seconds (-1 if allowed)
return {allowed, remaining, retry_after}
</code></pre>
</div>
<h3 id="python-redis-rate-limiter">Python Redis Rate Limiter</h3>
<pre><code class="language-python">import redis
import time
from dataclasses import dataclass
from typing import Optional
import hashlib

@dataclass
class DistributedRateLimitResult:
    allowed: bool
    remaining: int
    retry_after: Optional[float]
    limit: int
    reset_at: float

class RedisRateLimiter:
    &quot;&quot;&quot;
    Distributed rate limiter using Redis.

    Features:
    - Atomic operations via Lua scripting
    - Millisecond precision
    - Automatic key expiration
    - Connection pooling
    - Graceful degradation on Redis failure
    &quot;&quot;&quot;

    # Lua script for atomic token bucket
    TOKEN_BUCKET_SCRIPT = &quot;&quot;&quot;
    local key = KEYS[1]
    local rate = tonumber(ARGV[1])
    local capacity = tonumber(ARGV[2])
    local now = tonumber(ARGV[3])
    local requested = tonumber(ARGV[4])
    local ttl = tonumber(ARGV[5])

    local data = redis.call('HMGET', key, 'tokens', 'last_update')
    local tokens = tonumber(data[1])
    local last_update = tonumber(data[2])

    if tokens == nil then
        tokens = capacity
        last_update = now
    end

    local elapsed = math.max(0, now - last_update)
    tokens = math.min(capacity, tokens + elapsed * rate)

    local allowed = 0
    local remaining = math.floor(tokens)
    local retry_after = -1

    if tokens &gt;= requested then
        allowed = 1
        tokens = tokens - requested
        remaining = math.floor(tokens)
    else
        retry_after = (requested - tokens) / rate
    end

    redis.call('HMSET', key, 'tokens', tokens, 'last_update', now)
    redis.call('EXPIRE', key, ttl)

    return {allowed, remaining, retry_after}
    &quot;&quot;&quot;

    def __init__(
        self,
        redis_client: redis.Redis,
        rate: float,
        capacity: int,
        key_prefix: str = &quot;ratelimit:&quot;,
        ttl: int = 3600
    ):
        &quot;&quot;&quot;
        Args:
            redis_client: Redis connection (should use connection pool)
            rate: Tokens per second
            capacity: Maximum tokens
            key_prefix: Prefix for Redis keys
            ttl: Key expiration in seconds
        &quot;&quot;&quot;
        self.redis = redis_client
        self.rate = rate
        self.capacity = capacity
        self.key_prefix = key_prefix
        self.ttl = ttl

        # Register Lua script
        self._script = self.redis.register_script(self.TOKEN_BUCKET_SCRIPT)

    def _make_key(self, identifier: str) -&gt; str:
        &quot;&quot;&quot;Create Redis key for identifier.&quot;&quot;&quot;
        # Hash long identifiers to keep key size bounded
        if len(identifier) &gt; 100:
            identifier = hashlib.sha256(identifier.encode()).hexdigest()[:32]
        return f&quot;{self.key_prefix}{identifier}&quot;

    def allow(
        self,
        identifier: str,
        tokens: int = 1
    ) -&gt; DistributedRateLimitResult:
        &quot;&quot;&quot;
        Check rate limit for identifier.

        Args:
            identifier: Unique identifier (user ID, IP, API key)
            tokens: Number of tokens to consume
        &quot;&quot;&quot;
        key = self._make_key(identifier)
        now = time.time()

        try:
            result = self._script(
                keys=[key],
                args=[self.rate, self.capacity, now, tokens, self.ttl]
            )

            allowed = bool(result[0])
            remaining = int(result[1])
            retry_after = float(result[2]) if result[2] &gt; 0 else None

            # Calculate reset time
            if allowed:
                tokens_to_full = self.capacity - remaining
                reset_at = now + (tokens_to_full / self.rate)
            else:
                reset_at = now + retry_after if retry_after else now + self.ttl

            return DistributedRateLimitResult(
                allowed=allowed,
                remaining=remaining,
                retry_after=retry_after,
                limit=self.capacity,
                reset_at=reset_at
            )

        except redis.RedisError as e:
            # Graceful degradation - fail open with warning
            import logging
            logging.warning(f&quot;Redis rate limiter error: {e}, failing open&quot;)

            return DistributedRateLimitResult(
                allowed=True,
                remaining=self.capacity,
                retry_after=None,
                limit=self.capacity,
                reset_at=now + (self.capacity / self.rate)
            )

    def get_status(self, identifier: str) -&gt; dict:
        &quot;&quot;&quot;Get current rate limit status without consuming tokens.&quot;&quot;&quot;
        key = self._make_key(identifier)

        try:
            data = self.redis.hgetall(key)
            if not data:
                return {
                    'tokens': self.capacity,
                    'last_update': time.time(),
                    'exists': False
                }

            return {
                'tokens': float(data.get(b'tokens', self.capacity)),
                'last_update': float(data.get(b'last_update', time.time())),
                'exists': True
            }
        except redis.RedisError:
            return {'error': 'Redis unavailable'}


class RedisClusterRateLimiter:
    &quot;&quot;&quot;
    Rate limiter for Redis Cluster deployment.

    Key consideration: In Redis Cluster, keys are distributed across shards.
    Lua scripts can only operate on keys in the same shard.

    Solution: Use hash tags to ensure related keys go to same shard.
    &quot;&quot;&quot;

    def __init__(
        self,
        redis_cluster: redis.RedisCluster,
        rate: float,
        capacity: int
    ):
        self.redis = redis_cluster
        self.rate = rate
        self.capacity = capacity
        self._script = self.redis.register_script(
            RedisRateLimiter.TOKEN_BUCKET_SCRIPT
        )

    def _make_key(self, identifier: str) -&gt; str:
        &quot;&quot;&quot;
        Create key with hash tag for cluster.

        Hash tags ensure the key goes to specific shard.
        Format: ratelimit:{user_id}:bucket
        The {user_id} part determines the shard.
        &quot;&quot;&quot;
        return f&quot;ratelimit:{{{identifier}}}:bucket&quot;

    def allow(self, identifier: str, tokens: int = 1) -&gt; DistributedRateLimitResult:
        &quot;&quot;&quot;Same as single-node implementation.&quot;&quot;&quot;
        # Implementation identical to RedisRateLimiter.allow()
        # The hash tag in the key ensures cluster compatibility
        pass
</code></pre>
<h4 id="interview-questions-distributed-rate-limiting">Interview Questions: Distributed Rate Limiting</h4>
<div style="background: linear-gradient(135deg, #2d1f3d 0%, #4a3a5d 100%); border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Level 1: Why can't we just use Redis INCR with EXPIRE for rate limiting?</strong></p>
<p>The naive approach has several problems:</p>
<pre><code class="language-python"># Naive implementation - DON'T DO THIS
def naive_rate_limit(redis, key, limit, window):
    current = redis.incr(key)
    if current == 1:
        redis.expire(key, window)
    return current &lt;= limit
</code></pre>
<p><strong>Problems:</strong></p>
<ol>
<li>
<p><strong>Race condition on EXPIRE</strong>: If the process crashes between INCR and EXPIRE, the key never expires, blocking the user forever.</p>
</li>
<li>
<p><strong>Fixed window boundary issue</strong>: Counter resets at arbitrary times based on when the first request arrived, not at consistent intervals.</p>
</li>
<li>
<p><strong>No burst control</strong>: Can't allow bursts while maintaining average rate.</p>
</li>
</ol>
<p><strong>Partial fix using SET with NX and EX:</strong></p>
<pre><code class="language-python">def better_naive(redis, key, limit, window):
    # Atomic increment that sets expiry on creation
    pipe = redis.pipeline()
    pipe.incr(key)
    pipe.expire(key, window)
    current, _ = pipe.execute()
    return current &lt;= limit
</code></pre>
<p>Still has fixed window problem. Token bucket or sliding window is better.</p>
<p><span style="background: linear-gradient(90deg, rgba(46,204,113,0.3) 0%, rgba(46,204,113,0.1) 100%); padding: 2px 8px; border-radius: 4px"><strong>Key Insight</strong>: Simple INCR doesn't model time-based token replenishment. You need to track both count AND time to properly refill tokens.</span></p>
<hr />
<p><strong>Level 2: How do you handle clock skew between application servers when using distributed rate limiting?</strong></p>
<p>Clock skew occurs when different servers have slightly different system times. This can cause:</p>
<ul>
<li>Tokens being &quot;refilled from the future&quot; if server B's clock is ahead</li>
<li>Tokens being consumed before they exist if server A's clock is behind</li>
</ul>
<p><strong>Mitigation Strategies:</strong></p>
<ol>
<li><strong>Use Redis Server Time:</strong></li>
</ol>
<pre><code class="language-lua">-- In Lua script, use Redis TIME command
local time_result = redis.call('TIME')
local now = tonumber(time_result[1]) + tonumber(time_result[2]) / 1000000
</code></pre>
<p>All calculations use Redis's clock, which is consistent.</p>
<ol start="2">
<li><strong>Bound the Skew:</strong></li>
</ol>
<pre><code class="language-python">def get_rate_limit_time() -&gt; float:
    &quot;&quot;&quot;Get time for rate limiting with skew protection.&quot;&quot;&quot;
    local_time = time.time()

    # Fetch Redis time periodically (cache for 1 second)
    if needs_refresh():
        redis_time = get_redis_time()
        skew = redis_time - local_time

        # Alert if skew is significant
        if abs(skew) &gt; 1.0:
            alert_ops(f&quot;Clock skew detected: {skew}s&quot;)

        cache_skew(skew)

    # Apply cached skew adjustment
    return local_time + get_cached_skew()
</code></pre>
<ol start="3">
<li><strong>Monotonic Sequence Numbers:</strong></li>
</ol>
<pre><code class="language-python">class SkewResistantLimiter:
    &quot;&quot;&quot;
    Use sequence numbers instead of timestamps.

    Each &quot;tick&quot; is a rate limit period, not a timestamp.
    Ticks are incremented by a single source (Redis).
    &quot;&quot;&quot;

    def get_current_tick(self) -&gt; int:
        &quot;&quot;&quot;Get current tick from Redis.&quot;&quot;&quot;
        # Redis INCR provides monotonic sequence
        # Tick changes every 'period' seconds
        period = self.window_seconds

        # Use Redis TIME for consistency
        redis_time = self.redis.time()
        return int(redis_time[0] // period)
</code></pre>
<p><span style="background: linear-gradient(90deg, rgba(46,204,113,0.3) 0%, rgba(46,204,113,0.1) 100%); padding: 2px 8px; border-radius: 4px"><strong>Trade-off</strong>: Using Redis TIME adds a round trip. For high-throughput systems, accept small clock skew tolerance (few hundred ms) and use local time with periodic sync verification.</span></p>
<hr />
<p><strong>Level 3: How would you implement a hierarchical rate limiter that enforces limits at user, organization, and global levels simultaneously?</strong></p>
<p>This is a common enterprise requirement where:</p>
<ul>
<li>User: 100 req/min</li>
<li>Organization (all users): 10,000 req/min</li>
<li>Global (all organizations): 1,000,000 req/min</li>
</ul>
<pre><code class="language-python">from dataclasses import dataclass
from typing import List, Optional, Dict
from enum import Enum

class LimitLevel(Enum):
    USER = &quot;user&quot;
    ORGANIZATION = &quot;org&quot;
    GLOBAL = &quot;global&quot;

@dataclass
class HierarchicalLimit:
    level: LimitLevel
    rate: float
    capacity: int
    key_template: str  # e.g., &quot;user:{user_id}&quot;, &quot;org:{org_id}&quot;

@dataclass
class HierarchicalResult:
    allowed: bool
    limiting_level: Optional[LimitLevel]
    limits: Dict[LimitLevel, dict]  # Status at each level

class HierarchicalRateLimiter:
    &quot;&quot;&quot;
    Multi-level rate limiter.

    Design considerations:
    1. Check limits from most specific (user) to least specific (global)
    2. Early exit on first denial saves Redis calls
    3. Atomic consumption at all levels to prevent partial updates
    4. Rollback mechanism if higher level denies after lower allows
    &quot;&quot;&quot;

    # Lua script that checks and consumes at all levels atomically
    HIERARCHICAL_SCRIPT = &quot;&quot;&quot;
    -- KEYS: rate limit keys for each level (most specific first)
    -- ARGV: [rate, capacity, now, requested, ttl] repeated for each level

    local num_levels = #KEYS
    local args_per_level = 5

    local results = {}
    local all_allowed = true
    local first_denied_level = nil

    -- First pass: check all levels
    for i = 1, num_levels do
        local key = KEYS[i]
        local offset = (i - 1) * args_per_level
        local rate = tonumber(ARGV[offset + 1])
        local capacity = tonumber(ARGV[offset + 2])
        local now = tonumber(ARGV[offset + 3])
        local requested = tonumber(ARGV[offset + 4])
        local ttl = tonumber(ARGV[offset + 5])

        local data = redis.call('HMGET', key, 'tokens', 'last_update')
        local tokens = tonumber(data[1]) or capacity
        local last_update = tonumber(data[2]) or now

        local elapsed = math.max(0, now - last_update)
        tokens = math.min(capacity, tokens + elapsed * rate)

        results[i] = {
            key = key,
            tokens = tokens,
            last_update = now,
            rate = rate,
            capacity = capacity,
            ttl = ttl,
            requested = requested,
            allowed = tokens &gt;= requested
        }

        if not results[i].allowed then
            all_allowed = false
            if first_denied_level == nil then
                first_denied_level = i
            end
        end
    end

    -- Second pass: consume tokens only if all levels allow
    if all_allowed then
        for i = 1, num_levels do
            local r = results[i]
            local new_tokens = r.tokens - r.requested
            redis.call('HMSET', r.key, 'tokens', new_tokens, 'last_update', r.last_update)
            redis.call('EXPIRE', r.key, r.ttl)
            results[i].remaining = math.floor(new_tokens)
        end
    else
        -- Return remaining without consuming
        for i = 1, num_levels do
            results[i].remaining = math.floor(results[i].tokens)
        end
    end

    -- Build response: [allowed (0/1), denied_level, level1_remaining, level2_remaining, ...]
    local response = {all_allowed and 1 or 0, first_denied_level or 0}
    for i = 1, num_levels do
        table.insert(response, results[i].remaining)
    end

    return response
    &quot;&quot;&quot;

    def __init__(
        self,
        redis_client: redis.Redis,
        limits: List[HierarchicalLimit]
    ):
        &quot;&quot;&quot;
        Args:
            redis_client: Redis connection
            limits: List of limits from most specific to least specific
        &quot;&quot;&quot;
        self.redis = redis_client
        self.limits = limits
        self._script = redis_client.register_script(self.HIERARCHICAL_SCRIPT)

    def allow(
        self,
        identifiers: Dict[LimitLevel, str],
        tokens: int = 1
    ) -&gt; HierarchicalResult:
        &quot;&quot;&quot;
        Check rate limit at all levels.

        Args:
            identifiers: Map of level to identifier value
                e.g., {USER: &quot;user123&quot;, ORGANIZATION: &quot;org456&quot;, GLOBAL: &quot;global&quot;}
            tokens: Tokens to consume
        &quot;&quot;&quot;
        now = time.time()

        keys = []
        args = []

        for limit in self.limits:
            identifier = identifiers.get(limit.level, &quot;default&quot;)
            key = limit.key_template.format(**{limit.level.value: identifier})
            keys.append(f&quot;ratelimit:{key}&quot;)

            args.extend([
                limit.rate,
                limit.capacity,
                now,
                tokens,
                3600  # TTL
            ])

        try:
            result = self._script(keys=keys, args=args)

            allowed = bool(result[0])
            denied_level_idx = result[1]
            remaining_values = result[2:]

            # Build detailed response
            limits_status = {}
            for i, limit in enumerate(self.limits):
                limits_status[limit.level] = {
                    'remaining': remaining_values[i],
                    'limit': limit.capacity,
                    'rate': limit.rate
                }

            limiting_level = None
            if not allowed and denied_level_idx &gt; 0:
                limiting_level = self.limits[denied_level_idx - 1].level

            return HierarchicalResult(
                allowed=allowed,
                limiting_level=limiting_level,
                limits=limits_status
            )

        except redis.RedisError as e:
            # Fail open with all limits at capacity
            return HierarchicalResult(
                allowed=True,
                limiting_level=None,
                limits={l.level: {'remaining': l.capacity, 'limit': l.capacity}
                        for l in self.limits}
            )

# Usage
limiter = HierarchicalRateLimiter(
    redis_client=redis.Redis(),
    limits=[
        HierarchicalLimit(LimitLevel.USER, rate=100/60, capacity=100,
                          key_template=&quot;user:{user}&quot;),
        HierarchicalLimit(LimitLevel.ORGANIZATION, rate=10000/60, capacity=10000,
                          key_template=&quot;org:{org}&quot;),
        HierarchicalLimit(LimitLevel.GLOBAL, rate=1000000/60, capacity=100000,
                          key_template=&quot;global&quot;),
    ]
)

result = limiter.allow(
    identifiers={
        LimitLevel.USER: &quot;user123&quot;,
        LimitLevel.ORGANIZATION: &quot;org456&quot;,
        LimitLevel.GLOBAL: &quot;global&quot;
    }
)
</code></pre>
<p><span style="background: linear-gradient(90deg, rgba(46,204,113,0.3) 0%, rgba(46,204,113,0.1) 100%); padding: 2px 8px; border-radius: 4px"><strong>Design Choice</strong>: All-or-nothing consumption prevents partial state corruption. If global limit denies but user limit would allow, we don't consume user tokens - this prevents confusing scenarios where users see their quota decrease without successful requests.</span></p>
</div>
<hr />
<h2 id="testing-strategies">Testing Strategies</h2>
<h3 id="unit-testing-rate-limiters">Unit Testing Rate Limiters</h3>
<div style="background: #f0fdf4; border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Key Testing Challenges:</strong></p>
<ol>
<li><strong>Time-dependent behavior</strong>: Tests must control time</li>
<li><strong>Concurrency</strong>: Race conditions are hard to reproduce</li>
<li><strong>Boundary conditions</strong>: Edge cases at limit boundaries</li>
<li><strong>Distributed state</strong>: Testing Redis interactions</li>
</ol>
</div>
<pre><code class="language-python">import unittest
from unittest.mock import Mock, patch, MagicMock
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

class MockTime:
    &quot;&quot;&quot;Mock time for deterministic testing.&quot;&quot;&quot;

    def __init__(self, start_time: float = 0.0):
        self._current_time = start_time
        self._monotonic_time = 0.0

    def time(self) -&gt; float:
        return self._current_time

    def monotonic(self) -&gt; float:
        return self._monotonic_time

    def sleep(self, seconds: float) -&gt; None:
        self._current_time += seconds
        self._monotonic_time += seconds

    def advance(self, seconds: float) -&gt; None:
        &quot;&quot;&quot;Advance time without blocking.&quot;&quot;&quot;
        self._current_time += seconds
        self._monotonic_time += seconds


class TestTokenBucket(unittest.TestCase):
    &quot;&quot;&quot;Comprehensive token bucket tests.&quot;&quot;&quot;

    def setUp(self):
        self.mock_time = MockTime(start_time=1000.0)
        self.time_patcher = patch('time.time', self.mock_time.time)
        self.monotonic_patcher = patch('time.monotonic', self.mock_time.monotonic)
        self.time_patcher.start()
        self.monotonic_patcher.start()

    def tearDown(self):
        self.time_patcher.stop()
        self.monotonic_patcher.stop()

    def test_initial_state_is_full(self):
        &quot;&quot;&quot;Bucket should start at full capacity.&quot;&quot;&quot;
        bucket = TokenBucket(rate=10, capacity=20)

        self.assertEqual(bucket.peek(), 20)

    def test_consume_reduces_tokens(self):
        &quot;&quot;&quot;Consuming tokens should reduce available count.&quot;&quot;&quot;
        bucket = TokenBucket(rate=10, capacity=20)

        result = bucket.allow(tokens=5)

        self.assertTrue(result.allowed)
        self.assertEqual(result.remaining, 15)

    def test_cannot_consume_more_than_available(self):
        &quot;&quot;&quot;Request should be denied if not enough tokens.&quot;&quot;&quot;
        bucket = TokenBucket(rate=10, capacity=5)

        result = bucket.allow(tokens=10)

        self.assertFalse(result.allowed)
        self.assertIsNotNone(result.retry_after)

    def test_tokens_refill_over_time(self):
        &quot;&quot;&quot;Tokens should refill based on elapsed time and rate.&quot;&quot;&quot;
        bucket = TokenBucket(rate=10, capacity=20)

        # Consume all tokens
        bucket.allow(tokens=20)
        self.assertEqual(bucket.peek(), 0)

        # Advance time by 1 second (should add 10 tokens)
        self.mock_time.advance(1.0)

        self.assertEqual(bucket.peek(), 10)

    def test_tokens_cap_at_capacity(self):
        &quot;&quot;&quot;Tokens should not exceed capacity.&quot;&quot;&quot;
        bucket = TokenBucket(rate=10, capacity=20)

        # Consume some tokens
        bucket.allow(tokens=5)
        self.assertEqual(bucket.peek(), 15)

        # Wait long enough to theoretically refill 100 tokens
        self.mock_time.advance(10.0)

        # Should be capped at 20
        self.assertEqual(bucket.peek(), 20)

    def test_fractional_refill(self):
        &quot;&quot;&quot;Tokens should refill with fractional precision.&quot;&quot;&quot;
        bucket = TokenBucket(rate=10, capacity=20)

        bucket.allow(tokens=20)

        # Advance 0.5 seconds (should add 5 tokens)
        self.mock_time.advance(0.5)

        result = bucket.allow(tokens=5)
        self.assertTrue(result.allowed)

        # Trying to consume 1 more should fail
        result = bucket.allow(tokens=1)
        self.assertFalse(result.allowed)

    def test_retry_after_calculation(self):
        &quot;&quot;&quot;Retry-after should indicate when request can succeed.&quot;&quot;&quot;
        bucket = TokenBucket(rate=10, capacity=20)

        bucket.allow(tokens=20)

        # Try to consume 5 tokens (need 0.5 seconds)
        result = bucket.allow(tokens=5)

        self.assertFalse(result.allowed)
        self.assertAlmostEqual(result.retry_after, 0.5, places=2)

    def test_burst_behavior(self):
        &quot;&quot;&quot;Bucket should allow burst up to capacity.&quot;&quot;&quot;
        bucket = TokenBucket(rate=1, capacity=100)  # 1/sec but burst of 100

        # Should allow 100 requests immediately
        for i in range(100):
            result = bucket.allow()
            self.assertTrue(result.allowed, f&quot;Request {i+1} should be allowed&quot;)

        # Request 101 should be denied
        result = bucket.allow()
        self.assertFalse(result.allowed)

    def test_request_exceeding_capacity_never_succeeds(self):
        &quot;&quot;&quot;Request larger than capacity should always fail.&quot;&quot;&quot;
        bucket = TokenBucket(rate=10, capacity=5)

        result = bucket.allow(tokens=10)

        self.assertFalse(result.allowed)
        self.assertIsNone(result.retry_after)  # Will never succeed


class TestTokenBucketConcurrency(unittest.TestCase):
    &quot;&quot;&quot;Test thread safety of token bucket.&quot;&quot;&quot;

    def test_concurrent_consumption_is_safe(self):
        &quot;&quot;&quot;Multiple threads consuming tokens should not cause over-consumption.&quot;&quot;&quot;
        bucket = TokenBucket(rate=0, capacity=100)  # No refill

        successful_requests = []

        def try_consume():
            result = bucket.allow(tokens=1)
            return result.allowed

        with ThreadPoolExecutor(max_workers=200) as executor:
            futures = [executor.submit(try_consume) for _ in range(200)]
            results = [f.result() for f in as_completed(futures)]

        allowed_count = sum(results)

        # Exactly 100 should succeed
        self.assertEqual(allowed_count, 100)

    def test_concurrent_peek_is_consistent(self):
        &quot;&quot;&quot;Peek should return consistent values under concurrency.&quot;&quot;&quot;
        bucket = TokenBucket(rate=0, capacity=100)

        def peek_and_consume():
            before = bucket.peek()
            bucket.allow(tokens=1)
            after = bucket.peek()
            return before &gt;= after  # Should always be true or equal

        with ThreadPoolExecutor(max_workers=50) as executor:
            futures = [executor.submit(peek_and_consume) for _ in range(100)]
            results = [f.result() for f in futures]

        self.assertTrue(all(results))


class TestSlidingWindow(unittest.TestCase):
    &quot;&quot;&quot;Test sliding window rate limiter.&quot;&quot;&quot;

    def setUp(self):
        self.mock_time = MockTime(start_time=1000.0)
        self.time_patcher = patch('time.time', self.mock_time.time)
        self.time_patcher.start()

    def tearDown(self):
        self.time_patcher.stop()

    def test_boundary_protection(self):
        &quot;&quot;&quot;Sliding window should prevent boundary abuse.&quot;&quot;&quot;
        limiter = SlidingWindowCounter(limit=100, window_seconds=60)

        # Consume at end of window 1
        self.mock_time._current_time = 1059.0  # 59 seconds in
        for _ in range(100):
            limiter.allow(&quot;user&quot;)

        # Move to start of window 2
        self.mock_time._current_time = 1060.0

        # Should not allow 100 more immediately
        # Because weighted count includes previous window
        allowed = 0
        for _ in range(100):
            if limiter.allow(&quot;user&quot;).allowed:
                allowed += 1

        # Should allow significantly fewer than 100
        self.assertLess(allowed, 10)


class TestRedisRateLimiter(unittest.TestCase):
    &quot;&quot;&quot;Test Redis-based rate limiter with mocked Redis.&quot;&quot;&quot;

    def test_lua_script_execution(self):
        &quot;&quot;&quot;Test that Lua script is called with correct parameters.&quot;&quot;&quot;
        mock_redis = MagicMock()
        mock_script = MagicMock()
        mock_script.return_value = [1, 19, -1]  # allowed, remaining, retry_after
        mock_redis.register_script.return_value = mock_script

        limiter = RedisRateLimiter(
            redis_client=mock_redis,
            rate=10,
            capacity=20,
            key_prefix=&quot;test:&quot;,
            ttl=3600
        )

        result = limiter.allow(&quot;user123&quot;)

        self.assertTrue(result.allowed)
        self.assertEqual(result.remaining, 19)

        # Verify script was called with correct key
        mock_script.assert_called_once()
        call_args = mock_script.call_args
        self.assertEqual(call_args[1]['keys'], ['test:user123'])

    def test_graceful_degradation_on_redis_error(self):
        &quot;&quot;&quot;Should fail open when Redis is unavailable.&quot;&quot;&quot;
        mock_redis = MagicMock()
        mock_script = MagicMock()
        mock_script.side_effect = redis.RedisError(&quot;Connection refused&quot;)
        mock_redis.register_script.return_value = mock_script

        limiter = RedisRateLimiter(
            redis_client=mock_redis,
            rate=10,
            capacity=20
        )

        result = limiter.allow(&quot;user123&quot;)

        # Should allow when Redis fails (fail open)
        self.assertTrue(result.allowed)


class TestRateLimiterIntegration(unittest.TestCase):
    &quot;&quot;&quot;Integration tests with real Redis (requires running Redis).&quot;&quot;&quot;

    @unittest.skipUnless(
        os.environ.get('REDIS_TEST_HOST'),
        &quot;Set REDIS_TEST_HOST to run Redis integration tests&quot;
    )
    def test_distributed_consistency(self):
        &quot;&quot;&quot;Multiple clients should see consistent rate limits.&quot;&quot;&quot;
        redis_client = redis.Redis(
            host=os.environ['REDIS_TEST_HOST'],
            decode_responses=False
        )

        # Clean up before test
        redis_client.delete(&quot;ratelimit:integration_test&quot;)

        limiter1 = RedisRateLimiter(redis_client, rate=10, capacity=10)
        limiter2 = RedisRateLimiter(redis_client, rate=10, capacity=10)

        # Consume from both &quot;servers&quot;
        results = []
        for _ in range(5):
            results.append(limiter1.allow(&quot;integration_test&quot;))
            results.append(limiter2.allow(&quot;integration_test&quot;))

        allowed = sum(1 for r in results if r.allowed)

        # Exactly 10 should be allowed
        self.assertEqual(allowed, 10)
</code></pre>
<h4 id="interview-questions-testing-strategies">Interview Questions: Testing Strategies</h4>
<div style="background: linear-gradient(135deg, #4a3a5d 0%, #2d1f3d 100%); border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Level 1: How do you test time-dependent code without making tests slow or flaky?</strong></p>
<p><strong>The Problem:</strong> Real time-based tests are:</p>
<ul>
<li>Slow (waiting for actual seconds/minutes)</li>
<li>Flaky (timing variations cause intermittent failures)</li>
<li>Non-deterministic (hard to reproduce failures)</li>
</ul>
<p><strong>Solution: Time Abstraction</strong></p>
<pre><code class="language-python">from abc import ABC, abstractmethod

class Clock(ABC):
    &quot;&quot;&quot;Abstract clock interface for time operations.&quot;&quot;&quot;

    @abstractmethod
    def now(self) -&gt; float:
        &quot;&quot;&quot;Return current timestamp.&quot;&quot;&quot;
        pass

    @abstractmethod
    def monotonic(self) -&gt; float:
        &quot;&quot;&quot;Return monotonic time.&quot;&quot;&quot;
        pass

class RealClock(Clock):
    &quot;&quot;&quot;Production clock using system time.&quot;&quot;&quot;

    def now(self) -&gt; float:
        return time.time()

    def monotonic(self) -&gt; float:
        return time.monotonic()

class FakeClock(Clock):
    &quot;&quot;&quot;Fake clock for testing with controllable time.&quot;&quot;&quot;

    def __init__(self, initial_time: float = 0.0):
        self._time = initial_time
        self._monotonic = 0.0

    def now(self) -&gt; float:
        return self._time

    def monotonic(self) -&gt; float:
        return self._monotonic

    def advance(self, seconds: float) -&gt; None:
        &quot;&quot;&quot;Advance time by given seconds.&quot;&quot;&quot;
        self._time += seconds
        self._monotonic += seconds

    def set_time(self, timestamp: float) -&gt; None:
        &quot;&quot;&quot;Set absolute time.&quot;&quot;&quot;
        self._time = timestamp

# Usage in rate limiter
class TokenBucket:
    def __init__(self, rate: float, capacity: int, clock: Clock = None):
        self.clock = clock or RealClock()
        # ... rest of init

    def allow(self, tokens: int = 1) -&gt; RateLimitResult:
        now = self.clock.monotonic()
        # ... rest of implementation

# In tests
def test_token_refill():
    fake_clock = FakeClock()
    bucket = TokenBucket(rate=10, capacity=20, clock=fake_clock)

    bucket.allow(tokens=20)
    fake_clock.advance(1.0)  # Instant &quot;time travel&quot;

    result = bucket.allow(tokens=10)
    assert result.allowed
</code></pre>
<p><span style="background: linear-gradient(90deg, rgba(46,204,113,0.3) 0%, rgba(46,204,113,0.1) 100%); padding: 2px 8px; border-radius: 4px"><strong>Design Pattern</strong>: Dependency injection of the clock makes tests deterministic while keeping production code simple. This is the &quot;humble object&quot; pattern from TDD.</span></p>
<hr />
<p><strong>Level 2: How do you test for race conditions in concurrent rate limiter implementations?</strong></p>
<p>Race conditions are notoriously hard to test because they depend on specific thread scheduling. Here are strategies:</p>
<p><strong>1. Stress Testing with High Contention:</strong></p>
<pre><code class="language-python">def test_concurrent_safety():
    &quot;&quot;&quot;Stress test for race conditions.&quot;&quot;&quot;
    limiter = TokenBucket(rate=0, capacity=1000)  # No refill

    results = []
    barrier = threading.Barrier(100)  # Synchronize start

    def worker():
        barrier.wait()  # All threads start simultaneously
        local_results = []
        for _ in range(100):
            result = limiter.allow()
            local_results.append(result.allowed)
        return local_results

    with ThreadPoolExecutor(max_workers=100) as executor:
        futures = [executor.submit(worker) for _ in range(100)]
        for future in futures:
            results.extend(future.result())

    # With 100 threads × 100 requests = 10,000 attempts
    # Exactly 1000 should succeed
    allowed = sum(results)
    assert allowed == 1000, f&quot;Got {allowed}, expected 1000&quot;
</code></pre>
<p><strong>2. ThreadSanitizer / Race Detector:</strong></p>
<pre><code class="language-bash"># Python with thread sanitizer (limited support)
# Better: Go has built-in race detector
go test -race ./...
</code></pre>
<p><strong>3. Deterministic Scheduling with Hypothesis:</strong></p>
<pre><code class="language-python">from hypothesis import given, strategies as st, settings
from hypothesis.stateful import RuleBasedStateMachine, rule, Bundle

class RateLimiterStateMachine(RuleBasedStateMachine):
    &quot;&quot;&quot;Property-based testing for rate limiter invariants.&quot;&quot;&quot;

    def __init__(self):
        super().__init__()
        self.limiter = TokenBucket(rate=10, capacity=100)
        self.consumed = 0

    @rule(tokens=st.integers(min_value=1, max_value=10))
    def consume(self, tokens):
        result = self.limiter.allow(tokens)
        if result.allowed:
            self.consumed += tokens

        # Invariant: consumed should never exceed capacity
        assert self.consumed &lt;= 100

TestRateLimiter = RateLimiterStateMachine.TestCase
</code></pre>
<p><span style="background: linear-gradient(90deg, rgba(46,204,113,0.3) 0%, rgba(46,204,113,0.1) 100%); padding: 2px 8px; border-radius: 4px"><strong>Key Insight</strong>: Testing proves the presence of bugs, not their absence. For concurrent code, combine stress tests (find obvious races), property tests (find subtle invariant violations), and code review focused on critical sections.</span></p>
<hr />
<p><strong>Level 3: How would you implement chaos testing for a distributed rate limiter to verify it behaves correctly during partial failures?</strong></p>
<p>Chaos testing validates behavior during realistic failure scenarios:</p>
<pre><code class="language-python">import random
from contextlib import contextmanager
from typing import Generator
from unittest.mock import patch

class ChaosProxy:
    &quot;&quot;&quot;
    Proxy that injects failures into Redis operations.

    Failure modes:
    1. Connection failures
    2. Timeout/slow responses
    3. Partial failures (some keys fail)
    4. Data corruption (unlikely but testable)
    &quot;&quot;&quot;

    def __init__(
        self,
        real_redis: redis.Redis,
        failure_rate: float = 0.1,
        latency_ms: tuple = (0, 100)
    ):
        self.real_redis = real_redis
        self.failure_rate = failure_rate
        self.latency_ms = latency_ms
        self.failure_count = 0
        self.success_count = 0

    def _maybe_fail(self) -&gt; None:
        &quot;&quot;&quot;Randomly inject failure.&quot;&quot;&quot;
        if random.random() &lt; self.failure_rate:
            self.failure_count += 1
            raise redis.RedisError(&quot;Chaos: Simulated failure&quot;)

    def _maybe_delay(self) -&gt; None:
        &quot;&quot;&quot;Randomly inject latency.&quot;&quot;&quot;
        delay = random.uniform(*self.latency_ms) / 1000
        time.sleep(delay)

    def execute_command(self, *args, **kwargs):
        self._maybe_delay()
        self._maybe_fail()
        self.success_count += 1
        return self.real_redis.execute_command(*args, **kwargs)

    def __getattr__(self, name):
        &quot;&quot;&quot;Proxy all other methods to real Redis.&quot;&quot;&quot;
        attr = getattr(self.real_redis, name)
        if callable(attr):
            def wrapper(*args, **kwargs):
                self._maybe_delay()
                self._maybe_fail()
                return attr(*args, **kwargs)
            return wrapper
        return attr


class ChaosTestSuite:
    &quot;&quot;&quot;
    Chaos test scenarios for distributed rate limiter.
    &quot;&quot;&quot;

    def __init__(self, real_redis: redis.Redis):
        self.real_redis = real_redis

    @contextmanager
    def redis_failures(
        self,
        failure_rate: float
    ) -&gt; Generator[ChaosProxy, None, None]:
        &quot;&quot;&quot;Context manager for injecting Redis failures.&quot;&quot;&quot;
        chaos = ChaosProxy(self.real_redis, failure_rate=failure_rate)
        yield chaos
        print(f&quot;Chaos stats: {chaos.failure_count} failures, &quot;
              f&quot;{chaos.success_count} successes&quot;)

    def test_graceful_degradation(self):
        &quot;&quot;&quot;
        Test: System should remain functional during partial Redis failures.

        Expected behavior:
        - Some requests may fail temporarily
        - System should not crash
        - Metrics should be available
        - Recovery should be automatic when Redis returns
        &quot;&quot;&quot;
        with self.redis_failures(failure_rate=0.3) as chaos:
            limiter = RedisRateLimiter(chaos, rate=100, capacity=1000)

            results = {'allowed': 0, 'denied': 0, 'errors': 0}

            for _ in range(1000):
                try:
                    result = limiter.allow(&quot;test_user&quot;)
                    if result.allowed:
                        results['allowed'] += 1
                    else:
                        results['denied'] += 1
                except Exception as e:
                    results['errors'] += 1

            # Verify graceful degradation
            # - Should have some successful operations
            # - Errors should be bounded
            assert results['allowed'] &gt; 0, &quot;No requests succeeded&quot;
            assert results['errors'] &lt; 400, &quot;Too many errors&quot;

    def test_recovery_after_outage(self):
        &quot;&quot;&quot;
        Test: Rate limiting should resume correctly after Redis recovers.
        &quot;&quot;&quot;
        limiter = RedisRateLimiter(self.real_redis, rate=10, capacity=100)

        # Normal operation
        for _ in range(50):
            limiter.allow(&quot;recovery_test&quot;)

        # Simulate outage (patch Redis to fail)
        with patch.object(self.real_redis, 'execute_command',
                         side_effect=redis.RedisError(&quot;Outage&quot;)):
            # Requests during outage (fail open)
            for _ in range(100):
                result = limiter.allow(&quot;recovery_test&quot;)
                # Should fail open during outage

        # Recovery - Redis working again
        # State should be preserved from before outage
        result = limiter.allow(&quot;recovery_test&quot;)

        # Verify state continuity
        status = limiter.get_status(&quot;recovery_test&quot;)
        assert status.get('exists', False), &quot;State lost after outage&quot;

    def test_network_partition(self):
        &quot;&quot;&quot;
        Test: Behavior when Redis is partitioned from some app servers.

        Simulates: Server A can reach Redis, Server B cannot.
        &quot;&quot;&quot;
        limiter_a = RedisRateLimiter(self.real_redis, rate=10, capacity=100)

        # Server B has failing Redis
        failing_redis = ChaosProxy(self.real_redis, failure_rate=1.0)
        limiter_b = RedisRateLimiter(failing_redis, rate=10, capacity=100)

        # Both servers process requests
        results_a = [limiter_a.allow(&quot;partition_test&quot;).allowed for _ in range(50)]
        results_b = [limiter_b.allow(&quot;partition_test&quot;).allowed for _ in range(50)]

        # Server A should rate limit normally
        assert sum(results_a) &lt;= 100

        # Server B should fail open (all allowed in degraded mode)
        assert sum(results_b) == 50  # All allowed when failing open

        # WARNING: This means user could exceed limit during partition!
        # This is acceptable trade-off for availability.

# Running chaos tests
def run_chaos_tests():
    redis_client = redis.Redis(host='localhost')
    suite = ChaosTestSuite(redis_client)

    print(&quot;Running graceful degradation test...&quot;)
    suite.test_graceful_degradation()

    print(&quot;Running recovery test...&quot;)
    suite.test_recovery_after_outage()

    print(&quot;Running partition test...&quot;)
    suite.test_network_partition()

    print(&quot;All chaos tests passed!&quot;)
</code></pre>
<p><span style="background: linear-gradient(90deg, rgba(46,204,113,0.3) 0%, rgba(46,204,113,0.1) 100%); padding: 2px 8px; border-radius: 4px"><strong>Production Reality</strong>: Chaos testing reveals that distributed rate limiting has inherent CAP theorem trade-offs. During partitions, you must choose between accuracy (fail closed, potentially causing outage) or availability (fail open, potentially allowing abuse). Most systems choose availability.</span></p>
</div>
<hr />
<h2 id="algorithm-comparison">Algorithm Comparison</h2>
<div style="background: #eff6ff; border-radius: 12px; padding: 24px; margin: 20px 0">
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Time</th>
<th>Space</th>
<th>Accuracy</th>
<th>Burst</th>
<th>Best For</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Token Bucket</strong></td>
<td>O(1)</td>
<td>O(1)/key</td>
<td>High</td>
<td>Yes</td>
<td>API rate limiting</td>
</tr>
<tr>
<td><strong>Leaky Bucket</strong></td>
<td>O(1)</td>
<td>O(1)/key</td>
<td>High</td>
<td>No</td>
<td>Traffic shaping</td>
</tr>
<tr>
<td><strong>Fixed Window</strong></td>
<td>O(1)</td>
<td>O(1)/key</td>
<td>Low</td>
<td>N/A</td>
<td>Rough estimates</td>
</tr>
<tr>
<td><strong>Sliding Window Counter</strong></td>
<td>O(1)</td>
<td>O(1)/key</td>
<td>~94%</td>
<td>N/A</td>
<td>Smooth rate limiting</td>
</tr>
<tr>
<td><strong>Sliding Window Log</strong></td>
<td>O(n)</td>
<td>O(n)/key</td>
<td>100%</td>
<td>N/A</td>
<td>Compliance/billing</td>
</tr>
</tbody>
</table>
<p><strong>Memory Per Key:</strong></p>
<ul>
<li>Token Bucket: ~32 bytes (tokens + timestamp + padding)</li>
<li>Sliding Window Counter: ~48 bytes (2 counters + 2 timestamps)</li>
<li>Sliding Window Log: 8 bytes × requests in window</li>
</ul>
</div>
<hr />
<h2 id="production-considerations">Production Considerations</h2>
<h3 id="http-response-headers">HTTP Response Headers</h3>
<div style="background: #f0fdf4; border-radius: 12px; padding: 20px; margin: 16px 0">
<pre><code class="language-http">HTTP/1.1 429 Too Many Requests
X-RateLimit-Limit: 100
X-RateLimit-Remaining: 0
X-RateLimit-Reset: 1706012400
Retry-After: 30
Content-Type: application/json

{
    &quot;error&quot;: &quot;rate_limit_exceeded&quot;,
    &quot;message&quot;: &quot;Too many requests. Please retry after 30 seconds.&quot;,
    &quot;documentation_url&quot;: &quot;https://api.example.com/docs/rate-limits&quot;
}
</code></pre>
<p><strong>Standard Headers:</strong></p>
<table>
<thead>
<tr>
<th>Header</th>
<th>Description</th>
<th>Format</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>X-RateLimit-Limit</code></td>
<td>Maximum requests per window</td>
<td>Integer</td>
</tr>
<tr>
<td><code>X-RateLimit-Remaining</code></td>
<td>Remaining requests in window</td>
<td>Integer</td>
</tr>
<tr>
<td><code>X-RateLimit-Reset</code></td>
<td>When the limit resets</td>
<td>Unix timestamp</td>
</tr>
<tr>
<td><code>Retry-After</code></td>
<td>Seconds until retry (on 429)</td>
<td>Integer seconds</td>
</tr>
</tbody>
</table>
</div>
<h3 id="monitoring-and-alerting">Monitoring and Alerting</h3>
<pre><code class="language-python">from dataclasses import dataclass
from typing import Dict
import prometheus_client as prom

# Prometheus metrics
rate_limit_requests = prom.Counter(
    'rate_limit_requests_total',
    'Total rate limit checks',
    ['key_type', 'result']  # result: allowed, denied
)

rate_limit_tokens = prom.Gauge(
    'rate_limit_tokens_remaining',
    'Current tokens remaining',
    ['key']
)

rate_limit_latency = prom.Histogram(
    'rate_limit_check_duration_seconds',
    'Time to check rate limit',
    buckets=[.001, .005, .01, .025, .05, .1]
)

class InstrumentedRateLimiter:
    &quot;&quot;&quot;Rate limiter with observability.&quot;&quot;&quot;

    def __init__(self, limiter: RedisRateLimiter, key_type: str = &quot;user&quot;):
        self.limiter = limiter
        self.key_type = key_type

    def allow(self, key: str, tokens: int = 1) -&gt; DistributedRateLimitResult:
        with rate_limit_latency.time():
            result = self.limiter.allow(key, tokens)

        # Record metrics
        status = &quot;allowed&quot; if result.allowed else &quot;denied&quot;
        rate_limit_requests.labels(
            key_type=self.key_type,
            result=status
        ).inc()

        rate_limit_tokens.labels(key=key).set(result.remaining)

        return result
</code></pre>
<hr />
<h2 id="cross-references">Cross-References</h2>
<ul>
<li><a href="/topics/system-design/distributed-systems">[distributed-systems]</a> - CAP theorem implications for rate limiting</li>
<li><a href="/topics/databases/redis">[redis]</a> - Redis data structures and Lua scripting</li>
<li><a href="/topics/system-design/api-gateway">[api-gateway]</a> - Gateway-level rate limiting</li>
<li><a href="/topics/system-design/caching">[caching]</a> - Caching rate limit decisions</li>
<li><a href="/topics/design-patterns/circuit-breaker">[circuit-breaker]</a> - Complementary resilience pattern</li>
<li><a href="/topics/system-design/load-balancing">[load-balancing]</a> - Sticky sessions for local rate limiting</li>
</ul>
<hr />
<h2 id="key-takeaways">Key Takeaways</h2>
<div style="background: #f0fdf4; border-radius: 12px; padding: 24px; margin: 20px 0">
<ol>
<li>
<p><strong>Algorithm Selection</strong>: Token bucket for APIs (allows bursts), sliding window for strict enforcement, leaky bucket for traffic shaping</p>
</li>
<li>
<p><strong>Distributed Systems</strong>: Use Redis with Lua scripts for atomicity; accept that CAP theorem means trade-offs during partitions</p>
</li>
<li>
<p><strong>Graceful Degradation</strong>: Fail open with stricter local limits rather than complete denial during outages</p>
</li>
<li>
<p><strong>Testing</strong>: Mock time for unit tests, use chaos testing for distributed scenarios, property-based tests for invariants</p>
</li>
<li>
<p><strong>Observability</strong>: Track allowed/denied rates, latency percentiles, and set alerts for unusual patterns</p>
</li>
</ol>
<p><span style="background: linear-gradient(90deg, rgba(46,204,113,0.3) 0%, rgba(46,204,113,0.1) 100%); padding: 2px 8px; border-radius: 4px"><strong>Interview Tip</strong>: Always discuss the trade-offs. There's no perfect rate limiter - the right choice depends on consistency requirements, latency budget, and failure tolerance.</span></p>
</div>
