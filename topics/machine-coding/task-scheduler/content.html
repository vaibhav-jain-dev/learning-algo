<style>
/* Mobile-specific styles for iPhone 15 and similar devices */
@media screen and (max-width: 480px) {
    /* Force all grid layouts to single column */
    [style*="grid-template-columns"] {
        display: block !important;
    }
    [style*="grid-template-columns"] > div {
        margin-bottom: 16px !important;
    }
    /* Adjust padding for mobile */
    [style*="padding: 32px"],
    [style*="padding: 24px"] {
        padding: 16px !important;
    }
    /* Smaller headings */
    h4[style*="font-size: 18px"],
    h4[style*="font-size: 16px"] {
        font-size: 15px !important;
    }
    /* Readable font sizes */
    [style*="font-size: 13px"],
    [style*="font-size: 12px"],
    [style*="font-size: 11px"],
    [style*="font-size: 10px"] {
        font-size: 13px !important;
        line-height: 1.6 !important;
    }
    /* Flex containers stack vertically */
    [style*="display: flex"][style*="gap"] {
        flex-direction: column !important;
    }
    /* Better spacing for nested content */
    [style*="padding-left: 64px"],
    [style*="padding-left: 48px"],
    [style*="padding-left: 40px"] {
        padding-left: 16px !important;
    }
    /* Code blocks */
    pre {
        font-size: 12px !important;
        padding: 12px !important;
        overflow-x: auto !important;
    }
    pre code {
        font-size: 12px !important;
    }
    /* Tables */
    table {
        font-size: 12px !important;
        display: block !important;
        overflow-x: auto !important;
    }
    th, td {
        padding: 8px !important;
        font-size: 12px !important;
    }
}
</style>
<h1 id="task-scheduler">Task Scheduler</h1>
<h2 id="problem-statement">Problem Statement</h2>
<p>Design a task scheduler that executes tasks at specified times or intervals. Support one-time tasks, recurring tasks with cron expressions, distributed execution across multiple nodes, failure recovery, and idempotent operations.</p>
<hr />
<h2 id="core-concepts-deep-dive">Core Concepts Deep Dive</h2>
<h3 id="1-priority-queue-internals">1. Priority Queue Internals</h3>
<div style="background: #f8fafc; border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Why Priority Queues are Fundamental to Scheduling</strong></p>
<p>A task scheduler's primary operation is answering: &quot;What task should run next?&quot; This requires constant access to the minimum element (earliest scheduled time), making <a href="/data-structures/priority-queue">[priority-queues]</a> the natural choice.</p>
<p><strong>Internal Mechanism: Binary Heap</strong></p>
<p>The min-heap maintains the <strong>heap invariant</strong>: every parent node has a smaller (or equal) key than its children. For task scheduling, the key is a composite: <code>(scheduled_time, -priority, insertion_order)</code>.</p>
<p><strong>Why Three Components?</strong></p>
<ol>
<li><strong>scheduled_time</strong>: Primary ordering - earlier tasks execute first</li>
<li><strong>-priority</strong>: Tie-breaker - higher priority wins when times match (negated for min-heap)</li>
<li><strong>insertion_order</strong>: Stability - FIFO for identical time+priority (prevents starvation)</li>
</ol>
</div>
<div style="background: #eff6ff; border-radius: 16px; padding: 32px; margin: 24px 0">
<h4 style="color: #1e40af; margin: 0 0 24px 0; font-size: 16px">Heap Operations Visualized</h4>
<div style="display: flex; flex-wrap: wrap; gap: 16px">
<div style="background: #f0fdf4; padding: 16px; border-radius: 8px; flex: 1; min-width: 200px">
<div style="color: #166534; font-weight: bold; font-size: 13px; margin-bottom: 12px">Insert (Push)</div>
<div style="color: #1e293b; font-size: 11px; line-height: 1.8">
  1. Add element at end<br>
2. <span style="color: #ea580c">Bubble-up</span>: swap with parent while smaller<br>
  3. Stop when heap invariant restored
</div>
<div style="color: #64748b; font-size: 10px; margin-top: 8px">Time: O(log n)</div>
</div>
<div style="background: #fef2f2; padding: 16px; border-radius: 8px; flex: 1; min-width: 200px">
<div style="color: #991b1b; font-weight: bold; font-size: 13px; margin-bottom: 12px">Extract-Min (Pop)</div>
<div style="color: #1e293b; font-size: 11px; line-height: 1.8">
  1. Remove root (min element)<br>
  2. Move last element to root<br>
3. <span style="color: #ea580c">Bubble-down</span>: swap with smaller child
</div>
<div style="color: #64748b; font-size: 10px; margin-top: 8px">Time: O(log n)</div>
</div>
<div style="background: #f5f3ff; padding: 16px; border-radius: 8px; flex: 1; min-width: 200px">
<div style="color: #5b21b6; font-weight: bold; font-size: 13px; margin-bottom: 12px">Peek</div>
<div style="color: #1e293b; font-size: 11px; line-height: 1.8">
  1. Return root element<br>
  2. No modification needed<br>
  3. Heap unchanged
</div>
<div style="color: #64748b; font-size: 10px; margin-top: 8px">Time: O(1)</div>
</div>
</div>
<div style="background: #fff7ed; padding: 16px; border-radius: 8px; margin-top: 16px">
<div style="color: #c2410c; font-weight: bold; font-size: 12px; margin-bottom: 8px">Critical Insight: Decrease-Key Problem</div>
<div style="color: #1e293b; font-size: 11px">
  Standard binary heaps lack efficient decrease-key (needed for task rescheduling). Solutions:
<ul style="margin: 8px 0 0 16px; padding: 0">
<li><strong>Lazy deletion</strong>: Mark old entry invalid, insert new one. O(1) update but O(n) space waste</li>
<li><strong>Index tracking</strong>: Maintain position map. O(log n) update with O(n) extra space</li>
<li><strong>Fibonacci heap</strong>: O(1) amortized decrease-key but complex implementation</li>
</ul>
</div>
</div>
</div>
<div style="background: #f0fdf4; border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Assumption</strong>: Tasks arrive independently and uniformly over time.</p>
<p><strong>Trade-off Analysis: Data Structure Selection</strong></p>
<table>
<thead>
<tr>
<th>Structure</th>
<th>Peek</th>
<th>Insert</th>
<th>Delete</th>
<th>Decrease-Key</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td>Binary Heap</td>
<td>O(1)</td>
<td>O(log n)</td>
<td>O(n)*</td>
<td>O(n)</td>
<td>General purpose</td>
</tr>
<tr>
<td>Indexed Heap</td>
<td>O(1)</td>
<td>O(log n)</td>
<td>O(log n)</td>
<td>O(log n)</td>
<td>Frequent updates</td>
</tr>
<tr>
<td>Skip List</td>
<td>O(1)</td>
<td>O(log n)</td>
<td>O(log n)</td>
<td>O(log n)</td>
<td>Concurrent access</td>
</tr>
<tr>
<td>Timing Wheel</td>
<td>O(1)</td>
<td>O(1)</td>
<td>O(1)</td>
<td>O(1)</td>
<td>Fixed granularity</td>
</tr>
</tbody>
</table>
<p>*Delete requires finding the element first</p>
<p><strong>Design Choice</strong>: For most schedulers, a binary heap with lazy deletion provides the best simplicity-to-performance ratio. Use timing wheels when you have millions of tasks with second-level granularity (like network timeouts).</p>
</div>
<h4 id="interview-questions-priority-queue">Interview Questions: Priority Queue</h4>
<div style="background: #f5f3ff; border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Level 1</strong>: Why use a min-heap instead of a sorted array for task scheduling?</p>
<details>
<summary style="color: #5b21b6; cursor: pointer">Answer</summary>
<p>Sorted arrays have O(n) insertion (shifting elements) but O(1) removal of minimum. Min-heaps have O(log n) for both. Since schedulers frequently insert new tasks, the heap's balanced performance wins. With 10,000 tasks, heap insertion takes ~13 comparisons vs 5,000 average shifts for sorted arrays.</p>
</details>
<p><strong>Level 2</strong>: How would you handle efficient task cancellation in a heap-based scheduler?</p>
<details>
<summary style="color: #5b21b6; cursor: pointer">Answer</summary>
<p>Three approaches:</p>
<ol>
<li><strong>Lazy deletion</strong>: Mark task as cancelled, skip during extraction. Pros: O(1) cancel. Cons: Memory bloat, requires periodic cleanup.</li>
<li><strong>Index map</strong>: Maintain <code>task_id -&gt; heap_index</code> map. On cancel, swap with last element, bubble up/down. Pros: O(log n) cancel. Cons: Complex index maintenance.</li>
<li><strong>Soft state</strong>: Store tasks in both heap and hash map. Hash map is source of truth for status. Heap position doesn't matter for cancelled tasks.</li>
</ol>
<p>Production systems typically use lazy deletion with periodic compaction when cancelled tasks exceed a threshold (e.g., 20% of heap).</p>
</details>
<p><strong>Level 3</strong>: Design a priority queue that supports O(1) insertion for tasks scheduled within the next hour, while maintaining correctness for all tasks.</p>
<details>
<summary style="color: #5b21b6; cursor: pointer">Answer</summary>
<p>Use a <strong>Hierarchical Timing Wheel</strong> combined with a heap:</p>
<ol>
<li><strong>Near-future tasks (0-60 min)</strong>: Timing wheel with 60 slots (1-minute granularity). Insert is O(1) - hash timestamp to slot.</li>
<li><strong>Far-future tasks (&gt;60 min)</strong>: Standard min-heap for arbitrary future times.</li>
<li><strong>Overflow handling</strong>: Background thread moves tasks from heap to wheel as they enter the 60-minute window.</li>
</ol>
<p>This exploits the observation that most scheduled tasks execute within a short horizon. Netflix's scheduling system uses this pattern, achieving 99th percentile insert latency under 100 microseconds for millions of concurrent timers.</p>
<p>The wheel &quot;ticks&quot; every minute, promoting slot tasks to execution queue. Heap only consulted when wheel slots empty and we need to refill from far-future tasks.</p>
</details>
</div>
<hr />
<h3 id="2-cron-expression-parsing">2. Cron Expression Parsing</h3>
<div style="background: #f8fafc; border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>The Cron Expression Format</strong></p>
<p>Standard cron: <code>minute hour day-of-month month day-of-week</code></p>
<p>Extended (6-field): <code>second minute hour day-of-month month day-of-week</code></p>
<div style="display: flex; flex-wrap: wrap; gap: 8px; margin: 16px 0; background: #eff6ff; padding: 16px; border-radius: 8px">
<div style="text-align: center; min-width: 100px">
<div style="color: #1e40af; font-weight: bold; font-size: 12px">minute</div>
<div style="color: #64748b; font-size: 10px">(0-59)</div>
<div style="color: #3b82f6; font-size: 18px; font-family: monospace">*</div>
</div>
<div style="text-align: center; min-width: 100px">
<div style="color: #1e40af; font-weight: bold; font-size: 12px">hour</div>
<div style="color: #64748b; font-size: 10px">(0-23)</div>
<div style="color: #3b82f6; font-size: 18px; font-family: monospace">*</div>
</div>
<div style="text-align: center; min-width: 100px">
<div style="color: #1e40af; font-weight: bold; font-size: 12px">day of month</div>
<div style="color: #64748b; font-size: 10px">(1-31)</div>
<div style="color: #3b82f6; font-size: 18px; font-family: monospace">*</div>
</div>
<div style="text-align: center; min-width: 100px">
<div style="color: #1e40af; font-weight: bold; font-size: 12px">month</div>
<div style="color: #64748b; font-size: 10px">(1-12)</div>
<div style="color: #3b82f6; font-size: 18px; font-family: monospace">*</div>
</div>
<div style="text-align: center; min-width: 100px">
<div style="color: #1e40af; font-weight: bold; font-size: 12px">day of week</div>
<div style="color: #64748b; font-size: 10px">(0-6, Sun=0)</div>
<div style="color: #3b82f6; font-size: 18px; font-family: monospace">*</div>
</div>
</div>
<p><strong>Special Characters</strong>:<br />
- <code>*</code> - Any value<br />
- <code>,</code> - Value list separator (<code>1,3,5</code>)<br />
- <code>-</code> - Range (<code>1-5</code>)<br />
- <code>/</code> - Step values (<code>*/15</code> = every 15)<br />
- <code>L</code> - Last (last day of month/week)<br />
- <code>W</code> - Nearest weekday<br />
- <code>#</code> - Nth occurrence (<code>2#3</code> = third Monday)</p>
</div>
<div style="background: #eff6ff; border-radius: 16px; padding: 32px; margin: 24px 0">
<h4 style="color: #1e40af; margin: 0 0 24px 0; font-size: 16px">Cron Parser State Machine</h4>
<div style="display: flex; flex-direction: column; gap: 16px">
<div style="display: flex; justify-content: space-between; align-items: center; flex-wrap: wrap; gap: 12px">
<div style="background: #f0fdf4; padding: 12px 16px; border-radius: 8px; text-align: center; min-width: 100px">
<div style="color: #166534; font-weight: bold; font-size: 11px">TOKENIZE</div>
<div style="color: #1e293b; font-size: 9px">Split by spaces</div>
</div>
<div style="color: #22c55e; font-size: 20px">&#8594;</div>
<div style="background: #eff6ff; padding: 12px 16px; border-radius: 8px; text-align: center; min-width: 100px">
<div style="color: #1e40af; font-weight: bold; font-size: 11px">PARSE FIELD</div>
<div style="color: #1e293b; font-size: 9px">Handle *, ranges, steps</div>
</div>
<div style="color: #3b82f6; font-size: 20px">&#8594;</div>
<div style="background: #f5f3ff; padding: 12px 16px; border-radius: 8px; text-align: center; min-width: 100px">
<div style="color: #5b21b6; font-weight: bold; font-size: 11px">EXPAND</div>
<div style="color: #1e293b; font-size: 9px">Generate value sets</div>
</div>
<div style="color: #8b5cf6; font-size: 20px">&#8594;</div>
<div style="background: #fff7ed; padding: 12px 16px; border-radius: 8px; text-align: center; min-width: 100px">
<div style="color: #c2410c; font-weight: bold; font-size: 11px">VALIDATE</div>
<div style="color: #1e293b; font-size: 9px">Check bounds</div>
</div>
</div>
<div style="background: #f8fafc; padding: 16px; border-radius: 8px">
<div style="color: #c2410c; font-weight: bold; font-size: 12px; margin-bottom: 8px">Example: Parsing "*/15 9-17 * * MON-FRI"</div>
<div style="color: #1e293b; font-size: 11px; font-family: monospace; line-height: 1.8">
  minute: */15 &#8594; {0, 15, 30, 45}<br>
  hour: 9-17 &#8594; {9, 10, 11, 12, 13, 14, 15, 16, 17}<br>
  day: * &#8594; {1, 2, ..., 31}<br>
  month: * &#8594; {1, 2, ..., 12}<br>
  dow: MON-FRI &#8594; {1, 2, 3, 4, 5}
</div>
</div>
</div>
</div>
<div style="background: #f0fdf4; border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Assumption</strong>: Server timezone is consistent. Daylight saving time transitions are handled.</p>
<p><strong>The Next Execution Time Algorithm</strong></p>
<p>Given current time and cron expression, find the next valid execution time:</p>
<pre><code class="language-python">                  def next_execution(cron, from_time):
                  &quot;&quot;&quot;
                  Strategy: Increment smallest unit until all constraints satisfied.
                  Key insight: Search space is bounded - at most 4 years forward
                  (handles Feb 29 edge case).
                  &quot;&quot;&quot;
                  t = from_time + timedelta(minutes=1)  # Start from next minute
                  t = t.replace(second=0, microsecond=0)  # Align to minute boundary

                  for _ in range(4 * 366 * 24 * 60):  # Max iterations safety
                  if matches_all_fields(cron, t):
                  return t
                  t = increment_to_next_candidate(cron, t)

                  raise ValueError(&quot;No valid execution time in next 4 years&quot;)</code></pre>
<p><strong>Trade-off</strong>: Naive iteration vs smart jumping</p>
<pre><code>                  - **Naive**: Check every minute. Simple but slow for sparse schedules (`0 0 29 2 *` runs once every ~4 years)
                  - **Smart jumping**: Skip to next valid value per field. Complex but O(fields) per calculation
</code></pre>
<p><strong>Design Choice</strong>: Most production cron libraries use smart jumping with field-by-field advancement. When a field doesn't match, jump to its next valid value and reset all smaller fields.</p>
</div>
<div style="background: #fef2f2; border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Edge Cases That Break Naive Implementations</strong></p>
<ol>
<li>
<p><strong>February 30th</strong>: <code>0 0 30 2 *</code> - Never fires. Must detect impossible schedules.</p>
</li>
<li>
<p><strong>DST Spring Forward</strong>: 2:30 AM doesn't exist on spring-forward day. Options:<br />
- Skip the execution entirely<br />
- Execute at 3:00 AM instead<br />
- Execute at 1:59 AM (before gap)</p>
</li>
<li>
<p><strong>DST Fall Back</strong>: 1:30 AM occurs twice. Options:<br />
- Execute twice (dangerous for non-idempotent tasks)<br />
- Execute on first occurrence only<br />
- Execute on second occurrence only</p>
</li>
<li>
<p><strong>Day-of-month AND Day-of-week</strong>: Does <code>0 0 15 * MON</code> mean &quot;15th AND Monday&quot; or &quot;15th OR Monday&quot;?<br />
- Original cron: OR semantics<br />
- Quartz scheduler: AND semantics<br />
- Must document clearly!</p>
</li>
<li>
<p><strong>Leap seconds</strong>: 23:59:60 exists on some days. Most systems ignore this.</p>
</li>
</ol>
</div>
<h4 id="interview-questions-cron-expressions">Interview Questions: Cron Expressions</h4>
<div style="background: #f5f3ff;border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Level 1</strong>: Parse the cron expression <code>0 */2 * * *</code> and explain when it fires.</p>
<details>
<summary style="color: #5b21b6; cursor: pointer">Answer</summary>
<pre><code>- minute: 0 (exactly on the hour)
- hour: */2 = {0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22} (every 2 hours)
- day/month/dow: * (any)
</code></pre>
<p>Fires at: 00:00, 02:00, 04:00, 06:00, 08:00, 10:00, 12:00, 14:00, 16:00, 18:00, 20:00, 22:00 every day.</p>
</details>
<p><strong>Level 2</strong>: How would you handle timezone-aware cron scheduling when users are in different timezones?</p>
<details>
<summary style="color: #5b21b6; cursor: pointer">Answer</summary>
<p>Store and compute in UTC internally, convert for user display:</p>
<ol>
<li><strong>Storage</strong>: Store cron expression + user's timezone (e.g., &quot;America/New_York&quot;)</li>
<li><strong>Calculation</strong>:<br />
<code>python user_tz = pytz.timezone(task.timezone) now_in_user_tz = datetime.now(user_tz) next_fire_user_tz = calculate_next(cron, now_in_user_tz) next_fire_utc = next_fire_user_tz.astimezone(pytz.UTC) </code></li>
<li><strong>DST handling</strong>: Use timezone-aware libraries (pytz, java.time.ZonedDateTime) that handle DST transitions.</li>
</ol>
<p>Critical: Never store just UTC offset (-05:00). Store timezone name so DST rules apply correctly.</p>
</details>
<p><strong>Level 3</strong>: Design an efficient data structure to support querying &quot;which tasks fire in the next N minutes&quot; across 1 million cron-scheduled tasks.</p>
<details>
<summary style="color: #5b21b6; cursor: pointer">Answer</summary>
<p><strong>Problem</strong>: Computing next-fire for 1M tasks on every query is too slow.</p>
<p><strong>Solution</strong>: Pre-computed timeline with lazy refresh</p>
<ol>
<li>
<p><strong>Background worker</strong>: Continuously computes next-fire times and maintains a sorted index:<br />
<code>next_fire_index: SortedDict[timestamp] -&gt; Set[task_ids]</code></p>
</li>
<li>
<p><strong>Query &quot;tasks in next N minutes&quot;</strong>:<br />
<code>python now = time.time() return next_fire_index.range(now, now + N*60) </code><br />
Time: O(log M + K) where M = unique timestamps, K = matching tasks</p>
</li>
<li>
<p><strong>Index maintenance</strong>:<br />
- After task executes: Calculate new next-fire, insert into index<br />
- Periodic cleanup: Remove past timestamps<br />
- On cron update: Remove old entry, calculate new, insert</p>
</li>
<li>
<p><strong>Memory optimization</strong>: Only index next 24 hours. Tasks beyond that use on-demand calculation with caching.</p>
</li>
</ol>
<p>Airflow uses a similar approach with a &quot;next_dagrun&quot; precomputed column that's updated after each run.</p>
</details>
</div>
<hr />
<h3 id="3-distributed-scheduling">3. Distributed Scheduling</h3>
<div style="background: #f8fafc;border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>The Distributed Scheduling Challenge</strong></p>
<p>Single-node schedulers don't scale. Multiple scheduler nodes introduce:<br />
- <strong>Consistency</strong>: Same task might be picked by multiple nodes<br />
- <strong>Availability</strong>: Node failure shouldn't stop task execution<br />
- <strong>Partition tolerance</strong>: Network splits shouldn't cause duplicate execution</p>
<p>This is a classic <a href="/system-design/cap-theorem">[CAP theorem]</a> trade-off scenario.</p>
</div>
<div style="background: #eff6ff; border-radius: 16px; padding: 32px; margin: 24px 0">
<h4 style="color: #1e40af; margin: 0 0 24px 0; font-size: 16px">Distributed Scheduling Architectures</h4>
<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px">
<div style="background: #f8fafc; padding: 20px; border-radius: 12px">
<div style="color: #166534; font-weight: bold; font-size: 14px; margin-bottom: 16px">Leader-Based (Active-Passive)</div>
<div style="display: flex; flex-direction: column; gap: 12px">
<div style="background: #f0fdf4;padding: 12px; border-radius: 8px; text-align: center">
<div style="color: #fff; font-weight: bold; font-size: 11px">LEADER</div>
<div style="color: #166534; font-size: 9px">Schedules all tasks</div>
</div>
<div style="display: flex; justify-content: center; gap: 8px">
<div style="background: #30363d; padding: 10px; border-radius: 6px; text-align: center">
<div style="color: #64748b; font-size: 10px">Follower 1</div>
<div style="color: #6e7681; font-size: 8px">Standby</div>
</div>
<div style="background: #30363d; padding: 10px; border-radius: 6px; text-align: center">
<div style="color: #64748b; font-size: 10px">Follower 2</div>
<div style="color: #6e7681; font-size: 8px">Standby</div>
</div>
</div>
</div>
<div style="color: #1e293b; font-size: 11px; margin-top: 12px">
<strong>Pros</strong>: Simple, no coordination<br>
<strong>Cons</strong>: Leader bottleneck, failover delay
</div>
</div>
<div style="background: #f8fafc; padding: 20px; border-radius: 12px">
<div style="color: #1e40af; font-weight: bold; font-size: 14px; margin-bottom: 16px">Partition-Based (Active-Active)</div>
<div style="display: flex; justify-content: space-around; gap: 8px">
<div style="background: #eff6ff;padding: 12px; border-radius: 8px; text-align: center">
<div style="color: #fff; font-weight: bold; font-size: 11px">Node 1</div>
<div style="color: #1e40af; font-size: 9px">Tasks 0-999</div>
</div>
<div style="background: #f5f3ff;padding: 12px; border-radius: 8px; text-align: center">
<div style="color: #fff; font-weight: bold; font-size: 11px">Node 2</div>
<div style="color: #5b21b6; font-size: 9px">Tasks 1000-1999</div>
</div>
<div style="background: #fff7ed;padding: 12px; border-radius: 8px; text-align: center">
<div style="color: #fff; font-weight: bold; font-size: 11px">Node 3</div>
<div style="color: #fff; font-size: 9px">Tasks 2000-2999</div>
</div>
</div>
<div style="color: #1e293b; font-size: 11px; margin-top: 12px">
<strong>Pros</strong>: Scales horizontally, no SPOF<br>
<strong>Cons</strong>: Rebalancing complexity on node changes
</div>
</div>
</div>
<div style="background: #f0fdf4; padding: 16px; border-radius: 8px; margin-top: 20px">
<div style="color: #c2410c; font-weight: bold; font-size: 12px; margin-bottom: 12px">Coordination Mechanisms</div>
<div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 12px">
<div style="text-align: center">
<div style="color: #166534; font-size: 11px; font-weight: bold">[[ZooKeeper]](/system-design/zookeeper)</div>
<div style="color: #64748b; font-size: 9px">Leader election, distributed locks</div>
</div>
<div style="text-align: center">
<div style="color: #1e40af; font-size: 11px; font-weight: bold">[[Redis]](/databases/redis)</div>
<div style="color: #64748b; font-size: 9px">SETNX for locks, Redlock for safety</div>
</div>
<div style="text-align: center">
<div style="color: #5b21b6; font-size: 11px; font-weight: bold">[[etcd]](/system-design/etcd)</div>
<div style="color: #64748b; font-size: 9px">Lease-based locking, watch API</div>
</div>
</div>
</div>
</div>
<div style="background: #f0fdf4; border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Assumption</strong>: Network partitions are possible but rare. Clocks are synchronized within acceptable bounds (NTP).</p>
<p><strong>The Lock-Based Approach (Pessimistic)</strong></p>
<pre><code class="language-python">def try_execute_task(task_id):
lock_key = f&quot;task_lock:{task_id}&quot;

# Try to acquire distributed lock
if redis.set(lock_key, node_id, nx=True, ex=300):  # 5-min TTL
try:
execute_task(task_id)
mark_completed(task_id)
finally:
redis.delete(lock_key)
return True
return False  # Another node has the lock</code></pre>
<p><strong>Trade-off</strong>: Lock granularity<br />
- <strong>Task-level locks</strong>: High parallelism but many lock operations<br />
- <strong>Partition-level locks</strong>: Fewer locks but reduced parallelism<br />
- <strong>Global lock</strong>: Simplest but defeats the purpose of distribution</p>
<p><strong>Design Choice</strong>: Task-level locks with lock pooling. Maintain a connection pool to Redis/ZooKeeper to amortize connection overhead.</p>
</div>
<div style="background: #eff6ff; border-radius: 16px; padding: 32px; margin: 24px 0">
<h4 style="color: #5b21b6; margin: 0 0 24px 0; font-size: 16px">Database-Based Scheduling with Row Locking</h4>
<div style="background: #f8fafc; padding: 16px; border-radius: 8px; margin-bottom: 16px">
<div style="color: #166534; font-weight: bold; font-size: 12px; margin-bottom: 8px">The "Claim" Pattern</div>
<div style="color: #1e293b; font-size: 11px; font-family: monospace; line-height: 1.8">
<span style="color: #c2410c">-- Atomic claim: only one node succeeds</span><br>
  UPDATE tasks<br>
  SET status = 'running',<br>
  &nbsp;&nbsp;&nbsp;&nbsp;claimed_by = 'node-1',<br>
  &nbsp;&nbsp;&nbsp;&nbsp;claimed_at = NOW()<br>
  WHERE id = (SELECT id FROM tasks<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;WHERE status = 'pending'<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AND scheduled_time <= NOW()<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ORDER BY scheduled_time, priority DESC<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LIMIT 1<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #dc2626">FOR UPDATE SKIP LOCKED</span>)<br>
  RETURNING *;
</div>
</div>
<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 16px">
<div style="background: #238636; padding: 12px; border-radius: 8px">
<div style="color: #fff; font-weight: bold; font-size: 11px">SKIP LOCKED Advantage</div>
<div style="color: #166534; font-size: 10px; margin-top: 4px">Nodes don't block each other. If row is locked, skip to next candidate.</div>
</div>
<div style="background: #f85149; padding: 12px; border-radius: 8px">
<div style="color: #fff; font-weight: bold; font-size: 11px">Without SKIP LOCKED</div>
<div style="color: #ffd7d5; font-size: 10px; margin-top: 4px">Nodes serialize on same row. Throughput collapses under load.</div>
</div>
</div>
</div>
<div style="background: #fef2f2; border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Critical Distributed Scheduling Pitfalls</strong></p>
<ol>
<li>
<p><strong>Clock Skew</strong>: Node A's clock is 5 seconds ahead. It picks up tasks &quot;early&quot; before they're due on other nodes. Solution: Use server-side timestamps (DB/Redis), not client timestamps.</p>
</li>
<li>
<p><strong>Split Brain</strong>: Two nodes both believe they're the leader. Both execute the same task. Solution: Fencing tokens - each task execution includes a monotonic token; storage rejects older tokens.</p>
</li>
<li>
<p><strong>Zombie Workers</strong>: Node claims task, dies, task stays &quot;running&quot; forever. Solution: Heartbeat + timeout-based recovery (covered in failure recovery section).</p>
</li>
<li>
<p><strong>Thundering Herd</strong>: All N nodes poll database simultaneously on each tick. Solution: Jittered polling intervals, claim batches of tasks.</p>
</li>
<li>
<p><strong>Rebalancing Storms</strong>: Node joins/leaves, all partitions reassign, massive state transfer. Solution: Consistent hashing with virtual nodes for minimal reassignment.</p>
</li>
</ol>
</div>
<h4 id="interview-questions-distributed-scheduling">Interview Questions: Distributed Scheduling</h4>
<div style="background: #f5f3ff;border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Level 1</strong>: Why can't we simply use a shared database table with optimistic locking for distributed task scheduling?</p>
<details>
<summary style="color: #5b21b6; cursor: pointer">Answer</summary>
<p>Optimistic locking (version numbers/timestamps) causes high contention:</p>
<ol>
<li>Node A reads task with version=1</li>
<li>Node B reads same task with version=1</li>
<li>Node A updates with version=2, succeeds</li>
<li>Node B's update fails (version mismatch), must retry</li>
</ol>
<p>With N nodes and M due tasks, collision rate approaches 100% as N grows. Each collision wastes a round trip. Pessimistic locking (<code>SELECT FOR UPDATE SKIP LOCKED</code>) or distributed locks are preferred because they prevent the conflict upfront.</p>
</details>
<p><strong>Level 2</strong>: How would you implement leader election for a scheduler cluster using Redis?</p>
<details>
<summary style="color: #5b21b6; cursor: pointer">Answer</summary>
<pre><code class="language-python">class RedisLeaderElection:
def __init__(self, redis_client, key, node_id, ttl=30):
self.redis = redis_client
self.key = key
self.node_id = node_id
self.ttl = ttl
self.is_leader = False

def try_become_leader(self):
# SETNX with expiry - atomic
acquired = self.redis.set(
self.key, self.node_id,
nx=True,  # Only if not exists
ex=self.ttl
)
self.is_leader = acquired
return acquired

def maintain_leadership(self):
&quot;&quot;&quot;Call periodically (every ttl/3 seconds)&quot;&quot;&quot;
if self.is_leader:
# Refresh only if we still own the lock
# Lua script for atomicity
script = &quot;&quot;&quot;
if redis.call('get', KEYS[1]) == ARGV[1] then
return redis.call('expire', KEYS[1], ARGV[2])
else
return 0
end
&quot;&quot;&quot;
result = self.redis.eval(script, 1, self.key, self.node_id, self.ttl)
self.is_leader = (result == 1)
else:
self.try_become_leader()
return self.is_leader</code></pre>
<p>Critical: Use Lua script for refresh to ensure atomicity. Otherwise another node might acquire between GET and EXPIRE.</p>
</details>
<p><strong>Level 3</strong>: Design a scheduler that guarantees exactly-once execution even during network partitions, without relying on distributed transactions.</p>
<details>
<summary style="color: #5b21b6; cursor: pointer">Answer</summary>
<p><strong>Approach: Idempotent execution + outbox pattern + deduplication</strong></p>
<ol>
<li>
<p><strong>Task state machine with fencing</strong>:<br />
<code>sql ALTER TABLE tasks ADD COLUMN fence_token BIGINT; -- Monotonically increasing, assigned by coordinator </code></p>
</li>
<li>
<p><strong>Claim with fence token</strong>:<br />
<code>sql UPDATE tasks SET status = 'claimed', fence_token = (SELECT MAX(fence_token) + 1 FROM tasks), claimed_by = ? WHERE id = ? AND status = 'pending' RETURNING fence_token; </code></p>
</li>
<li>
<p><strong>Execution wrapper</strong>:<br />
```python<br />
def execute_with_fence(task, fence_token):<br />
# All downstream writes include fence_token<br />
# Storage systems reject writes with stale tokens<br />
result = task.func(fence_token=fence_token)</p>
<pre><code># Completion uses same fence
db.execute(&quot;&quot;&quot;
UPDATE tasks SET status = 'completed'
WHERE id = ? AND fence_token = ?
&quot;&quot;&quot;, task.id, fence_token)
```
</code></pre>
</li>
<li>
<p><strong>Downstream idempotency</strong>:<br />
```python<br />
def process_payment(order_id, fence_token):<br />
# Check if already processed with same or higher fence<br />
existing = db.query(<br />
&quot;SELECT fence_token FROM payments WHERE order_id = ?&quot;,<br />
order_id<br />
)<br />
if existing and existing.fence_token &gt;= fence_token:<br />
return existing.result  # Idempotent return</p>
<pre><code># Process and record fence
result = charge_card(order_id)
db.execute(
&quot;INSERT INTO payments (order_id, fence_token, result) VALUES (?, ?, ?)&quot;,
order_id, fence_token, result
)
return result
```
</code></pre>
</li>
</ol>
<p>This ensures that even if two nodes somehow both try to execute (partition scenario), only the one with the valid fence token succeeds at writes. The other's writes are rejected, and its completion update fails.</p>
</details>
</div>
<hr />
<h3 id="4-failure-recovery">4. Failure Recovery</h3>
<div style="background: #f8fafc;border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Failure Modes in Task Scheduling</strong></p>
<table>
<thead>
<tr>
<th>Failure Type</th>
<th>Symptom</th>
<th>Recovery Strategy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Worker crash</td>
<td>Task stuck in &quot;running&quot;</td>
<td>Timeout-based detection, reassignment</td>
</tr>
<tr>
<td>Network partition</td>
<td>Worker alive but unreachable</td>
<td>Heartbeat failure, may cause duplicate</td>
</tr>
<tr>
<td>Task timeout</td>
<td>Execution exceeds limit</td>
<td>Kill task, mark failed, optional retry</td>
</tr>
<tr>
<td>Dependency failure</td>
<td>Upstream task failed</td>
<td>Skip dependent tasks or wait</td>
</tr>
<tr>
<td>Resource exhaustion</td>
<td>OOM, disk full</td>
<td>Graceful degradation, alerting</td>
</tr>
</tbody>
</table>
</div>
<div style="background: #eff6ff; border-radius: 16px; padding: 32px; margin: 24px 0">
<h4 style="color: #1e40af; margin: 0 0 24px 0; font-size: 16px">Heartbeat-Based Failure Detection</h4>
<div style="display: flex; flex-direction: column; gap: 16px">
<div style="display: flex; justify-content: space-between; align-items: flex-start; gap: 20px; flex-wrap: wrap">
<div style="flex: 1; min-width: 200px; background: #f8fafc; padding: 16px; border-radius: 8px">
<div style="color: #166534; font-weight: bold; font-size: 12px; margin-bottom: 12px">Worker Heartbeat Loop</div>
<div style="color: #1e293b; font-size: 10px; font-family: monospace; line-height: 1.8">
  while running:<br>
  &nbsp;&nbsp;send_heartbeat(worker_id, current_task)<br>
  &nbsp;&nbsp;sleep(heartbeat_interval)
</div>
</div>
<div style="flex: 1; min-width: 200px; background: #f8fafc; padding: 16px; border-radius: 8px">
<div style="color: #dc2626; font-weight: bold; font-size: 12px; margin-bottom: 12px">Recovery Monitor Loop</div>
<div style="color: #1e293b; font-size: 10px; font-family: monospace; line-height: 1.8">
  while running:<br>
  &nbsp;&nbsp;stale = find_stale_heartbeats(threshold)<br>
  &nbsp;&nbsp;for worker in stale:<br>
  &nbsp;&nbsp;&nbsp;&nbsp;recover_tasks(worker)<br>
  &nbsp;&nbsp;sleep(check_interval)
</div>
</div>
</div>
<div style="background: #f0fdf4; padding: 16px; border-radius: 8px">
<div style="color: #c2410c; font-weight: bold; font-size: 12px; margin-bottom: 8px">Tuning Heartbeat Parameters</div>
<div style="color: #1e293b; font-size: 11px">
<strong>heartbeat_interval</strong>: 10-30 seconds typical. Too fast = network overhead. Too slow = delayed detection.<br>
<strong>failure_threshold</strong>: Usually 3x heartbeat_interval. Accounts for network jitter, GC pauses.<br>
<strong>check_interval</strong>: heartbeat_interval / 2. Ensures timely detection.
</div>
</div>
</div>
</div>
<div style="background: #f0fdf4; border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Assumption</strong>: Task execution is either idempotent or has built-in deduplication.</p>
<p><strong>Retry Strategies with <a href="/algorithms/exponential-backoff">[Exponential Backoff]</a></strong></p>
<pre><code class="language-python">def calculate_retry_delay(attempt, base_delay=1, max_delay=3600, jitter=True):
&quot;&quot;&quot;
Exponential backoff with optional jitter.
attempt: 0-indexed retry count
&quot;&quot;&quot;
delay = min(base_delay * (2 ** attempt), max_delay)

if jitter:
# Full jitter: random between 0 and calculated delay
delay = random.uniform(0, delay)

return delay

# Retry schedule for base_delay=1, max_delay=3600:
# Attempt 0: ~0-1s
# Attempt 1: ~0-2s
# Attempt 2: ~0-4s
# Attempt 3: ~0-8s
# ...
# Attempt 12+: ~0-3600s (capped)</code></pre>
<p><strong>Trade-off</strong>: Retry policy options</p>
<table>
<thead>
<tr>
<th>Policy</th>
<th>Behavior</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td>Fixed delay</td>
<td>Same wait between retries</td>
<td>Idempotent, predictable failures</td>
</tr>
<tr>
<td>Linear backoff</td>
<td>Delay increases by fixed amount</td>
<td>Rate-limited APIs</td>
</tr>
<tr>
<td>Exponential backoff</td>
<td>Delay doubles each time</td>
<td>Transient failures, overload</td>
</tr>
<tr>
<td>Exponential + jitter</td>
<td>Randomized exponential</td>
<td>Thundering herd prevention</td>
</tr>
</tbody>
</table>
<p><strong>Design Choice</strong>: Exponential backoff with full jitter is the default for modern systems. It spreads retry load over time, preventing synchronized retry storms after an outage recovers.</p>
</div>
<div style="background: #eff6ff; border-radius: 16px; padding: 32px; margin: 24px 0">
<h4 style="color: #5b21b6; margin: 0 0 24px 0; font-size: 16px">Dead Letter Queue Pattern</h4>
<div style="display: flex; gap: 16px; align-items: center; flex-wrap: wrap">
<div style="flex: 2; min-width: 250px">
<div style="display: flex; flex-direction: column; gap: 12px">
<div style="display: flex; align-items: center; gap: 12px">
<div style="background: #f0fdf4;padding: 10px 14px; border-radius: 8px">
<div style="color: #fff; font-size: 10px; font-weight: bold">Main Queue</div>
</div>
<div style="color: #166534">&#8594;</div>
<div style="background: #f8fafc; padding: 10px 14px; border-radius: 8px">
<div style="color: #1e293b; font-size: 10px">Process Task</div>
</div>
</div>
<div style="display: flex; align-items: center; gap: 12px; margin-left: 120px">
<div style="color: #dc2626">&#8595; failure</div>
</div>
<div style="display: flex; align-items: center; gap: 12px">
<div style="background: #fff7ed;padding: 10px 14px; border-radius: 8px">
<div style="color: #fff; font-size: 10px; font-weight: bold">Retry Queue</div>
</div>
<div style="color: #c2410c">&#8594;</div>
<div style="background: #f8fafc; padding: 10px 14px; border-radius: 8px">
<div style="color: #1e293b; font-size: 10px">Retry (with backoff)</div>
</div>
</div>
<div style="display: flex; align-items: center; gap: 12px; margin-left: 120px">
<div style="color: #dc2626">&#8595; max retries exceeded</div>
</div>
<div style="display: flex; align-items: center; gap: 12px">
<div style="background: linear-gradient(135deg, #da3633 0%, #f85149 100%); padding: 10px 14px; border-radius: 8px">
<div style="color: #fff; font-size: 10px; font-weight: bold">Dead Letter Queue</div>
</div>
<div style="color: #dc2626">&#8594;</div>
<div style="background: #f8fafc; padding: 10px 14px; border-radius: 8px">
<div style="color: #1e293b; font-size: 10px">Manual review / Alert</div>
</div>
</div>
</div>
</div>
<div style="flex: 1; min-width: 180px; background: #f8fafc; padding: 16px; border-radius: 8px">
<div style="color: #1e40af; font-weight: bold; font-size: 11px; margin-bottom: 8px">DLQ Contents</div>
<div style="color: #1e293b; font-size: 10px; line-height: 1.6">
                                                                      - Original task payload<br>
                                                                        - All retry attempt logs<br>
                                                                          - Final error message<br>
                                                                            - Stack trace<br>
                                                                              - Timestamp of each attempt<br>
                                                                                - Related task IDs
</div>
</div>
</div>
</div>
<div style="background: #fef2f2; border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>The &quot;Stuck Task&quot; Problem</strong></p>
<p>Scenario: Worker claims task, starts execution, hangs indefinitely (deadlock, infinite loop, waiting on external resource).</p>
<p><strong>Detection Approaches</strong>:</p>
<ol>
<li>
<p><strong>Heartbeat-based</strong>: Worker sends periodic &quot;I'm alive and working on task X&quot; messages. Absence indicates death.</p>
</li>
<li>
<p><strong>Visibility timeout</strong>: Task becomes invisible to other workers for T seconds. If not completed within T, becomes visible again for retry. (SQS model)</p>
</li>
<li>
<p><strong>Lease-based</strong>: Worker acquires time-limited lease. Must renew periodically. Expired lease = task available for reprocessing.</p>
</li>
</ol>
<p><strong>Recovery Approaches</strong>:</p>
<ol>
<li>
<p><strong>Timeout + Retry</strong>: After visibility timeout, another worker picks up. Original might still be running (duplicate risk).</p>
</li>
<li>
<p><strong>Process killing</strong>: Monitor sends SIGTERM to stuck process, waits, then SIGKILL. Requires process visibility.</p>
</li>
<li>
<p><strong>Checkpoint-resume</strong>: Task periodically saves progress. On recovery, resume from last checkpoint rather than restart.</p>
</li>
</ol>
</div>
<h4 id="interview-questions-failure-recovery">Interview Questions: Failure Recovery</h4>
<div style="background: #f5f3ff;border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Level 1</strong>: What's the difference between at-least-once and at-most-once task execution semantics?</p>
<details>
<summary style="color: #5b21b6; cursor: pointer">Answer</summary>
<p><strong>At-most-once</strong>: Task executed 0 or 1 times. Achieved by acknowledging before execution. If worker dies during execution, task is lost. Use for: logging, metrics, non-critical notifications.</p>
<p><strong>At-least-once</strong>: Task executed 1 or more times. Achieved by acknowledging after execution. If worker dies after execution but before ack, task is re-executed. Use for: payments (with idempotency), order processing.</p>
<p><strong>Exactly-once</strong>: Task executed exactly 1 time. Requires idempotent operations + at-least-once delivery + deduplication. Hardest to achieve.</p>
</details>
<p><strong>Level 2</strong>: Design a visibility timeout mechanism for a database-backed task queue.</p>
<details>
<summary style="color: #5b21b6; cursor: pointer">Answer</summary>
<pre><code class="language-sql">-- Schema
CREATE TABLE tasks (
id BIGSERIAL PRIMARY KEY,
payload JSONB,
status VARCHAR(20) DEFAULT 'pending',
visible_after TIMESTAMP DEFAULT NOW(),
attempts INT DEFAULT 0,
max_attempts INT DEFAULT 3,
created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_tasks_pending ON tasks(visible_after)
WHERE status = 'pending';

-- Claim task (makes it invisible)
UPDATE tasks
SET status = 'processing',
visible_after = NOW() + INTERVAL '5 minutes',
attempts = attempts + 1
WHERE id = (
SELECT id FROM tasks
WHERE status IN ('pending', 'processing')
AND visible_after &lt;= NOW()
AND attempts &lt; max_attempts
ORDER BY visible_after
LIMIT 1
FOR UPDATE SKIP LOCKED
)
RETURNING *;

-- Complete task
UPDATE tasks SET status = 'completed' WHERE id = ?;

-- Background recovery (runs every minute)
-- No action needed! Stale tasks automatically become
-- visible when visible_after passes</code></pre>
<p>The key insight: <code>visible_after</code> serves dual purpose - scheduling AND failure recovery. Processing tasks with old <code>visible_after</code> are implicitly &quot;timed out&quot; and become claimable.</p>
</details>
<p><strong>Level 3</strong>: How would you implement checkpoint-based recovery for long-running tasks that process millions of records?</p>
<details>
<summary style="color: #5b21b6; cursor: pointer">Answer</summary>
<pre><code class="language-python">class CheckpointedTask:
def __init__(self, task_id, checkpoint_store):
self.task_id = task_id
self.store = checkpoint_store  # Redis, DB, or file

def execute(self, records):
# Load checkpoint
checkpoint = self.store.get(self.task_id)
start_offset = checkpoint.get('offset', 0) if checkpoint else 0
accumulated_state = checkpoint.get('state', {}) if checkpoint else {}

batch_size = 1000
for i in range(start_offset, len(records), batch_size):
batch = records[i:i+batch_size]

# Process batch
for record in batch:
accumulated_state = self.process_record(record, accumulated_state)

# Checkpoint after each batch
self.store.set(self.task_id, {
'offset': i + batch_size,
'state': accumulated_state,
'last_checkpoint': time.time()
})

# Task completed - clean up checkpoint
self.store.delete(self.task_id)
return accumulated_state

def process_record(self, record, state):
# Business logic here
state['count'] = state.get('count', 0) + 1
state['sum'] = state.get('sum', 0) + record['value']
return state</code></pre>
<p><strong>Checkpoint design considerations</strong>:</p>
<ol>
<li>
<p><strong>Frequency</strong>: Every N records or every T seconds. Balance between recovery granularity and checkpoint overhead.</p>
</li>
<li>
<p><strong>Atomicity</strong>: If processing and checkpointing aren't atomic, records between last checkpoint and failure may be processed twice. Solution: Process batch, checkpoint, then commit batch to output.</p>
</li>
<li>
<p><strong>Checkpoint storage</strong>:<br />
- Redis: Fast but volatile. Use for short-lived tasks.<br />
- Database: Durable. Use for critical long-running tasks.<br />
- Object storage (S3): For very large state that doesn't fit in Redis/DB.</p>
</li>
<li>
<p><strong>Checkpoint compaction</strong>: For tasks that run forever (streams), periodically compact old checkpoints to prevent unbounded growth.</p>
</li>
</ol>
</details>
</div>
<hr />
<h3 id="5-job-dependencies-and-dag-execution">5. Job Dependencies and DAG Execution</h3>
<div style="background: #f8fafc;border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Why Job Dependencies Matter</strong></p>
<p>Real-world workflows rarely consist of independent tasks. <span style="color: #166534"><strong>Data pipelines</strong></span>, <span style="color: #166534"><strong>ETL processes</strong></span>, and <span style="color: #166534"><strong>CI/CD builds</strong></span> all require tasks to execute in specific orders, with later tasks depending on outputs from earlier ones.</p>
<p><strong>The Core Problem</strong>: How do you ensure Task B doesn't start until Task A completes successfully, while maximizing parallelism across independent branches?</p>
<p><strong>Solution</strong>: Model workflows as <span style="color: #166534"><strong>Directed Acyclic Graphs (DAGs)</strong></span> where nodes are tasks and edges represent dependencies.</p>
</div>
<div style="background: #eff6ff; border-radius: 16px; padding: 32px; margin: 24px 0">
<h4 style="color: #1e40af; margin: 0 0 24px 0; font-size: 16px">DAG-Based Task Execution Model</h4>
<div style="display: flex; flex-direction: column; gap: 20px">
<div style="display: flex; justify-content: center; align-items: center; gap: 8px; flex-wrap: wrap">
<div style="background: #f0fdf4;padding: 14px 20px; border-radius: 10px; text-align: center; min-width: 80px">
<div style="color: #fff; font-weight: bold; font-size: 12px">Extract</div>
<div style="color: #166534; font-size: 9px">Source Data</div>
</div>
<div style="color: #166534; font-size: 24px">&#8594;</div>
<div style="display: flex; flex-direction: column; gap: 8px">
<div style="background: #eff6ff;padding: 10px 16px; border-radius: 8px; text-align: center">
<div style="color: #fff; font-weight: bold; font-size: 11px">Transform A</div>
<div style="color: #1e40af; font-size: 8px">Clean Users</div>
</div>
<div style="background: #eff6ff;padding: 10px 16px; border-radius: 8px; text-align: center">
<div style="color: #fff; font-weight: bold; font-size: 11px">Transform B</div>
<div style="color: #1e40af; font-size: 8px">Clean Orders</div>
</div>
</div>
<div style="color: #1e40af; font-size: 24px">&#8594;</div>
<div style="background: #f5f3ff;padding: 14px 20px; border-radius: 10px; text-align: center; min-width: 80px">
<div style="color: #fff; font-weight: bold; font-size: 12px">Join</div>
<div style="color: #5b21b6; font-size: 9px">Merge Data</div>
</div>
<div style="color: #5b21b6; font-size: 24px">&#8594;</div>
<div style="background: #fff7ed;padding: 14px 20px; border-radius: 10px; text-align: center; min-width: 80px">
<div style="color: #fff; font-weight: bold; font-size: 12px">Load</div>
<div style="color: #fff; font-size: 9px">Data Warehouse</div>
</div>
</div>
<div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 12px">
<div style="background: #f8fafc; padding: 14px; border-radius: 8px">
<div style="color: #166534; font-weight: bold; font-size: 11px; margin-bottom: 8px">Parallelism</div>
<div style="color: #1e293b; font-size: 10px">Transform A and B run concurrently since both only depend on Extract</div>
</div>
<div style="background: #f8fafc; padding: 14px; border-radius: 8px">
<div style="color: #1e40af; font-weight: bold; font-size: 11px; margin-bottom: 8px">Barrier Sync</div>
<div style="color: #1e293b; font-size: 10px">Join waits for ALL upstream transforms before starting</div>
</div>
<div style="background: #f8fafc; padding: 14px; border-radius: 8px">
<div style="color: #c2410c; font-weight: bold; font-size: 11px; margin-bottom: 8px">Failure Propagation</div>
<div style="color: #1e293b; font-size: 10px">If Transform A fails, Join and Load are skipped (not executed)</div>
</div>
</div>
</div>
</div>
<div style="background: #f0fdf4; border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Assumption</strong>: Tasks within a DAG share execution context (variables, outputs).</p>
<p><strong>Dependency Resolution Algorithm</strong> using <a href="/algorithms/topological-sort">[Topological Sort]</a>:</p>
<pre><code class="language-python">class DAGScheduler:
def __init__(self):
self.tasks: Dict[str, Task] = {}
self.dependencies: Dict[str, Set[str]] = defaultdict(set)  # task -&gt; upstream tasks
self.dependents: Dict[str, Set[str]] = defaultdict(set)    # task -&gt; downstream tasks

def add_dependency(self, task_id: str, depends_on: str):
&quot;&quot;&quot;Task depends_on must complete before task_id can start.&quot;&quot;&quot;
self.dependencies[task_id].add(depends_on)
self.dependents[depends_on].add(task_id)

def get_ready_tasks(self, completed: Set[str]) -&gt; List[str]:
&quot;&quot;&quot;
Return tasks whose dependencies are ALL satisfied.
These can be executed in parallel.
&quot;&quot;&quot;
ready = []
for task_id, deps in self.dependencies.items():
if task_id not in completed and deps.issubset(completed):
ready.append(task_id)

# Also include tasks with no dependencies
for task_id in self.tasks:
if task_id not in completed and not self.dependencies[task_id]:
if task_id not in ready:
ready.append(task_id)

return ready

def detect_cycle(self) -&gt; bool:
&quot;&quot;&quot;Use DFS to detect cycles - cycles make DAG invalid.&quot;&quot;&quot;
WHITE, GRAY, BLACK = 0, 1, 2
color = {t: WHITE for t in self.tasks}

def dfs(node):
color[node] = GRAY
for dep in self.dependents[node]:  # downstream tasks
if color[dep] == GRAY:  # Back edge = cycle
return True
if color[dep] == WHITE and dfs(dep):
return True
color[node] = BLACK
return False

return any(dfs(t) for t in self.tasks if color[t] == WHITE)</code></pre>
<p><strong>Trade-off</strong>: Dependency specification approaches</p>
<table>
<thead>
<tr>
<th>Approach</th>
<th>Syntax</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr>
<td>Explicit edges</td>
<td><code>task_b.depends_on(task_a)</code></td>
<td>Clear, flexible</td>
<td>Verbose for many deps</td>
</tr>
<tr>
<td>Decorator-based</td>
<td><code>@depends_on(&quot;task_a&quot;)</code></td>
<td>Concise</td>
<td>Magic behavior</td>
</tr>
<tr>
<td>Return-value based</td>
<td><code>task_b(task_a())</code></td>
<td>Type-safe</td>
<td>Forces single upstream</td>
</tr>
<tr>
<td>Configuration file</td>
<td>YAML/JSON DAG definition</td>
<td>External tooling</td>
<td>Separate from code</td>
</tr>
</tbody>
</table>
<p><strong>Design Choice</strong>: Apache Airflow uses explicit <code>&gt;&gt;</code> operator (<code>task_a &gt;&gt; task_b</code>) for readability. Prefect uses return values for type safety. Choose based on team familiarity.</p>
</div>
<div style="background: #eff6ff; border-radius: 16px; padding: 32px; margin: 24px 0">
<h4 style="color: #5b21b6; margin: 0 0 24px 0; font-size: 16px">Dependency State Machine</h4>
<div style="display: flex; flex-direction: column; gap: 16px">
<div style="display: flex; justify-content: space-between; align-items: center; flex-wrap: wrap; gap: 12px">
<div style="background: #30363d; padding: 12px 16px; border-radius: 8px; text-align: center">
<div style="color: #64748b; font-weight: bold; font-size: 11px">WAITING</div>
<div style="color: #6e7681; font-size: 9px">Deps not met</div>
</div>
<div style="color: #166534; font-size: 18px">&#8594;<br><span style="font-size: 9px">all deps done</span></div>
<div style="background: #f0fdf4;padding: 12px 16px; border-radius: 8px; text-align: center">
<div style="color: #fff; font-weight: bold; font-size: 11px">READY</div>
<div style="color: #166534; font-size: 9px">Can execute</div>
</div>
<div style="color: #1e40af; font-size: 18px">&#8594;<br><span style="font-size: 9px">worker picks</span></div>
<div style="background: #eff6ff;padding: 12px 16px; border-radius: 8px; text-align: center">
<div style="color: #fff; font-weight: bold; font-size: 11px">RUNNING</div>
<div style="color: #1e40af; font-size: 9px">Executing</div>
</div>
<div style="display: flex; flex-direction: column; gap: 4px; align-items: center">
<div style="color: #166534; font-size: 14px">&#8594; <span style="font-size: 9px; color: #166534">success</span></div>
<div style="color: #dc2626; font-size: 14px">&#8594; <span style="font-size: 9px; color: #dc2626">failure</span></div>
</div>
<div style="display: flex; flex-direction: column; gap: 8px">
<div style="background: #f0fdf4;padding: 8px 12px; border-radius: 6px; text-align: center">
<div style="color: #fff; font-weight: bold; font-size: 10px">SUCCESS</div>
</div>
<div style="background: linear-gradient(135deg, #da3633 0%, #f85149 100%); padding: 8px 12px; border-radius: 6px; text-align: center">
<div style="color: #fff; font-weight: bold; font-size: 10px">FAILED</div>
</div>
</div>
</div>
<div style="background: #f8fafc; padding: 16px; border-radius: 8px">
<div style="color: #c2410c; font-weight: bold; font-size: 12px; margin-bottom: 10px">Downstream Failure Handling Strategies</div>
<div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 12px">
<div>
<div style="color: #166534; font-size: 11px; font-weight: bold">all_success (default)</div>
<div style="color: #64748b; font-size: 10px">Run only if ALL upstream succeeded</div>
</div>
<div>
<div style="color: #1e40af; font-size: 11px; font-weight: bold">all_done</div>
<div style="color: #64748b; font-size: 10px">Run when all upstream completed (success or fail)</div>
</div>
<div>
<div style="color: #5b21b6; font-size: 11px; font-weight: bold">one_success</div>
<div style="color: #64748b; font-size: 10px">Run if ANY upstream succeeded</div>
</div>
<div>
<div style="color: #c2410c; font-size: 11px; font-weight: bold">none_failed</div>
<div style="color: #64748b; font-size: 10px">Run if no upstream failed (skipped is OK)</div>
</div>
</div>
</div>
</div>
</div>
<div style="background: #fef2f2; border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Critical Edge Cases in Dependency Management</strong></p>
<ol>
<li>
<p><span style="color: #166534"><strong>Diamond Dependencies</strong></span>: A  B, A  C, B  D, C  D. Task D has two paths from A. Ensure D doesn't run twice and receives outputs from both B and C.</p>
</li>
<li>
<p><span style="color: #166534"><strong>Dynamic Dependencies</strong></span>: Task creates new tasks at runtime. How do you handle dependencies on not-yet-existing tasks? Solutions:<br />
- <strong>Deferred resolution</strong>: Dependency references are strings, resolved at execution time<br />
- <strong>Task factories</strong>: Parent task spawns child DAG as atomic unit</p>
</li>
<li>
<p><span style="color: #166534"><strong>Cross-DAG Dependencies</strong></span>: Task in DAG-1 depends on task in DAG-2. Options:<br />
- <strong>Sensors</strong>: DAG-1 task polls for DAG-2 completion (Airflow approach)<br />
- <strong>Events</strong>: DAG-2 publishes completion event, DAG-1 subscribes<br />
- <strong>Shared state</strong>: Both check common database/cache for status</p>
</li>
<li>
<p><span style="color: #166534"><strong>Conditional Dependencies</strong></span>: Task B depends on A only if A produced specific output. Requires runtime dependency evaluation, not static DAG structure.</p>
</li>
</ol>
</div>
<h4 id="interview-questions-job-dependencies">Interview Questions: Job Dependencies</h4>
<div style="background: #f5f3ff;border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Level 1</strong>: What happens if you have a cycle in your task dependency graph?</p>
<details>
<summary style="color: #5b21b6; cursor: pointer">Answer</summary>
<p>A cycle creates an impossible situation: Task A waits for B, B waits for C, C waits for A. No task can ever start. This is called a <span style="color: #166534"><strong>deadlock</strong></span>.</p>
<p><strong>Detection</strong>: Run <a href="/algorithms/graph-cycle-detection">[cycle detection]</a> using DFS. If you find a back edge (edge to a node currently in the recursion stack), there's a cycle.</p>
<p><strong>Prevention</strong>: Validate DAG structure at definition time, before any task executes. Reject workflows with cycles.</p>
<pre><code class="language-python">def validate_dag(dependencies):
# Kahn's algorithm - if we can't process all nodes, there's a cycle
in_degree = defaultdict(int)
for task, deps in dependencies.items():
for dep in deps:
in_degree[task] += 1

queue = [t for t in dependencies if in_degree[t] == 0]
processed = 0

while queue:
node = queue.pop(0)
processed += 1
for dependent in get_dependents(node):
in_degree[dependent] -= 1
if in_degree[dependent] == 0:
queue.append(dependent)

return processed == len(dependencies)  # False if cycle exists</code></pre>
</details>
<p><strong>Level 2</strong>: How would you implement partial DAG re-execution after a mid-pipeline failure?</p>
<details>
<summary style="color: #5b21b6; cursor: pointer">Answer</summary>
<p>Scenario: 5-task pipeline runs. Task 3 fails. After fixing, user wants to resume from task 3 without re-running tasks 1-2.</p>
<p><strong>Implementation</strong>:</p>
<ol>
<li>
<p><span style="color: #166534"><strong>Persist task outputs</strong></span>: Store each task's output with execution ID<br />
<code>python outputs[f&quot;{dag_run_id}:{task_id}&quot;] = task_result </code></p>
</li>
<li>
<p><span style="color: #166534"><strong>Mark task states</strong></span>: Track which tasks completed successfully in this run<br />
<code>python task_states = { &quot;task_1&quot;: &quot;success&quot;, &quot;task_2&quot;: &quot;success&quot;, &quot;task_3&quot;: &quot;failed&quot;, &quot;task_4&quot;: &quot;skipped&quot;,  # downstream of failure &quot;task_5&quot;: &quot;skipped&quot; } </code></p>
</li>
<li>
<p><span style="color: #166534"><strong>Selective re-execution</strong></span>:<br />
```python<br />
def rerun_from_failure(dag_run_id, failed_task_id):<br />
# Get all downstream tasks (including failed one)<br />
to_rerun = get_downstream_tasks(failed_task_id) | {failed_task_id}</p>
<pre><code># Clear their states
for task_id in to_rerun:
task_states[task_id] = &quot;pending&quot;

# Upstream outputs already exist - they'll be loaded, not recomputed
execute_dag(dag_run_id, skip_completed=True)
```
</code></pre>
</li>
<li>
<p><span style="color: #166534"><strong>Dependency resolution</strong></span>: When task 3 runs, it loads outputs from tasks 1-2 from storage instead of waiting for them to execute.</p>
</li>
</ol>
<p>Airflow calls this &quot;clearing&quot; a task - it resets the task and all downstream tasks to pending state.</p>
</details>
<p><strong>Level 3</strong>: Design a scheduler that supports dynamic task fan-out where one task spawns N child tasks at runtime, and a downstream task must wait for all N to complete.</p>
<details>
<summary style="color: #5b21b6; cursor: pointer">Answer</summary>
<p><strong>Challenge</strong>: Traditional DAGs have static structure. Here, the number of child tasks is unknown until runtime.</p>
<p><strong>Solution: Dynamic Task Groups with Barrier Synchronization</strong></p>
<pre><code class="language-python">class DynamicDAGScheduler:
def __init__(self):
self.task_groups: Dict[str, TaskGroup] = {}
self.barriers: Dict[str, Barrier] = {}

def execute_fan_out(self, parent_task_id: str, items: List[Any],
child_task_fn: Callable, downstream_task_id: str):
&quot;&quot;&quot;
Parent task calls this to spawn N child tasks.
downstream_task waits for all children to complete.
&quot;&quot;&quot;
group_id = f&quot;{parent_task_id}_children&quot;

# Create barrier for N children + 1 (for bookkeeping)
barrier = Barrier(len(items))
self.barriers[group_id] = barrier

# Spawn child tasks
child_task_ids = []
for i, item in enumerate(items):
child_id = f&quot;{group_id}_{i}&quot;
child_task = Task(
id=child_id,
func=lambda: self._run_child(child_task_fn, item, group_id),
group_id=group_id
)
self.schedule(child_task)
child_task_ids.append(child_id)

# Register downstream dependency on the GROUP, not individual tasks
self.add_group_dependency(downstream_task_id, group_id)

return child_task_ids

def _run_child(self, fn, item, group_id):
&quot;&quot;&quot;Execute child and update barrier.&quot;&quot;&quot;
try:
result = fn(item)
self.barriers[group_id].mark_success()
return result
except Exception as e:
self.barriers[group_id].mark_failure()
raise

def is_group_ready(self, group_id: str) -&gt; Tuple[bool, str]:
&quot;&quot;&quot;Check if all tasks in group completed.&quot;&quot;&quot;
barrier = self.barriers.get(group_id)
if not barrier:
return False, &quot;pending&quot;

if barrier.all_success():
return True, &quot;success&quot;
elif barrier.any_failure():
return True, &quot;failed&quot;  # Group failed, downstream should handle
else:
return False, &quot;running&quot;

class Barrier:
def __init__(self, expected_count: int):
self.expected = expected_count
self.success_count = 0
self.failure_count = 0
self.lock = threading.Lock()

def mark_success(self):
with self.lock:
self.success_count += 1

def mark_failure(self):
with self.lock:
self.failure_count += 1

def all_success(self) -&gt; bool:
with self.lock:
return self.success_count == self.expected

def any_failure(self) -&gt; bool:
with self.lock:
return self.failure_count &gt; 0

def all_done(self) -&gt; bool:
with self.lock:
return (self.success_count + self.failure_count) == self.expected</code></pre>
<p><strong>Key Design Decisions</strong>:</p>
<ol>
<li>
<p><span style="color: #166534"><strong>Group abstraction</strong></span>: Downstream depends on group, not individual tasks. Group is single entity in dependency graph.</p>
</li>
<li>
<p><span style="color: #166534"><strong>Barrier synchronization</strong></span>: Atomic counter tracks child completion. Downstream task wakes when barrier complete.</p>
</li>
<li>
<p><span style="color: #166534"><strong>Failure semantics</strong></span>: Define policy - fail-fast (first child failure cancels siblings) or wait-all (collect all results, then fail).</p>
</li>
<li>
<p><span style="color: #166534"><strong>Result aggregation</strong></span>: Store child results in shared location. Downstream task retrieves all results:<br />
<code>python def collect_results(group_id) -&gt; List[Any]: return [outputs[f&quot;{group_id}_{i}&quot;] for i in range(barrier.expected)] </code></p>
</li>
</ol>
<p>Real-world example: Airflow's <code>expand()</code> (formerly <code>mapped tasks</code>) implements exactly this pattern.</p>
</details>
</div>
<hr />
<h3 id="6-idempotency">6. Idempotency</h3>
<div style="background: #f8fafc;border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Why Idempotency is Non-Negotiable for Schedulers</strong></p>
<p>In distributed systems, exactly-once delivery is a myth. Networks fail, workers crash, retries happen. Your system WILL execute the same task multiple times. The question is: will that break things?</p>
<p><strong>Definition</strong>: An operation is idempotent if executing it multiple times has the same effect as executing it once.</p>
<p><strong>Idempotent operations</strong>:<br />
- <code>SET user.email = 'new@email.com'</code> (absolute assignment)<br />
- <code>DELETE FROM users WHERE id = 5</code><br />
- <code>PUT /api/users/5</code> (full replacement)</p>
<p><strong>Non-idempotent operations</strong>:<br />
- <code>INCREMENT user.balance BY 100</code> (additive)<br />
- <code>INSERT INTO orders (...)</code> (creates new row each time)<br />
- <code>POST /api/orders</code> (creates new resource)</p>
</div>
<div style="background: #eff6ff; border-radius: 16px; padding: 32px; margin: 24px 0">
<h4 style="color: #1e40af; margin: 0 0 24px 0; font-size: 16px">Idempotency Key Pattern</h4>
<div style="display: flex; flex-direction: column; gap: 16px">
<div style="background: #f8fafc; padding: 16px; border-radius: 8px">
<div style="color: #166534; font-weight: bold; font-size: 12px; margin-bottom: 12px">The Pattern</div>
<div style="color: #1e293b; font-size: 11px; font-family: monospace; line-height: 1.8">
<span style="color: #c2410c">def</span> process_with_idempotency(idempotency_key, operation):<br>
&nbsp;&nbsp;<span style="color: #64748b"># Check if already processed</span><br>
  &nbsp;&nbsp;existing = idempotency_store.get(idempotency_key)<br>
&nbsp;&nbsp;<span style="color: #c2410c">if</span> existing:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #c2410c">return</span> existing.result &nbsp;<span style="color: #64748b"># Return cached result</span><br>
  <br>
&nbsp;&nbsp;<span style="color: #64748b"># Execute operation</span><br>
  &nbsp;&nbsp;result = operation()<br>
  <br>
&nbsp;&nbsp;<span style="color: #64748b"># Store result (with TTL for cleanup)</span><br>
&nbsp;&nbsp;idempotency_store.set(idempotency_key, result, ttl=<span style="color: #1e40af">86400</span>)<br>
  <br>
&nbsp;&nbsp;<span style="color: #c2410c">return</span> result
</div>
</div>
<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 16px">
<div style="background: #238636; padding: 12px; border-radius: 8px">
<div style="color: #fff; font-weight: bold; font-size: 11px; margin-bottom: 4px">Good Idempotency Keys</div>
<div style="color: #166534; font-size: 10px">
                                                                                                              - task_id + scheduled_time<br>
                                                                                                                - order_id + action + timestamp<br>
                                                                                                                  - user_id + operation + date
</div>
</div>
<div style="background: #f85149; padding: 12px; border-radius: 8px">
<div style="color: #fff; font-weight: bold; font-size: 11px; margin-bottom: 4px">Bad Idempotency Keys</div>
<div style="color: #ffd7d5; font-size: 10px">
                                                                                                                  - Random UUID (different each retry)<br>
                                                                                                                    - Timestamp only (too granular)<br>
                                                                                                                      - Mutable fields (user can change)
</div>
</div>
</div>
</div>
</div>
<div style="background: #f0fdf4; border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Assumption</strong>: Idempotency keys are unique per logical operation instance.</p>
<p><strong>Trade-off</strong>: Idempotency storage strategies</p>
<table>
<thead>
<tr>
<th>Strategy</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr>
<td>In-memory cache</td>
<td>Fast, simple</td>
<td>Lost on restart, limited size</td>
</tr>
<tr>
<td>Redis</td>
<td>Fast, distributed</td>
<td>Extra infrastructure, TTL management</td>
</tr>
<tr>
<td>Database table</td>
<td>Durable, queryable</td>
<td>Slower writes, storage growth</td>
</tr>
<tr>
<td>Request log + hash</td>
<td>Full audit trail</td>
<td>Highest overhead</td>
</tr>
</tbody>
</table>
<p><strong>Design Choice</strong>: Use Redis with 24-hour TTL for most scheduled tasks. Tasks that run daily have natural idempotency keys (task_id + date). For critical financial operations, use database with indefinite retention.</p>
</div>
<div style="background: #eff6ff; border-radius: 16px; padding: 32px; margin: 24px 0">
<h4 style="color: #5b21b6; margin: 0 0 24px 0; font-size: 16px">Making Non-Idempotent Operations Idempotent</h4>
<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px">
<div style="background: #f8fafc; padding: 16px; border-radius: 8px">
<div style="color: #dc2626; font-weight: bold; font-size: 12px; margin-bottom: 12px">Problem: Additive Operations</div>
<div style="color: #1e293b; font-size: 10px; font-family: monospace; line-height: 1.6">
<span style="color: #64748b">-- Non-idempotent: runs twice = double credit</span><br>
  UPDATE accounts<br>
  SET balance = balance + 100<br>
  WHERE user_id = 5;
</div>
</div>
<div style="background: #f8fafc; padding: 16px; border-radius: 8px">
<div style="color: #166534; font-weight: bold; font-size: 12px; margin-bottom: 12px">Solution: Transaction Log</div>
<div style="color: #1e293b; font-size: 10px; font-family: monospace; line-height: 1.6">
<span style="color: #64748b">-- Idempotent: unique constraint prevents duplicate</span><br>
  INSERT INTO transactions<br>
  &nbsp;&nbsp;(id, user_id, amount, idempotency_key)<br>
  VALUES (gen_id(), 5, 100, 'task-123-2024-01-15')<br>
  ON CONFLICT (idempotency_key) DO NOTHING;<br><br>
<span style="color: #64748b">-- Balance computed from transactions</span><br>
  SELECT SUM(amount) FROM transactions<br>
  WHERE user_id = 5;
</div>
</div>
<div style="background: #f8fafc; padding: 16px; border-radius: 8px">
<div style="color: #dc2626; font-weight: bold; font-size: 12px; margin-bottom: 12px">Problem: External API Calls</div>
<div style="color: #1e293b; font-size: 10px; font-family: monospace; line-height: 1.6">
<span style="color: #64748b"># Non-idempotent: duplicate emails</span><br>
  email_api.send(<br>
  &nbsp;&nbsp;to='user@example.com',<br>
  &nbsp;&nbsp;subject='Your order shipped'<br>
  )
</div>
</div>
<div style="background: #f8fafc; padding: 16px; border-radius: 8px">
<div style="color: #166534; font-weight: bold; font-size: 12px; margin-bottom: 12px">Solution: Check-then-send with Lock</div>
<div style="color: #1e293b; font-size: 10px; font-family: monospace; line-height: 1.6">
<span style="color: #64748b"># Idempotent: check before sending</span><br>
  key = f'email_sent:{order_id}:shipped'<br>
  if not redis.setnx(key, '1', ex=86400):<br>
&nbsp;&nbsp;return <span style="color: #64748b"># Already sent</span><br><br>
  email_api.send(...)<br>
<span style="color: #64748b"># If send fails, key expires, retry works</span>
</div>
</div>
</div>
</div>
<div style="background: #fef2f2; border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>The Idempotency-Atomicity Gap</strong></p>
<p>Dangerous pattern:<br />
```python<br />
def process_order(order_id, idempotency_key):<br />
if is_processed(idempotency_key):<br />
return get_cached_result(idempotency_key)</p>
<pre><code>result = charge_payment(order_id)      # Step 1
update_inventory(order_id)              # Step 2
# CRASH HERE
mark_processed(idempotency_key, result) # Step 3 - never executes
```
</code></pre>
<p>On retry: <code>is_processed</code> returns False, payment charged again!</p>
<p><strong>Solution: Transactional outbox</strong></p>
<pre><code class="language-python">def process_order(order_id, idempotency_key):
with db.transaction():
if is_processed(idempotency_key):
return get_cached_result(idempotency_key)

# All state changes in same transaction
record_payment_intent(order_id)
update_inventory(order_id)
mark_processed(idempotency_key)

# Async: separate process reads outbox, calls payment API
# Payment API must also be idempotent (Stripe idempotency keys)</code></pre>
</div>
<h4 id="interview-questions-idempotency">Interview Questions: Idempotency</h4>
<div style="background: #f5f3ff;border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Level 1</strong>: Why is <code>INSERT INTO table VALUES (...)</code> not idempotent, and how would you make it idempotent?</p>
<details>
<summary style="color: #5b21b6; cursor: pointer">Answer</summary>
<p>Each INSERT creates a new row with a new auto-generated ID. Running twice = two rows.</p>
<p><strong>Solutions</strong>:</p>
<ol>
<li>
<p><strong>Natural key constraint</strong>:<br />
<code>sql CREATE UNIQUE INDEX ON orders(customer_id, order_date, product_id); INSERT INTO orders (...) ON CONFLICT DO NOTHING; </code></p>
</li>
<li>
<p><strong>Idempotency key column</strong>:<br />
<code>sql ALTER TABLE orders ADD COLUMN idempotency_key VARCHAR UNIQUE; INSERT INTO orders (..., idempotency_key) VALUES (..., 'task-123') ON CONFLICT (idempotency_key) DO NOTHING; </code></p>
</li>
<li>
<p><strong>UPSERT pattern</strong>:<br />
<code>sql INSERT INTO orders (...) VALUES (...) ON CONFLICT (order_id) DO UPDATE SET updated_at = NOW(); </code></p>
</li>
</ol>
</details>
<p><strong>Level 2</strong>: How do you handle idempotency when your task calls multiple external services sequentially?</p>
<details>
<summary style="color: #5b21b6; cursor: pointer">Answer</summary>
<p><strong>Saga pattern with compensation</strong>:</p>
<pre><code class="language-python">class OrderSaga:
def execute(self, order_id, idempotency_key):
# Track progress in saga state
saga_state = self.load_or_create_state(idempotency_key)

try:
# Step 1: Reserve inventory
if not saga_state.inventory_reserved:
inventory_service.reserve(
order_id,
idempotency_key=f&quot;{idempotency_key}:inventory&quot;
)
saga_state.inventory_reserved = True
self.save_state(saga_state)

# Step 2: Charge payment
if not saga_state.payment_charged:
payment_service.charge(
order_id,
idempotency_key=f&quot;{idempotency_key}:payment&quot;
)
saga_state.payment_charged = True
self.save_state(saga_state)

# Step 3: Ship order
if not saga_state.shipment_created:
shipping_service.create_shipment(
order_id,
idempotency_key=f&quot;{idempotency_key}:shipping&quot;
)
saga_state.shipment_created = True
saga_state.completed = True
self.save_state(saga_state)

except Exception as e:
self.compensate(saga_state, order_id, idempotency_key)
raise

def compensate(self, saga_state, order_id, idempotency_key):
# Undo in reverse order
if saga_state.payment_charged:
payment_service.refund(order_id, idempotency_key=f&quot;{idempotency_key}:refund&quot;)
if saga_state.inventory_reserved:
inventory_service.release(order_id, idempotency_key=f&quot;{idempotency_key}:release&quot;)</code></pre>
<p>Key insight: Each external call has its own idempotency key derived from the parent. Saga state tracks which steps completed, enabling retry from any failure point.</p>
</details>
<p><strong>Level 3</strong>: Design an idempotency system that handles concurrent duplicate requests arriving within milliseconds of each other.</p>
<details>
<summary style="color: #5b21b6; cursor: pointer">Answer</summary>
<p><strong>Problem</strong>: Two requests with same idempotency key arrive simultaneously. Both check &quot;is processed?&quot; -&gt; both get &quot;no&quot; -&gt; both execute.</p>
<p><strong>Solution: Distributed lock with request coalescing</strong></p>
<pre><code class="language-python">class IdempotencyService:
def __init__(self, redis_client, db):
self.redis = redis_client
self.db = db

def execute_idempotent(self, idempotency_key, operation, timeout=30):
# Phase 1: Check completed results
completed = self.db.query(
&quot;SELECT result FROM idempotency_results WHERE key = %s&quot;,
idempotency_key
)
if completed:
return completed.result

# Phase 2: Try to acquire execution lock
lock_key = f&quot;idempotency_lock:{idempotency_key}&quot;
lock_acquired = self.redis.set(
lock_key,
&quot;1&quot;,
nx=True,  # Only if not exists
ex=timeout
)

if lock_acquired:
try:
# We own the lock - execute
result = operation()

# Store result atomically
self.db.execute(
&quot;&quot;&quot;INSERT INTO idempotency_results (key, result, created_at)
VALUES (%s, %s, NOW())
ON CONFLICT (key) DO NOTHING&quot;&quot;&quot;,
idempotency_key, json.dumps(result)
)
return result
finally:
self.redis.delete(lock_key)
else:
# Another request is executing - wait for result
return self._wait_for_result(idempotency_key, timeout)

def _wait_for_result(self, idempotency_key, timeout):
&quot;&quot;&quot;Poll for result with exponential backoff&quot;&quot;&quot;
deadline = time.time() + timeout
delay = 0.01  # Start with 10ms

while time.time() &lt; deadline:
result = self.db.query(
&quot;SELECT result FROM idempotency_results WHERE key = %s&quot;,
idempotency_key
)
if result:
return json.loads(result.result)

time.sleep(delay)
delay = min(delay * 2, 1.0)  # Cap at 1 second

raise TimeoutError(f&quot;Idempotency result not available for {idempotency_key}&quot;)</code></pre>
<p><strong>Race condition eliminated</strong>:</p>
<ol>
<li>First request acquires lock, executes, stores result</li>
<li>Concurrent requests see lock exists, wait for result</li>
<li>All requests return same result</li>
</ol>
<p><strong>Bonus optimization</strong>: Use Redis pub/sub to notify waiters instead of polling.</p>
</details>
</div>
<hr />
<h2 id="complete-implementation">Complete Implementation</h2>
<h3 id="python-production-ready">Python (Production-Ready)</h3>
<pre><code class="language-python">import heapq
import threading
import time
import hashlib
import json
import re
from dataclasses import dataclass, field
from typing import Callable, Optional, List, Dict, Set, Any
from enum import Enum
from datetime import datetime, timedelta
from abc import ABC, abstractmethod
import uuid
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TaskStatus(Enum):
PENDING = &quot;pending&quot;
RUNNING = &quot;running&quot;
COMPLETED = &quot;completed&quot;
FAILED = &quot;failed&quot;
CANCELLED = &quot;cancelled&quot;
RETRYING = &quot;retrying&quot;


class RecurrenceType(Enum):
ONCE = &quot;once&quot;
INTERVAL = &quot;interval&quot;
CRON = &quot;cron&quot;


# ============================================================
# Cron Expression Parser
# ============================================================

class CronField:
&quot;&quot;&quot;Represents a single field in a cron expression.&quot;&quot;&quot;

def __init__(self, expr: str, min_val: int, max_val: int, aliases: Dict[str, int] = None):
self.min_val = min_val
self.max_val = max_val
self.aliases = aliases or {}
self.values: Set[int] = self._parse(expr)

def _parse(self, expr: str) -&gt; Set[int]:
&quot;&quot;&quot;Parse cron field expression into set of valid values.&quot;&quot;&quot;
expr = expr.upper()

# Apply aliases (JAN-&gt;1, MON-&gt;1, etc.)
for alias, value in self.aliases.items():
expr = expr.replace(alias, str(value))

values = set()

for part in expr.split(','):
if '/' in part:
# Step values: */5 or 1-10/2
range_part, step = part.split('/')
step = int(step)
if range_part == '*':
start, end = self.min_val, self.max_val
else:
start, end = self._parse_range(range_part)
values.update(range(start, end + 1, step))
elif '-' in part:
# Range: 1-5
start, end = self._parse_range(part)
values.update(range(start, end + 1))
elif part == '*':
values.update(range(self.min_val, self.max_val + 1))
else:
# Single value
values.add(int(part))

return values

def _parse_range(self, expr: str) -&gt; tuple:
parts = expr.split('-')
return int(parts[0]), int(parts[1])

def matches(self, value: int) -&gt; bool:
return value in self.values

def next_value(self, current: int) -&gt; Optional[int]:
&quot;&quot;&quot;Find next valid value &gt;= current, or None if wrapped.&quot;&quot;&quot;
for v in sorted(self.values):
if v &gt;= current:
return v
return None


class CronExpression:
&quot;&quot;&quot;
Parse and evaluate cron expressions.
Format: minute hour day-of-month month day-of-week
&quot;&quot;&quot;

MONTH_ALIASES = {
'JAN': 1, 'FEB': 2, 'MAR': 3, 'APR': 4, 'MAY': 5, 'JUN': 6,
'JUL': 7, 'AUG': 8, 'SEP': 9, 'OCT': 10, 'NOV': 11, 'DEC': 12
}
DOW_ALIASES = {
'SUN': 0, 'MON': 1, 'TUE': 2, 'WED': 3, 'THU': 4, 'FRI': 5, 'SAT': 6
}

def __init__(self, expression: str):
parts = expression.strip().split()
if len(parts) != 5:
raise ValueError(f&quot;Cron expression must have 5 fields, got {len(parts)}&quot;)

self.minute = CronField(parts[0], 0, 59)
self.hour = CronField(parts[1], 0, 23)
self.day = CronField(parts[2], 1, 31)
self.month = CronField(parts[3], 1, 12, self.MONTH_ALIASES)
self.dow = CronField(parts[4], 0, 6, self.DOW_ALIASES)
self.expression = expression

def next_execution(self, from_time: datetime = None) -&gt; datetime:
&quot;&quot;&quot;Calculate next execution time after from_time.&quot;&quot;&quot;
if from_time is None:
from_time = datetime.now()

# Start from next minute
t = from_time.replace(second=0, microsecond=0) + timedelta(minutes=1)

# Search for up to 4 years (handles Feb 29)
max_iterations = 4 * 366 * 24 * 60

for _ in range(max_iterations):
if self._matches(t):
return t
t = self._advance(t)

raise ValueError(f&quot;No valid execution time found for: {self.expression}&quot;)

def _matches(self, t: datetime) -&gt; bool:
&quot;&quot;&quot;Check if datetime matches cron expression.&quot;&quot;&quot;
return (
self.minute.matches(t.minute) and
self.hour.matches(t.hour) and
self.day.matches(t.day) and
self.month.matches(t.month) and
self.dow.matches(t.weekday())  # Monday=0 in Python
)

def _advance(self, t: datetime) -&gt; datetime:
&quot;&quot;&quot;Advance to next candidate time.&quot;&quot;&quot;
# Simple advancement: next minute
# Production systems use smarter jumping
return t + timedelta(minutes=1)


# ============================================================
# Idempotency Manager
# ============================================================

class IdempotencyStore(ABC):
@abstractmethod
def check_and_set(self, key: str, ttl: int = 86400) -&gt; bool:
&quot;&quot;&quot;Return True if key was set (first execution), False if already exists.&quot;&quot;&quot;
pass

@abstractmethod
def get_result(self, key: str) -&gt; Optional[Any]:
&quot;&quot;&quot;Get cached result for idempotency key.&quot;&quot;&quot;
pass

@abstractmethod
def set_result(self, key: str, result: Any, ttl: int = 86400) -&gt; None:
&quot;&quot;&quot;Cache result for idempotency key.&quot;&quot;&quot;
pass


class InMemoryIdempotencyStore(IdempotencyStore):
&quot;&quot;&quot;Simple in-memory idempotency store for single-node deployments.&quot;&quot;&quot;

def __init__(self):
self._keys: Dict[str, float] = {}  # key -&gt; expiry_time
self._results: Dict[str, Any] = {}
self._lock = threading.Lock()

def check_and_set(self, key: str, ttl: int = 86400) -&gt; bool:
with self._lock:
self._cleanup_expired()
if key in self._keys:
return False
self._keys[key] = time.time() + ttl
return True

def get_result(self, key: str) -&gt; Optional[Any]:
with self._lock:
if key in self._keys and self._keys[key] &gt; time.time():
return self._results.get(key)
return None

def set_result(self, key: str, result: Any, ttl: int = 86400) -&gt; None:
with self._lock:
self._keys[key] = time.time() + ttl
self._results[key] = result

def _cleanup_expired(self):
now = time.time()
expired = [k for k, v in self._keys.items() if v &lt;= now]
for k in expired:
del self._keys[k]
self._results.pop(k, None)


# ============================================================
# Task Definition
# ============================================================

@dataclass
class Task:
task_id: str
name: str
func: Callable
args: tuple = ()
kwargs: dict = field(default_factory=dict)
scheduled_time: float = field(default_factory=time.time)
priority: int = 0
recurrence: RecurrenceType = RecurrenceType.ONCE
interval: float = 0
cron_expr: Optional[CronExpression] = None
max_retries: int = 3
retry_count: int = 0
retry_delay_base: float = 1.0
status: TaskStatus = TaskStatus.PENDING
result: Any = None
error: Optional[str] = None
idempotency_key: Optional[str] = None
timeout: float = 300  # 5 minutes default
created_at: float = field(default_factory=time.time)
started_at: Optional[float] = None
completed_at: Optional[float] = None

def __lt__(self, other):
# For heap comparison: (scheduled_time, -priority)
if self.scheduled_time != other.scheduled_time:
return self.scheduled_time &lt; other.scheduled_time
return self.priority &gt; other.priority  # Higher priority wins ties

def generate_idempotency_key(self) -&gt; str:
&quot;&quot;&quot;Generate deterministic idempotency key for this task execution.&quot;&quot;&quot;
if self.idempotency_key:
return self.idempotency_key

# Combine task_id with scheduled execution time
key_data = f&quot;{self.task_id}:{int(self.scheduled_time)}&quot;
return hashlib.sha256(key_data.encode()).hexdigest()[:16]


# ============================================================
# Task Scheduler
# ============================================================

class TaskScheduler:
&quot;&quot;&quot;
Production-ready task scheduler with:
- Priority queue based scheduling
- Cron expression support
- Failure recovery with retries
- Idempotent execution
- Thread-safe operations
&quot;&quot;&quot;

def __init__(
self,
num_workers: int = 4,
idempotency_store: Optional[IdempotencyStore] = None
):
self.task_queue: List[Task] = []
self.tasks: Dict[str, Task] = {}
self.lock = threading.Lock()
self.condition = threading.Condition(self.lock)
self.running = False
self.num_workers = num_workers
self.workers: List[threading.Thread] = []
self.idempotency_store = idempotency_store or InMemoryIdempotencyStore()

# Heartbeat tracking for failure detection
self.worker_heartbeats: Dict[int, float] = {}
self.heartbeat_interval = 10  # seconds
self.heartbeat_timeout = 30  # seconds

def schedule(
self,
func: Callable,
delay: float = 0,
priority: int = 0,
name: str = &quot;&quot;,
max_retries: int = 3,
timeout: float = 300,
idempotency_key: str = None,
*args,
**kwargs
) -&gt; str:
&quot;&quot;&quot;Schedule a one-time task.&quot;&quot;&quot;
task = Task(
task_id=str(uuid.uuid4())[:8],
name=name or func.__name__,
func=func,
args=args,
kwargs=kwargs,
scheduled_time=time.time() + delay,
priority=priority,
max_retries=max_retries,
timeout=timeout,
idempotency_key=idempotency_key
)

return self._add_task(task)

def schedule_recurring(
self,
func: Callable,
interval: float,
priority: int = 0,
name: str = &quot;&quot;,
start_delay: float = 0,
max_retries: int = 3,
*args,
**kwargs
) -&gt; str:
&quot;&quot;&quot;Schedule a recurring task with fixed interval.&quot;&quot;&quot;
task = Task(
task_id=str(uuid.uuid4())[:8],
name=name or func.__name__,
func=func,
args=args,
kwargs=kwargs,
scheduled_time=time.time() + start_delay,
priority=priority,
recurrence=RecurrenceType.INTERVAL,
interval=interval,
max_retries=max_retries
)

return self._add_task(task)

def schedule_cron(
self,
func: Callable,
cron_expression: str,
priority: int = 0,
name: str = &quot;&quot;,
max_retries: int = 3,
*args,
**kwargs
) -&gt; str:
&quot;&quot;&quot;Schedule a task with cron expression.&quot;&quot;&quot;
cron = CronExpression(cron_expression)
next_run = cron.next_execution()

task = Task(
task_id=str(uuid.uuid4())[:8],
name=name or func.__name__,
func=func,
args=args,
kwargs=kwargs,
scheduled_time=next_run.timestamp(),
priority=priority,
recurrence=RecurrenceType.CRON,
cron_expr=cron,
max_retries=max_retries
)

return self._add_task(task)

def _add_task(self, task: Task) -&gt; str:
&quot;&quot;&quot;Add task to queue with thread safety.&quot;&quot;&quot;
with self.condition:
heapq.heappush(self.task_queue, task)
self.tasks[task.task_id] = task
self.condition.notify()
logger.info(f&quot;Scheduled task '{task.name}' ({task.task_id}) for {datetime.fromtimestamp(task.scheduled_time)}&quot;)

return task.task_id

def cancel(self, task_id: str) -&gt; bool:
&quot;&quot;&quot;Cancel a pending task.&quot;&quot;&quot;
with self.lock:
if task_id in self.tasks:
task = self.tasks[task_id]
if task.status == TaskStatus.PENDING:
task.status = TaskStatus.CANCELLED
logger.info(f&quot;Cancelled task '{task.name}' ({task_id})&quot;)
return True
return False

def get_status(self, task_id: str) -&gt; Optional[Dict]:
&quot;&quot;&quot;Get detailed task status.&quot;&quot;&quot;
with self.lock:
task = self.tasks.get(task_id)
if task:
return {
'task_id': task.task_id,
'name': task.name,
'status': task.status.value,
'scheduled_time': datetime.fromtimestamp(task.scheduled_time).isoformat(),
'retry_count': task.retry_count,
'max_retries': task.max_retries,
'result': task.result,
'error': task.error,
'created_at': datetime.fromtimestamp(task.created_at).isoformat(),
'started_at': datetime.fromtimestamp(task.started_at).isoformat() if task.started_at else None,
'completed_at': datetime.fromtimestamp(task.completed_at).isoformat() if task.completed_at else None
}
return None

def _worker(self, worker_id: int):
&quot;&quot;&quot;Worker thread that processes tasks.&quot;&quot;&quot;
logger.info(f&quot;Worker-{worker_id} started&quot;)

while self.running:
task = None

with self.condition:
# Update heartbeat
self.worker_heartbeats[worker_id] = time.time()

# Wait for task to be due
while self.running:
if not self.task_queue:
self.condition.wait(timeout=1.0)
continue

next_task = self.task_queue[0]
wait_time = next_task.scheduled_time - time.time()

if wait_time &lt;= 0:
# Task is due
task = heapq.heappop(self.task_queue)
break
else:
# Wait until task is due or new task arrives
self.condition.wait(timeout=min(wait_time, 1.0))

if not self.running:
break

if task and task.status == TaskStatus.PENDING:
self._execute_task(task, worker_id)

def _execute_task(self, task: Task, worker_id: int):
&quot;&quot;&quot;Execute a task with idempotency and retry handling.&quot;&quot;&quot;
idempotency_key = task.generate_idempotency_key()

# Check idempotency
if not self.idempotency_store.check_and_set(idempotency_key):
cached_result = self.idempotency_store.get_result(idempotency_key)
logger.info(f&quot;Task '{task.name}' already executed (idempotent), returning cached result&quot;)
task.result = cached_result
task.status = TaskStatus.COMPLETED
return

task.status = TaskStatus.RUNNING
task.started_at = time.time()

try:
logger.info(f&quot;Worker-{worker_id} executing task '{task.name}' ({task.task_id})&quot;)
result = task.func(*task.args, **task.kwargs)

task.result = result
task.status = TaskStatus.COMPLETED
task.completed_at = time.time()

# Cache result for idempotency
self.idempotency_store.set_result(idempotency_key, result)

logger.info(f&quot;Task '{task.name}' completed successfully: {result}&quot;)

# Schedule next occurrence for recurring tasks
self._schedule_next_occurrence(task)

except Exception as e:
task.error = str(e)
logger.error(f&quot;Task '{task.name}' failed: {e}&quot;)

# Retry logic
if task.retry_count &lt; task.max_retries:
self._schedule_retry(task)
else:
task.status = TaskStatus.FAILED
task.completed_at = time.time()
logger.error(f&quot;Task '{task.name}' exhausted retries, marked as FAILED&quot;)

def _schedule_retry(self, task: Task):
&quot;&quot;&quot;Schedule task retry with exponential backoff.&quot;&quot;&quot;
task.retry_count += 1

# Exponential backoff with jitter
delay = task.retry_delay_base * (2 ** (task.retry_count - 1))
delay = delay * (0.5 + 0.5 * (hash(task.task_id) % 100) / 100)  # Add jitter

task.scheduled_time = time.time() + delay
task.status = TaskStatus.RETRYING
task.error = None

with self.condition:
heapq.heappush(self.task_queue, task)
self.condition.notify()

logger.info(f&quot;Scheduled retry {task.retry_count}/{task.max_retries} for task '{task.name}' in {delay:.1f}s&quot;)

def _schedule_next_occurrence(self, task: Task):
&quot;&quot;&quot;Schedule next occurrence for recurring tasks.&quot;&quot;&quot;
if task.recurrence == RecurrenceType.ONCE:
return

# Create new task instance for next occurrence
if task.recurrence == RecurrenceType.INTERVAL:
next_time = time.time() + task.interval
elif task.recurrence == RecurrenceType.CRON:
next_time = task.cron_expr.next_execution().timestamp()
else:
return

new_task = Task(
task_id=str(uuid.uuid4())[:8],
name=task.name,
func=task.func,
args=task.args,
kwargs=task.kwargs,
scheduled_time=next_time,
priority=task.priority,
recurrence=task.recurrence,
interval=task.interval,
cron_expr=task.cron_expr,
max_retries=task.max_retries
)

with self.condition:
heapq.heappush(self.task_queue, new_task)
self.tasks[new_task.task_id] = new_task
self.condition.notify()

logger.info(f&quot;Scheduled next occurrence of '{task.name}' for {datetime.fromtimestamp(next_time)}&quot;)

def start(self):
&quot;&quot;&quot;Start the scheduler.&quot;&quot;&quot;
self.running = True

for i in range(self.num_workers):
worker = threading.Thread(
target=self._worker,
args=(i,),
name=f&quot;Worker-{i}&quot;,
daemon=True
)
worker.start()
self.workers.append(worker)

logger.info(f&quot;Scheduler started with {self.num_workers} workers&quot;)

def stop(self, timeout: float = 5.0):
&quot;&quot;&quot;Stop the scheduler gracefully.&quot;&quot;&quot;
logger.info(&quot;Stopping scheduler...&quot;)
self.running = False

with self.condition:
self.condition.notify_all()

for worker in self.workers:
worker.join(timeout=timeout)

self.workers.clear()
logger.info(&quot;Scheduler stopped&quot;)

def get_stats(self) -&gt; Dict:
&quot;&quot;&quot;Get scheduler statistics.&quot;&quot;&quot;
with self.lock:
status_counts = {}
for task in self.tasks.values():
status = task.status.value
status_counts[status] = status_counts.get(status, 0) + 1

return {
'total_tasks': len(self.tasks),
'pending_in_queue': len(self.task_queue),
'status_breakdown': status_counts,
'active_workers': len(self.workers)
}


# ============================================================
# Usage Example
# ============================================================

if __name__ == &quot;__main__&quot;:
# Define task functions
def send_notification(user_id: int, message: str) -&gt; str:
time.sleep(0.5)  # Simulate work
return f&quot;Notification sent to user {user_id}: {message}&quot;

def process_batch(batch_id: int) -&gt; Dict:
time.sleep(1)  # Simulate work
return {&quot;batch_id&quot;: batch_id, &quot;processed&quot;: 100}

def health_check() -&gt; str:
return f&quot;healthy at {datetime.now().isoformat()}&quot;

def flaky_task() -&gt; str:
import random
if random.random() &lt; 0.7:  # 70% failure rate
raise Exception(&quot;Random failure!&quot;)
return &quot;Success!&quot;

# Create and start scheduler
scheduler = TaskScheduler(num_workers=2)
scheduler.start()

# Schedule various tasks
task1 = scheduler.schedule(
send_notification,
delay=1,
priority=10,
name=&quot;welcome_notification&quot;,
user_id=123,
message=&quot;Welcome to the platform!&quot;
)

task2 = scheduler.schedule(
process_batch,
delay=2,
priority=5,
name=&quot;batch_processing&quot;,
batch_id=456
)

# Recurring task with interval
task3 = scheduler.schedule_recurring(
health_check,
interval=5,
name=&quot;health_monitor&quot;
)

# Cron-scheduled task (every minute)
task4 = scheduler.schedule_cron(
lambda: logger.info(&quot;Cron task executed!&quot;),
&quot;* * * * *&quot;,
name=&quot;minutely_task&quot;
)

# Task with retries
task5 = scheduler.schedule(
flaky_task,
delay=3,
name=&quot;flaky_operation&quot;,
max_retries=5
)

# Let tasks run
time.sleep(20)

# Check statistics
print(&quot;\n--- Scheduler Stats ---&quot;)
print(scheduler.get_stats())

# Check individual task status
print(&quot;\n--- Task Status ---&quot;)
for task_id in [task1, task2, task5]:
status = scheduler.get_status(task_id)
if status:
print(f&quot;{status['name']}: {status['status']}&quot;)

scheduler.stop()</code></pre>
<h3 id="go-implementation">Go Implementation</h3>
<pre><code class="language-go">package main

import (
&quot;container/heap&quot;
&quot;context&quot;
&quot;crypto/sha256&quot;
&quot;encoding/hex&quot;
&quot;fmt&quot;
&quot;log&quot;
&quot;math&quot;
&quot;math/rand&quot;
&quot;strconv&quot;
&quot;strings&quot;
&quot;sync&quot;
&quot;time&quot;
)

// TaskStatus represents the current state of a task
type TaskStatus string

const (
StatusPending   TaskStatus = &quot;pending&quot;
StatusRunning   TaskStatus = &quot;running&quot;
StatusCompleted TaskStatus = &quot;completed&quot;
StatusFailed    TaskStatus = &quot;failed&quot;
StatusCancelled TaskStatus = &quot;cancelled&quot;
StatusRetrying  TaskStatus = &quot;retrying&quot;
)

// RecurrenceType defines how a task recurs
type RecurrenceType string

const (
RecurrenceOnce     RecurrenceType = &quot;once&quot;
RecurrenceInterval RecurrenceType = &quot;interval&quot;
RecurrenceCron     RecurrenceType = &quot;cron&quot;
)

// ============================================================
// Cron Expression Parser
// ============================================================

// CronField represents a single field in a cron expression
type CronField struct {
values map[int]bool
min    int
max    int
}

func NewCronField(expr string, min, max int, aliases map[string]int) (*CronField, error) {
cf := &amp;CronField{
values: make(map[int]bool),
min:    min,
max:    max,
}

expr = strings.ToUpper(expr)
for alias, val := range aliases {
expr = strings.ReplaceAll(expr, alias, strconv.Itoa(val))
}

parts := strings.Split(expr, &quot;,&quot;)
for _, part := range parts {
if err := cf.parsePart(part); err != nil {
return nil, err
}
}

return cf, nil
}

func (cf *CronField) parsePart(part string) error {
if strings.Contains(part, &quot;/&quot;) {
// Step values: */5 or 1-10/2
segments := strings.Split(part, &quot;/&quot;)
step, _ := strconv.Atoi(segments[1])

var start, end int
if segments[0] == &quot;*&quot; {
start, end = cf.min, cf.max
} else {
rangeParts := strings.Split(segments[0], &quot;-&quot;)
start, _ = strconv.Atoi(rangeParts[0])
end, _ = strconv.Atoi(rangeParts[1])
}

for i := start; i &lt;= end; i += step {
cf.values[i] = true
}
} else if strings.Contains(part, &quot;-&quot;) {
// Range: 1-5
rangeParts := strings.Split(part, &quot;-&quot;)
start, _ := strconv.Atoi(rangeParts[0])
end, _ := strconv.Atoi(rangeParts[1])
for i := start; i &lt;= end; i++ {
cf.values[i] = true
}
} else if part == &quot;*&quot; {
for i := cf.min; i &lt;= cf.max; i++ {
cf.values[i] = true
}
} else {
val, _ := strconv.Atoi(part)
cf.values[val] = true
}
return nil
}

func (cf *CronField) Matches(value int) bool {
return cf.values[value]
}

// CronExpression parses and evaluates cron expressions
type CronExpression struct {
minute *CronField
hour   *CronField
day    *CronField
month  *CronField
dow    *CronField
expr   string
}

var monthAliases = map[string]int{
&quot;JAN&quot;: 1, &quot;FEB&quot;: 2, &quot;MAR&quot;: 3, &quot;APR&quot;: 4, &quot;MAY&quot;: 5, &quot;JUN&quot;: 6,
&quot;JUL&quot;: 7, &quot;AUG&quot;: 8, &quot;SEP&quot;: 9, &quot;OCT&quot;: 10, &quot;NOV&quot;: 11, &quot;DEC&quot;: 12,
}

var dowAliases = map[string]int{
&quot;SUN&quot;: 0, &quot;MON&quot;: 1, &quot;TUE&quot;: 2, &quot;WED&quot;: 3, &quot;THU&quot;: 4, &quot;FRI&quot;: 5, &quot;SAT&quot;: 6,
}

func NewCronExpression(expression string) (*CronExpression, error) {
parts := strings.Fields(expression)
if len(parts) != 5 {
return nil, fmt.Errorf(&quot;cron expression must have 5 fields, got %d&quot;, len(parts))
}

minute, _ := NewCronField(parts[0], 0, 59, nil)
hour, _ := NewCronField(parts[1], 0, 23, nil)
day, _ := NewCronField(parts[2], 1, 31, nil)
month, _ := NewCronField(parts[3], 1, 12, monthAliases)
dow, _ := NewCronField(parts[4], 0, 6, dowAliases)

return &amp;CronExpression{
minute: minute,
hour:   hour,
day:    day,
month:  month,
dow:    dow,
expr:   expression,
}, nil
}

func (ce *CronExpression) NextExecution(from time.Time) time.Time {
t := from.Truncate(time.Minute).Add(time.Minute)

maxIterations := 4 * 366 * 24 * 60
for i := 0; i &lt; maxIterations; i++ {
if ce.matches(t) {
return t
}
t = t.Add(time.Minute)
}

return time.Time{}
}

func (ce *CronExpression) matches(t time.Time) bool {
return ce.minute.Matches(t.Minute()) &amp;&amp;
ce.hour.Matches(t.Hour()) &amp;&amp;
ce.day.Matches(t.Day()) &amp;&amp;
ce.month.Matches(int(t.Month())) &amp;&amp;
ce.dow.Matches(int(t.Weekday()))
}

// ============================================================
// Idempotency Store
// ============================================================

type IdempotencyStore interface {
CheckAndSet(key string, ttl time.Duration) bool
GetResult(key string) (interface{}, bool)
SetResult(key string, result interface{}, ttl time.Duration)
}

type InMemoryIdempotencyStore struct {
mu      sync.RWMutex
keys    map[string]time.Time
results map[string]interface{}
}

func NewInMemoryIdempotencyStore() *InMemoryIdempotencyStore {
return &amp;InMemoryIdempotencyStore{
keys:    make(map[string]time.Time),
results: make(map[string]interface{}),
}
}

func (s *InMemoryIdempotencyStore) CheckAndSet(key string, ttl time.Duration) bool {
s.mu.Lock()
defer s.mu.Unlock()

s.cleanupExpired()

if _, exists := s.keys[key]; exists {
return false
}

s.keys[key] = time.Now().Add(ttl)
return true
}

func (s *InMemoryIdempotencyStore) GetResult(key string) (interface{}, bool) {
s.mu.RLock()
defer s.mu.RUnlock()

if expiry, exists := s.keys[key]; exists &amp;&amp; expiry.After(time.Now()) {
result, ok := s.results[key]
return result, ok
}
return nil, false
}

func (s *InMemoryIdempotencyStore) SetResult(key string, result interface{}, ttl time.Duration) {
s.mu.Lock()
defer s.mu.Unlock()

s.keys[key] = time.Now().Add(ttl)
s.results[key] = result
}

func (s *InMemoryIdempotencyStore) cleanupExpired() {
now := time.Now()
for k, expiry := range s.keys {
if expiry.Before(now) {
delete(s.keys, k)
delete(s.results, k)
}
}
}

// ============================================================
// Task Definition
// ============================================================

type Task struct {
ID             string
Name           string
Func           func() (interface{}, error)
ScheduledTime  time.Time
Priority       int
Recurrence     RecurrenceType
Interval       time.Duration
CronExpr       *CronExpression
MaxRetries     int
RetryCount     int
RetryDelayBase time.Duration
Status         TaskStatus
Result         interface{}
Error          error
IdempotencyKey string
Timeout        time.Duration
CreatedAt      time.Time
StartedAt      time.Time
CompletedAt    time.Time
index          int // for heap
}

func (t *Task) GenerateIdempotencyKey() string {
if t.IdempotencyKey != &quot;&quot; {
return t.IdempotencyKey
}
data := fmt.Sprintf(&quot;%s:%d&quot;, t.ID, t.ScheduledTime.Unix())
hash := sha256.Sum256([]byte(data))
return hex.EncodeToString(hash[:])[:16]
}

// TaskQueue implements heap.Interface
type TaskQueue []*Task

func (pq TaskQueue) Len() int { return len(pq) }

func (pq TaskQueue) Less(i, j int) bool {
if pq[i].ScheduledTime.Equal(pq[j].ScheduledTime) {
return pq[i].Priority &gt; pq[j].Priority
}
return pq[i].ScheduledTime.Before(pq[j].ScheduledTime)
}

func (pq TaskQueue) Swap(i, j int) {
pq[i], pq[j] = pq[j], pq[i]
pq[i].index = i
pq[j].index = j
}

func (pq *TaskQueue) Push(x interface{}) {
n := len(*pq)
task := x.(*Task)
task.index = n
*pq = append(*pq, task)
}

func (pq *TaskQueue) Pop() interface{} {
old := *pq
n := len(old)
task := old[n-1]
old[n-1] = nil
task.index = -1
*pq = old[0 : n-1]
return task
}

// ============================================================
// Task Scheduler
// ============================================================

type Scheduler struct {
queue            TaskQueue
tasks            map[string]*Task
mu               sync.Mutex
cond             *sync.Cond
running          bool
numWorkers       int
wg               sync.WaitGroup
idempotencyStore IdempotencyStore
ctx              context.Context
cancel           context.CancelFunc
taskIDCounter    int
}

func NewScheduler(numWorkers int) *Scheduler {
s := &amp;Scheduler{
queue:            make(TaskQueue, 0),
tasks:            make(map[string]*Task),
numWorkers:       numWorkers,
idempotencyStore: NewInMemoryIdempotencyStore(),
}
s.cond = sync.NewCond(&amp;s.mu)
heap.Init(&amp;s.queue)
return s
}

func (s *Scheduler) generateTaskID() string {
s.taskIDCounter++
return fmt.Sprintf(&quot;task-%d&quot;, s.taskIDCounter)
}

func (s *Scheduler) Schedule(name string, delay time.Duration, priority int, fn func() (interface{}, error)) string {
s.mu.Lock()
defer s.mu.Unlock()

task := &amp;Task{
ID:             s.generateTaskID(),
Name:           name,
Func:           fn,
ScheduledTime:  time.Now().Add(delay),
Priority:       priority,
Recurrence:     RecurrenceOnce,
MaxRetries:     3,
RetryDelayBase: time.Second,
Status:         StatusPending,
Timeout:        5 * time.Minute,
CreatedAt:      time.Now(),
}

heap.Push(&amp;s.queue, task)
s.tasks[task.ID] = task
s.cond.Signal()

log.Printf(&quot;Scheduled task '%s' (%s) for %v&quot;, task.Name, task.ID, task.ScheduledTime)
return task.ID
}

func (s *Scheduler) ScheduleRecurring(name string, interval time.Duration, priority int, fn func() (interface{}, error)) string {
s.mu.Lock()
defer s.mu.Unlock()

task := &amp;Task{
ID:             s.generateTaskID(),
Name:           name,
Func:           fn,
ScheduledTime:  time.Now(),
Priority:       priority,
Recurrence:     RecurrenceInterval,
Interval:       interval,
MaxRetries:     3,
RetryDelayBase: time.Second,
Status:         StatusPending,
Timeout:        5 * time.Minute,
CreatedAt:      time.Now(),
}

heap.Push(&amp;s.queue, task)
s.tasks[task.ID] = task
s.cond.Signal()

log.Printf(&quot;Scheduled recurring task '%s' (%s) with interval %v&quot;, task.Name, task.ID, interval)
return task.ID
}

func (s *Scheduler) ScheduleCron(name string, cronExpr string, priority int, fn func() (interface{}, error)) (string, error) {
cron, err := NewCronExpression(cronExpr)
if err != nil {
return &quot;&quot;, err
}

s.mu.Lock()
defer s.mu.Unlock()

nextRun := cron.NextExecution(time.Now())

task := &amp;Task{
ID:             s.generateTaskID(),
Name:           name,
Func:           fn,
ScheduledTime:  nextRun,
Priority:       priority,
Recurrence:     RecurrenceCron,
CronExpr:       cron,
MaxRetries:     3,
RetryDelayBase: time.Second,
Status:         StatusPending,
Timeout:        5 * time.Minute,
CreatedAt:      time.Now(),
}

heap.Push(&amp;s.queue, task)
s.tasks[task.ID] = task
s.cond.Signal()

log.Printf(&quot;Scheduled cron task '%s' (%s) for %v&quot;, task.Name, task.ID, nextRun)
return task.ID, nil
}

func (s *Scheduler) Cancel(taskID string) bool {
s.mu.Lock()
defer s.mu.Unlock()

if task, exists := s.tasks[taskID]; exists {
if task.Status == StatusPending {
task.Status = StatusCancelled
log.Printf(&quot;Cancelled task '%s' (%s)&quot;, task.Name, taskID)
return true
}
}
return false
}

func (s *Scheduler) GetStatus(taskID string) map[string]interface{} {
s.mu.Lock()
defer s.mu.Unlock()

task, exists := s.tasks[taskID]
if !exists {
return nil
}

return map[string]interface{}{
&quot;task_id&quot;:        task.ID,
&quot;name&quot;:           task.Name,
&quot;status&quot;:         task.Status,
&quot;scheduled_time&quot;: task.ScheduledTime,
&quot;retry_count&quot;:    task.RetryCount,
&quot;max_retries&quot;:    task.MaxRetries,
&quot;result&quot;:         task.Result,
&quot;error&quot;:          task.Error,
}
}

func (s *Scheduler) worker(id int) {
defer s.wg.Done()
log.Printf(&quot;Worker-%d started&quot;, id)

for {
s.mu.Lock()

for s.running &amp;&amp; (len(s.queue) == 0 || s.queue[0].ScheduledTime.After(time.Now())) {
if len(s.queue) &gt; 0 {
waitTime := time.Until(s.queue[0].ScheduledTime)
if waitTime &gt; 0 {
timer := time.AfterFunc(waitTime, func() {
s.cond.Broadcast()
})
s.cond.Wait()
timer.Stop()
}
} else {
s.cond.Wait()
}

if !s.running {
s.mu.Unlock()
return
}
}

if !s.running {
s.mu.Unlock()
return
}

var task *Task
if len(s.queue) &gt; 0 &amp;&amp; !s.queue[0].ScheduledTime.After(time.Now()) {
task = heap.Pop(&amp;s.queue).(*Task)
}
s.mu.Unlock()

if task != nil &amp;&amp; task.Status == StatusPending {
s.executeTask(task, id)
}
}
}

func (s *Scheduler) executeTask(task *Task, workerID int) {
idempotencyKey := task.GenerateIdempotencyKey()

// Check idempotency
if !s.idempotencyStore.CheckAndSet(idempotencyKey, 24*time.Hour) {
if result, ok := s.idempotencyStore.GetResult(idempotencyKey); ok {
log.Printf(&quot;Task '%s' already executed (idempotent), returning cached result&quot;, task.Name)
task.Result = result
task.Status = StatusCompleted
return
}
}

task.Status = StatusRunning
task.StartedAt = time.Now()

log.Printf(&quot;Worker-%d executing task '%s' (%s)&quot;, workerID, task.Name, task.ID)

result, err := task.Func()

s.mu.Lock()
if err != nil {
task.Error = err
log.Printf(&quot;Task '%s' failed: %v&quot;, task.Name, err)

if task.RetryCount &lt; task.MaxRetries {
s.scheduleRetry(task)
} else {
task.Status = StatusFailed
task.CompletedAt = time.Now()
log.Printf(&quot;Task '%s' exhausted retries, marked as FAILED&quot;, task.Name)
}
} else {
task.Result = result
task.Status = StatusCompleted
task.CompletedAt = time.Now()

s.idempotencyStore.SetResult(idempotencyKey, result, 24*time.Hour)

log.Printf(&quot;Task '%s' completed successfully: %v&quot;, task.Name, result)

s.scheduleNextOccurrence(task)
}
s.mu.Unlock()
}

func (s *Scheduler) scheduleRetry(task *Task) {
task.RetryCount++

// Exponential backoff with jitter
delay := float64(task.RetryDelayBase) * math.Pow(2, float64(task.RetryCount-1))
jitter := 0.5 + 0.5*rand.Float64()
delay = delay * jitter

task.ScheduledTime = time.Now().Add(time.Duration(delay))
task.Status = StatusRetrying
task.Error = nil

heap.Push(&amp;s.queue, task)
s.cond.Signal()

log.Printf(&quot;Scheduled retry %d/%d for task '%s' in %v&quot;,
task.RetryCount, task.MaxRetries, task.Name, time.Duration(delay))
}

func (s *Scheduler) scheduleNextOccurrence(task *Task) {
if task.Recurrence == RecurrenceOnce {
return
}

var nextTime time.Time
if task.Recurrence == RecurrenceInterval {
nextTime = time.Now().Add(task.Interval)
} else if task.Recurrence == RecurrenceCron {
nextTime = task.CronExpr.NextExecution(time.Now())
} else {
return
}

newTask := &amp;Task{
ID:             s.generateTaskID(),
Name:           task.Name,
Func:           task.Func,
ScheduledTime:  nextTime,
Priority:       task.Priority,
Recurrence:     task.Recurrence,
Interval:       task.Interval,
CronExpr:       task.CronExpr,
MaxRetries:     task.MaxRetries,
RetryDelayBase: task.RetryDelayBase,
Status:         StatusPending,
Timeout:        task.Timeout,
CreatedAt:      time.Now(),
}

heap.Push(&amp;s.queue, newTask)
s.tasks[newTask.ID] = newTask
s.cond.Signal()

log.Printf(&quot;Scheduled next occurrence of '%s' for %v&quot;, task.Name, nextTime)
}

func (s *Scheduler) Start() {
s.mu.Lock()
s.running = true
s.ctx, s.cancel = context.WithCancel(context.Background())
s.mu.Unlock()

for i := 0; i &lt; s.numWorkers; i++ {
s.wg.Add(1)
go s.worker(i)
}

log.Printf(&quot;Scheduler started with %d workers&quot;, s.numWorkers)
}

func (s *Scheduler) Stop() {
s.mu.Lock()
s.running = false
s.cancel()
s.cond.Broadcast()
s.mu.Unlock()

s.wg.Wait()
log.Println(&quot;Scheduler stopped&quot;)
}

func (s *Scheduler) GetStats() map[string]interface{} {
s.mu.Lock()
defer s.mu.Unlock()

statusCounts := make(map[TaskStatus]int)
for _, task := range s.tasks {
statusCounts[task.Status]++
}

return map[string]interface{}{
&quot;total_tasks&quot;:      len(s.tasks),
&quot;pending_in_queue&quot;: len(s.queue),
&quot;status_breakdown&quot;: statusCounts,
&quot;active_workers&quot;:   s.numWorkers,
}
}

// ============================================================
// Main
// ============================================================

func main() {
scheduler := NewScheduler(2)
scheduler.Start()

// One-time task
scheduler.Schedule(&quot;send_notification&quot;, time.Second, 10, func() (interface{}, error) {
return &quot;Notification sent!&quot;, nil
})

// Recurring task
scheduler.ScheduleRecurring(&quot;health_check&quot;, 5*time.Second, 5, func() (interface{}, error) {
return fmt.Sprintf(&quot;healthy at %v&quot;, time.Now().Format(&quot;15:04:05&quot;)), nil
})

// Cron task (every minute)
scheduler.ScheduleCron(&quot;minutely_report&quot;, &quot;* * * * *&quot;, 3, func() (interface{}, error) {
return &quot;Minutely report generated&quot;, nil
})

// Flaky task to demonstrate retries
scheduler.Schedule(&quot;flaky_operation&quot;, 2*time.Second, 5, func() (interface{}, error) {
if rand.Float64() &lt; 0.7 {
return nil, fmt.Errorf(&quot;random failure&quot;)
}
return &quot;Success!&quot;, nil
})

time.Sleep(30 * time.Second)

fmt.Println(&quot;\n--- Scheduler Stats ---&quot;)
stats := scheduler.GetStats()
fmt.Printf(&quot;%+v\n&quot;, stats)

scheduler.Stop()
}</code></pre>
<hr />
<h2 id="design-considerations-summary">Design Considerations Summary</h2>
<div style="background: #eff6ff; border-radius: 16px; padding: 32px; margin: 24px 0">
<h4 style="color: #1e40af; margin: 0 0 24px 0; font-size: 16px">Architecture Decision Matrix</h4>
<div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 16px">
<div style="background: #f8fafc; padding: 16px; border-radius: 8px">
<div style="color: #166534; font-weight: bold; font-size: 12px; margin-bottom: 8px">Single Node</div>
<div style="color: #1e293b; font-size: 10px; line-height: 1.6">
  Use when: < 10K tasks, simple ops<br>
  Data structure: Binary min-heap<br>
  Concurrency: Condition variables<br>
  Recovery: Process restart
</div>
</div>
<div style="background: #f8fafc; padding: 16px; border-radius: 8px">
<div style="color: #1e40af; font-weight: bold; font-size: 12px; margin-bottom: 8px">Multi-Node (Leader)</div>
<div style="color: #1e293b; font-size: 10px; line-height: 1.6">
  Use when: Need HA, < 100K tasks<br>
  Coordination: ZooKeeper/etcd<br>
  Concurrency: Leader election<br>
  Recovery: Automatic failover
</div>
</div>
<div style="background: #f8fafc; padding: 16px; border-radius: 8px">
<div style="color: #5b21b6; font-weight: bold; font-size: 12px; margin-bottom: 8px">Multi-Node (Partitioned)</div>
<div style="color: #1e293b; font-size: 10px; line-height: 1.6">
  Use when: > 100K tasks, horizontal scale<br>
  Coordination: Consistent hashing<br>
  Concurrency: Partition locks<br>
  Recovery: Partition rebalancing
</div>
</div>
<div style="background: #f8fafc; padding: 16px; border-radius: 8px">
<div style="color: #c2410c; font-weight: bold; font-size: 12px; margin-bottom: 8px">Database-Backed</div>
<div style="color: #1e293b; font-size: 10px; line-height: 1.6">
  Use when: Durability critical, audit needed<br>
  Data structure: Indexed table<br>
  Concurrency: SKIP LOCKED<br>
  Recovery: Transaction rollback
</div>
</div>
</div>
</div>
<hr />
<h2 id="deep-dive-interview-qa-3-level-recursive-format">Deep-Dive Interview Q&amp;A: 3-Level Recursive Format</h2>
<div style="background: #f8fafc;border-radius: 16px; padding: 32px; margin: 24px 0">
<h3 style="color: #1e40af; margin: 0 0 24px 0">Complete Task Scheduler System Design</h3>
<div style="background: #f8fafc; padding: 20px; border-radius: 12px; margin-bottom: 20px">
<div style="color: #166534; font-weight: bold; font-size: 14px; margin-bottom: 12px">Level 1: "Design a task scheduler system"</div>
<details>
<summary style="color: #5b21b6; cursor: pointer; font-size: 13px">Comprehensive Answer</summary>
<div style="margin-top: 16px; padding: 16px; background: #f0fdf4; border-radius: 8px">
<p><strong>High-Level Architecture</strong>:</p>
<p>A task scheduler consists of four main components:</p>
<ol>
<li><span style="color: #166534"><strong>Task Storage</strong></span>: Where task definitions and states are persisted</li>
<li><span style="color: #166534"><strong>Scheduler Core</strong></span>: Determines which tasks to execute and when</li>
<li><span style="color: #166534"><strong>Worker Pool</strong></span>: Executes the actual task logic</li>
<li><span style="color: #166534"><strong>Coordination Layer</strong></span>: Handles distributed concerns (if multi-node)</li>
</ol>
<p><strong>Core Data Structures</strong>:</p>
<pre><code class="language-python">class Task:
id: str
name: str
payload: dict                    # Task arguments
scheduled_time: datetime         # When to execute
priority: int                    # Higher = more important
status: Enum[PENDING, RUNNING, COMPLETED, FAILED]
recurrence: Optional[CronExpr]   # For recurring tasks
max_retries: int
retry_count: int

class Scheduler:
task_queue: MinHeap[Task]        # Ordered by (scheduled_time, -priority)
workers: ThreadPool
storage: TaskStorage             # Persistence layer</code></pre>
<p><strong>Scheduling Algorithm</strong>:</p>
<ol>
<li>Maintain a <span style="color: #166534"><strong>min-heap</strong></span> ordered by scheduled_time</li>
<li>Worker threads wait on condition variable</li>
<li>When top task's scheduled_time &lt;= now, pop and execute</li>
<li>After execution, handle retries or schedule next occurrence</li>
</ol>
<p><strong>Key Features to Mention</strong>:<br />
- One-time and recurring task support<br />
- Priority-based execution<br />
- Failure handling with configurable retries<br />
- Idempotent execution to handle duplicates<br />
- Persistence for crash recovery</p>
</div>
</details>
</div>
<div style="background: #f8fafc; padding: 20px; border-radius: 12px; margin-bottom: 20px; margin-left: 24px">
<div style="color: #1e40af; font-weight: bold; font-size: 14px; margin-bottom: 12px">Level 2: "How do you ensure exactly-once execution in a distributed scheduler?"</div>
<details>
<summary style="color: #5b21b6; cursor: pointer; font-size: 13px">Comprehensive Answer</summary>
<div style="margin-top: 16px; padding: 16px; background: #f0fdf4; border-radius: 8px">
<p><strong>The Core Challenge</strong>: In distributed systems, &quot;exactly-once&quot; is theoretically impossible due to the <a href="/algorithms/two-generals">[Two Generals Problem]</a>. We achieve <span style="color: #166534"><strong>effectively exactly-once</strong></span> through:</p>
<p><strong>Strategy 1: At-Least-Once Delivery + Idempotency</strong></p>
<pre><code class="language-python">def execute_task(task):
idempotency_key = f&quot;{task.id}:{task.scheduled_time.timestamp()}&quot;

# Atomic check-and-set
if not redis.setnx(idempotency_key, &quot;processing&quot;, ex=3600):
# Already executed or in progress
return get_cached_result(idempotency_key)

try:
result = task.execute()
cache_result(idempotency_key, result)
return result
except:
redis.delete(idempotency_key)  # Allow retry
raise</code></pre>
<p><strong>Strategy 2: Fencing Tokens</strong></p>
<pre><code class="language-python">def claim_task(task_id, worker_id):
# Increment fence token on claim
result = db.execute(&quot;&quot;&quot;
UPDATE tasks
SET status = 'claimed',
fence_token = fence_token + 1,
claimed_by = %s
WHERE id = %s AND status = 'pending'
RETURNING fence_token
&quot;&quot;&quot;, worker_id, task_id)

return result.fence_token

def execute_with_fence(task, fence_token):
# All writes include fence token
# Storage layer rejects writes with lower fence tokens
external_service.call(task.payload, fence_token=fence_token)</code></pre>
<p><strong>Strategy 3: Outbox Pattern for External Effects</strong></p>
<pre><code class="language-python">def process_order(order_id):
with db.transaction():
# All state changes in same transaction
update_order_status(order_id, 'processing')
# Don't call external API directly - write to outbox
insert_outbox_event({
'type': 'send_email',
'payload': {'order_id': order_id}
})

# Separate process reads outbox and calls external APIs
# with idempotency keys</code></pre>
<p><strong>Key Insight</strong>: The combination ensures that even if a task runs twice, external effects happen exactly once (via idempotency keys) and internal state is consistent (via fencing).</p>
</div>
</details>
</div>
<div style="background: #f8fafc; padding: 20px; border-radius: 12px; margin-bottom: 20px; margin-left: 48px">
<div style="color: #5b21b6; font-weight: bold; font-size: 14px; margin-bottom: 12px">Level 3: "What if the idempotency store (Redis) fails during the check-and-set?"</div>
<details>
<summary style="color: #5b21b6; cursor: pointer; font-size: 13px">Comprehensive Answer</summary>
<div style="margin-top: 16px; padding: 16px; background: #f0fdf4; border-radius: 8px">
<p><strong>Failure Modes</strong>:</p>
<ol>
<li><span style="color: #dc2626"><strong>Redis down before SETNX</strong></span>: Can't check, can't proceed</li>
<li><span style="color: #dc2626"><strong>Redis down after SETNX, before task execution</strong></span>: Key set but work not done</li>
<li><span style="color: #dc2626"><strong>Redis down after execution, before caching result</strong></span>: Work done but key may expire</li>
<li><span style="color: #dc2626"><strong>Redis comes back with data loss</strong></span>: Keys gone, duplicates possible</li>
</ol>
<p><strong>Solution: Multi-Layer Idempotency</strong></p>
<pre><code class="language-python">class RobustIdempotencyManager:
def __init__(self, redis, database):
self.redis = redis
self.db = database

def execute_once(self, idempotency_key, operation):
# Layer 1: Fast path - check Redis
try:
if self.redis.exists(idempotency_key):
return self.get_cached_result(idempotency_key)
except RedisError:
pass  # Fallback to database

# Layer 2: Durable check - database
existing = self.db.query(
&quot;SELECT result FROM idempotency_log WHERE key = %s&quot;,
idempotency_key
)
if existing:
# Backfill Redis for next time
self.try_cache_to_redis(idempotency_key, existing.result)
return existing.result

# Layer 3: Atomic claim in database
claimed = self.db.execute(&quot;&quot;&quot;
INSERT INTO idempotency_log (key, status, started_at)
VALUES (%s, 'processing', NOW())
ON CONFLICT (key) DO NOTHING
RETURNING key
&quot;&quot;&quot;, idempotency_key)

if not claimed:
# Another worker claimed it - wait and fetch result
return self.wait_for_result(idempotency_key)

try:
result = operation()

# Record completion in database (durable)
self.db.execute(&quot;&quot;&quot;
UPDATE idempotency_log
SET status = 'completed', result = %s, completed_at = NOW()
WHERE key = %s
&quot;&quot;&quot;, json.dumps(result), idempotency_key)

# Cache in Redis (fast path for future)
self.try_cache_to_redis(idempotency_key, result)

return result

except Exception as e:
# Mark as failed, allow retry
self.db.execute(&quot;&quot;&quot;
UPDATE idempotency_log
SET status = 'failed', error = %s
WHERE key = %s
&quot;&quot;&quot;, str(e), idempotency_key)
raise

def wait_for_result(self, key, timeout=30):
&quot;&quot;&quot;Poll database for result from other worker.&quot;&quot;&quot;
deadline = time.time() + timeout
while time.time() &lt; deadline:
row = self.db.query(
&quot;SELECT status, result FROM idempotency_log WHERE key = %s&quot;,
key
)
if row.status == 'completed':
return json.loads(row.result)
if row.status == 'failed':
raise Exception(&quot;Task failed on another worker&quot;)
time.sleep(0.5)
raise TimeoutError(&quot;Waiting for idempotency result&quot;)</code></pre>
<p><strong>Trade-offs</strong>:</p>
<table>
<thead>
<tr>
<th>Approach</th>
<th>Durability</th>
<th>Speed</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr>
<td>Redis only</td>
<td>Low (data loss)</td>
<td>Fast</td>
<td>Low</td>
</tr>
<tr>
<td>Database only</td>
<td>High</td>
<td>Slower</td>
<td>Low</td>
</tr>
<tr>
<td>Redis + DB</td>
<td>High</td>
<td>Fast for hits</td>
<td>Medium</td>
</tr>
<tr>
<td>Redis + DB + Polling</td>
<td>High</td>
<td>Fast + handles races</td>
<td>High</td>
</tr>
</tbody>
</table>
<p><strong>Production Recommendation</strong>: Use Redis as cache layer, database as source of truth. Accept 2x latency when Redis is down. Set up <a href="/databases/redis-sentinel">[Redis Sentinel]</a> or Cluster for HA.</p>
</div>
</details>
</div>
</div>
<div style="background: #f8fafc;border-radius: 16px; padding: 32px; margin: 24px 0">
<h3 style="color: #1e40af; margin: 0 0 24px 0">Distributed Scheduling Deep Dive</h3>
<div style="background: #f8fafc; padding: 20px; border-radius: 12px; margin-bottom: 20px">
<div style="color: #166534; font-weight: bold; font-size: 14px; margin-bottom: 12px">Level 1: "How do you scale a task scheduler horizontally?"</div>
<details>
<summary style="color: #5b21b6; cursor: pointer; font-size: 13px">Comprehensive Answer</summary>
<div style="margin-top: 16px; padding: 16px; background: #f0fdf4; border-radius: 8px">
<p><strong>Approach 1: Leader-Based (Active-Passive)</strong></p>
<p>One node is the scheduler (leader), others are workers only:</p>
<div style="display: flex; flex-wrap: wrap; gap: 12px; align-items: center; justify-content: center; background: #f8fafc; padding: 16px; border-radius: 8px; margin: 12px 0">
<div style="background: #eff6ff; padding: 12px 16px; border-radius: 8px;text-align: center">
<div style="color: #1e40af; font-weight: bold; font-size: 12px">Leader</div>
<div style="color: #64748b; font-size: 10px">schedules tasks</div>
</div>
<div style="color: #3b82f6; font-size: 16px">&#8594;</div>
<div style="background: #f0fdf4; padding: 12px 16px; border-radius: 8px;text-align: center">
<div style="color: #166534; font-weight: bold; font-size: 12px">Task Queue</div>
</div>
<div style="color: #22c55e; font-size: 16px">&#8592;</div>
<div style="display: flex; flex-direction: column; gap: 4px">
<div style="background: #fff7ed; padding: 8px 12px; border-radius: 6px;text-align: center">
<span style="color: #c2410c; font-size: 11px">Worker 1</span>
</div>
<div style="background: #fff7ed; padding: 8px 12px; border-radius: 6px;text-align: center">
<span style="color: #c2410c; font-size: 11px">Worker 2</span>
</div>
<div style="background: #fff7ed; padding: 8px 12px; border-radius: 6px;text-align: center">
<span style="color: #c2410c; font-size: 11px">Worker 3</span>
</div>
</div>
</div>
<div style="text-align: center; color: #64748b; font-size: 11px; margin-top: 8px">Leader heartbeat/election via ZooKeeper/etcd</div>
<pre><code>- Leader handles all scheduling decisions
- Workers pull tasks from shared queue
- On leader failure, election promotes new leader
- **Good for**: &lt; 100K tasks, simple coordination needs
</code></pre>
<p><strong>Approach 2: Partition-Based (Active-Active)</strong></p>
<p>Tasks are sharded across scheduler nodes:</p>
<pre><code class="language-python">def get_scheduler_for_task(task_id, num_schedulers):
# Consistent hashing
hash_val = hash(task_id) % 360
node = find_node_for_hash(hash_val)  # Virtual nodes
return node

# Each scheduler owns a hash range
Scheduler-1: [0, 120)    -&gt; Tasks hashing to this range
Scheduler-2: [120, 240)  -&gt; Tasks hashing to this range
Scheduler-3: [240, 360)  -&gt; Tasks hashing to this range
```

- Each node schedules its partition independently
- [[Consistent Hashing]](/algorithms/consistent-hashing) minimizes rebalancing
- **Good for**: &gt; 100K tasks, high throughput needs
</code></pre>
<p><strong>Approach 3: Database-Centric (Shared Nothing)</strong></p>
<p>No in-memory scheduling, all coordination through database:</p>
<pre><code class="language-sql">-- Each worker polls independently
SELECT id FROM tasks
WHERE status = 'pending'
AND scheduled_time &lt;= NOW()
AND (claimed_by IS NULL OR claimed_at &lt; NOW() - INTERVAL '5 minutes')
ORDER BY priority DESC, scheduled_time
LIMIT 10
FOR UPDATE SKIP LOCKED;
```

- No leader election needed
- Database handles consistency
- **Good for**: Durability-critical, moderate throughput
</code></pre>
</div>
</details>
</div>
<div style="background: #f8fafc; padding: 20px; border-radius: 12px; margin-bottom: 20px; margin-left: 24px">
<div style="color: #1e40af; font-weight: bold; font-size: 14px; margin-bottom: 12px">Level 2: "How do you handle scheduler node failures without losing tasks?"</div>
<details>
<summary style="color: #5b21b6; cursor: pointer; font-size: 13px">Comprehensive Answer</summary>
<div style="margin-top: 16px; padding: 16px; background: #f0fdf4; border-radius: 8px">
<p><strong>Principle</strong>: <span style="color: #166534">Separate task ownership from task execution</span></p>
<p><strong>Solution Architecture</strong>:</p>
<div style="display: flex; flex-wrap: wrap; gap: 12px; flex-direction: column; align-items: center; background: #f8fafc; padding: 16px; border-radius: 8px; margin: 12px 0">
<div style="background: #eff6ff; padding: 16px 24px; border-radius: 8px;text-align: center; width: 100%; max-width: 400px">
<div style="color: #1e40af; font-weight: bold; font-size: 14px">Durable Task Store</div>
<div style="color: #64748b; font-size: 11px; margin-top: 4px">PostgreSQL, MySQL, or distributed KV store</div>
<div style="color: #1e293b; font-size: 10px; margin-top: 8px; font-family: monospace">tasks: id, payload, status, owner_node, heartbeat_time</div>
</div>
<div style="display: flex; justify-content: center; gap: 8px">
<span style="color: #3b82f6">&#8593;</span>
<span style="color: #3b82f6">&#8593;</span>
<span style="color: #3b82f6">&#8593;</span>
</div>
  [Scheduler-1]       [Scheduler-2]       [Scheduler-3]
  (owns tasks       (owns tasks          (owns tasks
  0-999)            1000-1999)           2000-2999)
                                                                                                                                                                                        ```
<pre><code>**Heartbeat-Based Ownership**:

```python
</code></pre>
<p>class DistributedScheduler:<br />
def <strong>init</strong>(self, node_id, task_store):<br />
self.node_id = node_id<br />
self.store = task_store<br />
self.heartbeat_interval = 10  # seconds<br />
self.failure_threshold = 30   # seconds</p>
<p>def ownership_loop(self):<br />
while running:</p>
<h1 id="update-heartbeat-for-all-tasks-i-own">Update heartbeat for all tasks I own</h1>
<p>self.store.execute(&quot;&quot;&quot;<br />
UPDATE tasks<br />
SET heartbeat_time = NOW()<br />
WHERE owner_node = %s AND status = 'running'<br />
&quot;&quot;&quot;, self.node_id)</p>
<p>time.sleep(self.heartbeat_interval)</p>
<p>def recovery_loop(self):<br />
&quot;&quot;&quot;Run on every node - detects and recovers orphaned tasks.&quot;&quot;&quot;<br />
while running:</p>
<h1 id="find-tasks-with-stale-heartbeats">Find tasks with stale heartbeats</h1>
<p>orphaned = self.store.query(&quot;&quot;&quot;<br />
SELECT id FROM tasks<br />
WHERE status = 'running'<br />
AND heartbeat_time &lt; NOW() - INTERVAL '%s seconds'<br />
&quot;&quot;&quot;, self.failure_threshold)</p>
<p>for task_id in orphaned:<br />
self.attempt_recovery(task_id)</p>
<p>time.sleep(self.heartbeat_interval)</p>
<p>def attempt_recovery(self, task_id):</p>
<h1 id="atomic-claim---only-one-node-succeeds">Atomic claim - only one node succeeds</h1>
<p>claimed = self.store.execute(&quot;&quot;&quot;<br />
UPDATE tasks<br />
SET owner_node = %s,<br />
heartbeat_time = NOW(),<br />
status = 'pending',<br />
retry_count = retry_count + 1<br />
WHERE id = %s<br />
AND heartbeat_time &lt; NOW() - INTERVAL '%s seconds'<br />
&quot;&quot;&quot;, self.node_id, task_id, self.failure_threshold)</p>
<p>if claimed:<br />
log.info(f&quot;Recovered orphaned task {task_id}&quot;)<br />
```</p>
<pre><code>**Partition Rebalancing on Node Join/Leave**:

```python
</code></pre>
<p>def rebalance_partitions(current_nodes, new_nodes):<br />
&quot;&quot;&quot;Use consistent hashing to minimize movement.&quot;&quot;&quot;<br />
old_ring = ConsistentHashRing(current_nodes)<br />
new_ring = ConsistentHashRing(new_nodes)</p>
<p>migrations = []<br />
for task_id in all_task_ids():<br />
old_owner = old_ring.get_node(task_id)<br />
new_owner = new_ring.get_node(task_id)</p>
<p>if old_owner != new_owner:<br />
migrations.append((task_id, old_owner, new_owner))</p>
<h1 id="with-100-virtual-nodes-per-physical-node">With 100 virtual nodes per physical node,</h1>
<h1 id="only-1n-tasks-move-when-node-joinsleaves">only ~1/N tasks move when node joins/leaves</h1>
<p>return migrations<br />
```</p>
<pre><code>**Key Insight**: Never trust in-memory state for task ownership. Always persist to durable store and use heartbeats to detect failures.
</code></pre>
</div>
                                                                                                                                                                                    </details>
</div>
<div style="background: #f8fafc; padding: 20px; border-radius: 12px; margin-bottom: 20px; margin-left: 48px">
<div style="color: #5b21b6; font-weight: bold; font-size: 14px; margin-bottom: 12px">Level 3: "How do you prevent thundering herd when a failed node's tasks are redistributed?"</div>
<pre><code>&lt;details&gt;
  &lt;summary style=&quot;color: #5b21b6; cursor: pointer; font-size: 13px;&quot;&gt;Comprehensive Answer&lt;/summary&gt;
</code></pre>
<div style="margin-top: 16px; padding: 16px; background: #f0fdf4; border-radius: 8px">
<pre><code>**The Problem**: Node-3 dies. It owned 1000 tasks. All surviving nodes detect this simultaneously and try to claim all 1000 tasks at once:
- Database gets hammered with UPDATE queries
- Lock contention spikes
- Legitimate work gets starved

**Solution 1: Staggered Recovery with Jitter**

```python
</code></pre>
<p>def recovery_loop(self):<br />
while running:</p>
<h1 id="random-jitter-prevents-synchronized-recovery-attempts">Random jitter prevents synchronized recovery attempts</h1>
<p>jitter = random.uniform(0, self.heartbeat_interval / 2)<br />
time.sleep(self.heartbeat_interval + jitter)</p>
<h1 id="limit-batch-size-per-recovery-cycle">Limit batch size per recovery cycle</h1>
<p>orphaned = self.store.query(&quot;&quot;&quot;<br />
SELECT id FROM tasks<br />
WHERE status = 'running'<br />
AND heartbeat_time &lt; NOW() - INTERVAL '%s seconds'<br />
LIMIT 10  -- Don't grab everything at once<br />
&quot;&quot;&quot;, self.failure_threshold)</p>
<p>for task_id in orphaned:</p>
<h1 id="additional-per-task-jitter">Additional per-task jitter</h1>
<p>time.sleep(random.uniform(0.1, 0.5))<br />
self.attempt_recovery(task_id)<br />
```</p>
<pre><code>**Solution 2: Lease-Based Batch Assignment**

```python
</code></pre>
<p>def claim_orphan_batch(self):<br />
&quot;&quot;&quot;Claim a batch of orphaned tasks atomically.&quot;&quot;&quot;</p>
<h1 id="single-query-claims-up-to-n-tasks-for-this-node">Single query claims up to N tasks for this node</h1>
<h1 id="no-thundering-herd---each-node-claims-different-tasks">No thundering herd - each node claims different tasks</h1>
<p>claimed = self.store.execute(&quot;&quot;&quot;<br />
WITH claimable AS (<br />
SELECT id FROM tasks<br />
WHERE status = 'running'<br />
AND heartbeat_time &lt; NOW() - INTERVAL '30 seconds'<br />
ORDER BY id  -- Deterministic ordering<br />
LIMIT 50<br />
FOR UPDATE SKIP LOCKED  -- Don't block other nodes<br />
)<br />
UPDATE tasks<br />
SET owner_node = %s,<br />
heartbeat_time = NOW(),<br />
status = 'pending'<br />
WHERE id IN (SELECT id FROM claimable)<br />
RETURNING id<br />
&quot;&quot;&quot;, self.node_id)</p>
<p>return claimed<br />
```</p>
<pre><code>**Solution 3: Coordinator-Based Redistribution**

```python
</code></pre>
<p>class ClusterCoordinator:<br />
&quot;&quot;&quot;Elected leader handles all redistribution.&quot;&quot;&quot;</p>
<p>def on_node_failure(self, failed_node):</p>
<h1 id="only-coordinator-runs-this-no-thundering-herd">Only coordinator runs this, no thundering herd</h1>
<p>orphaned_tasks = self.get_tasks_owned_by(failed_node)</p>
<p>surviving_nodes = self.get_healthy_nodes()</p>
<h1 id="evenly-distribute-across-survivors">Evenly distribute across survivors</h1>
<p>assignments = self.compute_assignment(orphaned_tasks, surviving_nodes)</p>
<p>for node, tasks in assignments.items():</p>
<h1 id="batch-update-per-node">Batch update per node</h1>
<p>self.store.execute(&quot;&quot;&quot;<br />
UPDATE tasks<br />
SET owner_node = %s, status = 'pending'<br />
WHERE id = ANY(%s)<br />
&quot;&quot;&quot;, node, tasks)</p>
<h1 id="notify-node-about-new-tasks">Notify node about new tasks</h1>
<p>self.notify_node(node, tasks)<br />
```</p>
<pre><code>**Solution 4: Pull-Based with Rate Limiting**

```python
</code></pre>
<p>class RateLimitedRecovery:<br />
def <strong>init</strong>(self, max_claims_per_second=10):<br />
self.limiter = TokenBucket(max_claims_per_second)</p>
<p>def recovery_loop(self):<br />
while running:</p>
<h1 id="wait-for-token-before-claiming">Wait for token before claiming</h1>
<p>self.limiter.acquire()</p>
<p>task = self.claim_one_orphan()<br />
if task:<br />
self.schedule_for_execution(task)<br />
else:<br />
time.sleep(1)  # No orphans, back off</p>
<p>def claim_one_orphan(self):<br />
&quot;&quot;&quot;Claim exactly one orphan per call.&quot;&quot;&quot;<br />
return self.store.execute(&quot;&quot;&quot;<br />
UPDATE tasks<br />
SET owner_node = %s, status = 'pending'<br />
WHERE id = (<br />
SELECT id FROM tasks<br />
WHERE status = 'running'<br />
AND heartbeat_time &lt; NOW() - INTERVAL '30 seconds'<br />
LIMIT 1<br />
FOR UPDATE SKIP LOCKED<br />
)<br />
RETURNING *<br />
&quot;&quot;&quot;, self.node_id)<br />
```</p>
<pre><code>**Comparison**:

| Approach | Complexity | Fairness | Recovery Speed | DB Load |
|----------|------------|----------|----------------|---------|
| Jittered loop | Low | Poor | Slow | Medium |
| Batch + SKIP LOCKED | Medium | Good | Fast | Low |
| Coordinator | High | Best | Medium | Lowest |
| Rate-limited pull | Medium | Fair | Controlled | Low |

**Production Recommendation**: Combine batch claiming with `SKIP LOCKED` + rate limiting. Each node claims up to 10 tasks per second, database handles contention gracefully via skip locked.
</code></pre>
</div>
                                                                                                                                                                                    </details>
</div>
</div>
<div style="background: #f8fafc;border-radius: 16px; padding: 32px; margin: 24px 0">
<h3 style="color: #1e40af; margin: 0 0 24px 0">Cron and Timing Deep Dive</h3>
<div style="background: #f8fafc; padding: 20px; border-radius: 12px; margin-bottom: 20px">
<div style="color: #166534; font-weight: bold; font-size: 14px; margin-bottom: 12px">Level 1: "How do you implement cron-style recurring tasks?"</div>
<pre><code>&lt;details&gt;
  &lt;summary style=&quot;color: #5b21b6; cursor: pointer; font-size: 13px;&quot;&gt;Comprehensive Answer&lt;/summary&gt;
</code></pre>
<div style="margin-top: 16px; padding: 16px; background: #f0fdf4; border-radius: 8px">
<pre><code>**Core Concept**: Parse cron expression into field constraints, then find next matching datetime.

```python
</code></pre>
<p>class CronSchedule:<br />
def <strong>init</strong>(self, expression: str):</p>
<h1 id="15-9-17---mon-fri">&quot;*/15 9-17 * * MON-FRI&quot;</h1>
<p>parts = expression.split()<br />
self.minute = CronField(parts[0], 0, 59)<br />
self.hour = CronField(parts[1], 0, 23)<br />
self.day = CronField(parts[2], 1, 31)<br />
self.month = CronField(parts[3], 1, 12)<br />
self.dow = CronField(parts[4], 0, 6)</p>
<p>def next_run(self, after: datetime) -&gt; datetime:<br />
&quot;&quot;&quot;Find next datetime matching all field constraints.&quot;&quot;&quot;<br />
candidate = after.replace(second=0, microsecond=0) + timedelta(minutes=1)</p>
<p>for _ in range(4 * 366 * 24 * 60):  # Max 4 years<br />
if self._matches(candidate):<br />
return candidate<br />
candidate = self._advance(candidate)</p>
<p>raise ValueError(&quot;No matching time in next 4 years&quot;)</p>
<p>def _matches(self, dt: datetime) -&gt; bool:<br />
return (<br />
self.minute.contains(dt.minute) and<br />
self.hour.contains(dt.hour) and<br />
self.day.contains(dt.day) and<br />
self.month.contains(dt.month) and<br />
self.dow.contains(dt.weekday())<br />
)<br />
```</p>
<pre><code>**Optimized Advancement** (jump to next valid value):

```python
</code></pre>
<p>def _advance(self, dt: datetime) -&gt; datetime:</p>
<h1 id="if-month-doesnt-match-jump-to-next-valid-month">If month doesn't match, jump to next valid month</h1>
<p>if not self.month.contains(dt.month):<br />
next_month = self.month.next_value(dt.month)<br />
if next_month is None:  # Wrap to next year<br />
return dt.replace(year=dt.year+1, month=self.month.min_value, day=1, hour=0, minute=0)<br />
return dt.replace(month=next_month, day=1, hour=0, minute=0)</p>
<h1 id="similarly-for-day-hour-minute">Similarly for day, hour, minute...</h1>
<h1 id="each-field-jumps-to-next-valid-value-resetting-lower-fields">Each field jumps to next valid value, resetting lower fields</h1>
<pre><code>**Integration with Scheduler**:

```python
</code></pre>
<p>def schedule_next_occurrence(task):<br />
if task.cron_expr:<br />
next_run = task.cron_expr.next_run(datetime.now())<br />
new_task = Task(<br />
id=generate_id(),<br />
cron_expr=task.cron_expr,<br />
scheduled_time=next_run,</p>
<h1 id="-copy-other-fields">... copy other fields</h1>
<p>)<br />
scheduler.enqueue(new_task)<br />
```</p>
</div>
                                                                                                                                                                                    </details>
</div>
<div style="background: #f8fafc; padding: 20px; border-radius: 12px; margin-bottom: 20px; margin-left: 24px">
<div style="color: #1e40af; font-weight: bold; font-size: 14px; margin-bottom: 12px">Level 2: "How do you handle daylight saving time transitions?"</div>
<pre><code>&lt;details&gt;
  &lt;summary style=&quot;color: #5b21b6; cursor: pointer; font-size: 13px;&quot;&gt;Comprehensive Answer&lt;/summary&gt;
</code></pre>
<div style="margin-top: 16px; padding: 16px; background: #f0fdf4; border-radius: 8px">
<pre><code>**The Problem**:
</code></pre>
<ul>
<li>
<p><span style="color: #dc2626"><strong>Spring Forward</strong></span>: 2:00 AM  3:00 AM (2:30 AM doesn't exist)</p>
</li>
<li>
<p><span style="color: #c2410c"><strong>Fall Back</strong></span>: 2:00 AM  1:00 AM (1:30 AM happens twice)</p>
<pre><code>**Scenario 1**: Task scheduled for 2:30 AM on spring-forward day

```python
</code></pre>
</li>
</ul>
<h1 id="three-options">Three options:</h1>
<p>SKIP = &quot;skip&quot;           # Don't run at all that day<br />
RUN_AFTER = &quot;run_after&quot;  # Run at 3:00 AM instead<br />
RUN_BEFORE = &quot;run_before&quot; # Run at 1:59 AM instead</p>
<p>def resolve_nonexistent_time(scheduled_time, tz, policy):<br />
try:<br />
tz.localize(scheduled_time, is_dst=None)<br />
return scheduled_time  # Time exists normally<br />
except AmbiguousTimeError:</p>
<h1 id="fall-back---time-exists-twice">Fall back - time exists twice</h1>
<p>return handle_ambiguous(scheduled_time, tz)<br />
except NonExistentTimeError:</p>
<h1 id="spring-forward---time-doesnt-exist">Spring forward - time doesn't exist</h1>
<p>if policy == SKIP:<br />
return None<br />
elif policy == RUN_AFTER:</p>
<h1 id="jump-forward-by-dst-offset-usually-1-hour">Jump forward by DST offset (usually 1 hour)</h1>
<p>return scheduled_time + timedelta(hours=1)<br />
elif policy == RUN_BEFORE:<br />
return scheduled_time - timedelta(minutes=1)<br />
```</p>
<pre><code>**Scenario 2**: Task scheduled for 1:30 AM on fall-back day

```python
</code></pre>
<p>def handle_ambiguous(scheduled_time, tz):</p>
<h1 id="is-dsttrue-first-occurrence-before-clocks-change">is_dst=True: first occurrence (before clocks change)</h1>
<h1 id="is-dstfalse-second-occurrence-after-clocks-change">is_dst=False: second occurrence (after clocks change)</h1>
<h1 id="policy-options">Policy options:</h1>
<p>FIRST = &quot;first&quot;   # Run on first 1:30 AM<br />
SECOND = &quot;second&quot; # Run on second 1:30 AM<br />
BOTH = &quot;both&quot;     # Run twice (dangerous for non-idempotent tasks!)</p>
<p>if policy == FIRST:<br />
return tz.localize(scheduled_time, is_dst=True)<br />
elif policy == SECOND:<br />
return tz.localize(scheduled_time, is_dst=False)<br />
elif policy == BOTH:<br />
return [<br />
tz.localize(scheduled_time, is_dst=True),<br />
tz.localize(scheduled_time, is_dst=False)<br />
]<br />
```</p>
<pre><code>**Best Practice**: Store and compute in UTC internally

```python
</code></pre>
<p>class TimezoneAwareCronTask:<br />
def <strong>init</strong>(self, cron_expr, user_timezone):<br />
self.cron = CronSchedule(cron_expr)<br />
self.tz = pytz.timezone(user_timezone)</p>
<p>def next_run_utc(self, after_utc):</p>
<h1 id="convert-to-users-timezone-for-cron-matching">Convert to user's timezone for cron matching</h1>
<p>after_local = after_utc.astimezone(self.tz)</p>
<h1 id="find-next-match-in-users-timezone">Find next match in user's timezone</h1>
<p>next_local = self.cron.next_run(after_local)</p>
<h1 id="handle-dst-edge-cases">Handle DST edge cases</h1>
<p>next_local = resolve_dst_issues(next_local, self.tz)</p>
<h1 id="convert-back-to-utc-for-storage">Convert back to UTC for storage</h1>
<p>return next_local.astimezone(pytz.UTC)<br />
```</p>
<pre><code>**Critical**: Never store timezone offset (`-05:00`). Store timezone name (`America/New_York`) so DST rules apply correctly.
</code></pre>
</div>
                                                                                                                                                                                    </details>
</div>
<div style="background: #f8fafc; padding: 20px; border-radius: 12px; margin-bottom: 20px; margin-left: 48px">
<div style="color: #5b21b6; font-weight: bold; font-size: 14px; margin-bottom: 12px">Level 3: "How do you efficiently query 'which cron tasks fire in the next 5 minutes' across millions of tasks?"</div>
<pre><code>&lt;details&gt;
  &lt;summary style=&quot;color: #5b21b6; cursor: pointer; font-size: 13px;&quot;&gt;Comprehensive Answer&lt;/summary&gt;
</code></pre>
<div style="margin-top: 16px; padding: 16px; background: #f0fdf4; border-radius: 8px">
<pre><code>**The Problem**: Computing `next_run()` for 1 million cron tasks on every scheduler tick is too slow.

**Solution: Pre-computed Next-Fire Index**

```python
</code></pre>
<p>class CronIndexer:<br />
def <strong>init</strong>(self):</p>
<h1 id="sorted-set-score--next-fire-timestamp-value--task-id">Sorted set: score = next_fire_timestamp, value = task_id</h1>
<p>self.next_fire_index = SortedSet()</p>
<h1 id="task-id---crontask">task_id -&gt; CronTask</h1>
<p>self.tasks = {}</p>
<p>def add_task(self, task: CronTask):<br />
self.tasks[task.id] = task<br />
next_fire = task.next_run_utc(datetime.utcnow())<br />
self.next_fire_index.add((next_fire.timestamp(), task.id))</p>
<p>def get_tasks_firing_in(self, seconds: int) -&gt; List[str]:<br />
&quot;&quot;&quot;O(log n + k) where k is number of matching tasks.&quot;&quot;&quot;<br />
now = time.time()<br />
deadline = now + seconds</p>
<p>result = []<br />
for score, task_id in self.next_fire_index.irange_key(now, deadline):<br />
result.append(task_id)</p>
<p>return result</p>
<p>def update_after_execution(self, task_id: str):<br />
&quot;&quot;&quot;Called after task runs. Compute and index next occurrence.&quot;&quot;&quot;<br />
task = self.tasks[task_id]</p>
<h1 id="remove-old-entry">Remove old entry</h1>
<p>self.next_fire_index.discard((task.last_scheduled.timestamp(), task_id))</p>
<h1 id="add-new-entry">Add new entry</h1>
<p>next_fire = task.next_run_utc(datetime.utcnow())<br />
self.next_fire_index.add((next_fire.timestamp(), task_id))<br />
```</p>
<pre><code>**Redis Implementation**:

```python
</code></pre>
<p>class RedisCronIndex:<br />
def <strong>init</strong>(self, redis):<br />
self.redis = redis<br />
self.INDEX_KEY = &quot;cron:next_fire&quot;</p>
<p>def add_task(self, task_id, next_fire_timestamp):<br />
self.redis.zadd(self.INDEX_KEY, {task_id: next_fire_timestamp})</p>
<p>def get_tasks_firing_in(self, seconds):<br />
now = time.time()<br />
deadline = now + seconds</p>
<p>return self.redis.zrangebyscore(<br />
self.INDEX_KEY,<br />
min=now,<br />
max=deadline<br />
)</p>
<p>def update_after_execution(self, task_id, new_next_fire):</p>
<h1 id="atomic-update">Atomic update</h1>
<p>self.redis.zadd(self.INDEX_KEY, {task_id: new_next_fire})<br />
```</p>
<pre><code>**Sharded Index for Scale**:

```python
</code></pre>
<p>class ShardedCronIndex:<br />
&quot;&quot;&quot;Distribute index across multiple Redis instances.&quot;&quot;&quot;</p>
<p>def <strong>init</strong>(self, redis_nodes):<br />
self.nodes = redis_nodes<br />
self.num_shards = len(redis_nodes)</p>
<p>def _get_shard(self, task_id):<br />
return self.nodes[hash(task_id) % self.num_shards]</p>
<p>def get_all_tasks_firing_in(self, seconds):<br />
&quot;&quot;&quot;Query all shards in parallel.&quot;&quot;&quot;<br />
now = time.time()<br />
deadline = now + seconds</p>
<p>with ThreadPoolExecutor(max_workers=self.num_shards) as executor:<br />
futures = [<br />
executor.submit(<br />
node.zrangebyscore,<br />
&quot;cron:next_fire&quot;,<br />
min=now,<br />
max=deadline<br />
)<br />
for node in self.nodes<br />
]</p>
<p>results = []<br />
for future in futures:<br />
results.extend(future.result())</p>
<p>return results<br />
```</p>
<pre><code>**Memory Optimization**: Only index next 24 hours

```python
</code></pre>
<p>def maintenance_loop(self):<br />
&quot;&quot;&quot;Run periodically to refresh the index.&quot;&quot;&quot;<br />
while running:</p>
<h1 id="remove-entries-older-than-now-already-fired">Remove entries older than now (already fired)</h1>
<p>self.redis.zremrangebyscore(self.INDEX_KEY, '-inf', time.time())</p>
<h1 id="for-tasks-beyond-24h-only-index-first-occurrence">For tasks beyond 24h, only index first occurrence</h1>
<p>far_future = time.time() + 86400<br />
far_tasks = self.get_tasks_beyond(far_future)</p>
<p>for task_id in far_tasks:</p>
<h1 id="recompute---may-now-fall-within-24h-window">Recompute - may now fall within 24h window</h1>
<p>next_fire = self.compute_next_fire(task_id)<br />
if next_fire &lt; far_future:<br />
self.redis.zadd(self.INDEX_KEY, {task_id: next_fire})</p>
<p>time.sleep(3600)  # Run hourly<br />
```</p>
<pre><code>**Performance Comparison**:

| Approach | Query Time (1M tasks, 5 min window) | Memory |
|----------|-------------------------------------|--------|
| Scan all + compute | O(n) = seconds | None |
| Sorted set index | O(log n + k) = milliseconds | O(n) |
| Sharded index (10 nodes) | O(log(n/10) + k/10) = sub-millisecond | O(n) |
</code></pre>
</div>
                                                                                                                                                                                    </details>
</div>
</div>
<pre><code>---

## Related Topics

- [[Priority Queues]](/data-structures/priority-queue) - Heap data structure internals
- [[Distributed Locks]](/system-design/distributed-locking) - Lock implementation patterns
- [[Rate Limiting]](/system-design/rate-limiting) - Token bucket and leaky bucket
- [[Message Queues]](/system-design/message-queues) - Pub/sub and work queues
- [[CAP Theorem]](/system-design/cap-theorem) - Distributed systems trade-offs
- [[Exponential Backoff]](/algorithms/exponential-backoff) - Retry strategy algorithms
- [[Consistent Hashing]](/algorithms/consistent-hashing) - Partition distribution
- [[Topological Sort]](/algorithms/topological-sort) - DAG dependency resolution
- [[Two Generals Problem]](/algorithms/two-generals) - Distributed consensus limits
- [[Graph Cycle Detection]](/algorithms/graph-cycle-detection) - DAG validation
- [[Redis Sentinel]](/databases/redis-sentinel) - High availability caching
- [[Dead Letter Queues]](/system-design/dead-letter-queues) - Failed message handling
- [[Saga Pattern]](/design-patterns/saga) - Distributed transaction management
- [[Outbox Pattern]](/design-patterns/outbox) - Reliable event publishing
- [[Apache Airflow]](/tools/airflow) - Production DAG scheduler
- [[Celery]](/tools/celery) - Python distributed task queue
</code></pre>
