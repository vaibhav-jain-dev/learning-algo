<h1 id="lru-cache">LRU Cache</h1>
<h2 id="overview">Overview</h2>
<p>An LRU (Least Recently Used) Cache is a bounded data structure that maintains a fixed number of entries, automatically evicting the least recently accessed item when capacity is exceeded. <span>The fundamental insight is combining a HashMap for O(1) key lookups with a Doubly Linked List for O(1) recency tracking, achieving constant-time operations for both <code>get</code> and <code>put</code>.</span></p>
<p>This is among the most frequently asked machine coding problems because it tests the candidate's ability to compose data structures, understand pointer manipulation, and reason about time-space trade-offs under constraints.</p>
<div>
<div>Interview Frequency</div>
<div>LeetCode #146. Asked at Amazon (weekly), Google, Meta, Microsoft, Netflix, Uber, and virtually every major tech company. Often the first question in machine coding rounds.</div>
</div>
<hr />
<h2 id="why-this-problem-matters">Why This Problem Matters</h2>
<h3 id="skills-tested">Skills Tested</h3>
<table>
<thead>
<tr>
<th>Dimension</th>
<th>What Interviewers Evaluate</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Data Structure Composition</strong></td>
<td>Can you combine HashMap + LinkedList synergistically?</td>
</tr>
<tr>
<td><strong>Pointer Manipulation</strong></td>
<td>Correctness of doubly linked list operations</td>
</tr>
<tr>
<td><strong>API Design</strong></td>
<td>Clear, consistent interface with proper return semantics</td>
</tr>
<tr>
<td><strong>Edge Case Awareness</strong></td>
<td>Capacity boundaries, empty states, update semantics</td>
</tr>
<tr>
<td><strong>Systems Thinking</strong></td>
<td>Thread-safety, scalability, real-world deployment</td>
</tr>
<tr>
<td><strong>Code Organization</strong></td>
<td>Helper methods, separation of concerns, readability</td>
</tr>
</tbody>
</table>
<h3 id="real-world-applications">Real-World Applications</h3>
<ul>
<li><strong>Database Buffer Pools</strong>: PostgreSQL, MySQL use LRU variants for page caching</li>
<li><strong>Operating System Page Replacement</strong>: Virtual memory management</li>
<li><strong>CDN Edge Caching</strong>: Cloudflare, Akamai cache frequently accessed content</li>
<li><strong>Web Browser Caching</strong>: Back/forward cache, resource caching</li>
<li><strong>DNS Resolution Caching</strong>: Local DNS resolvers</li>
<li><strong>CPU Cache Eviction</strong>: Hardware-level cache management approximates LRU</li>
</ul>
<hr />
<h2 id="requirements-gathering">Requirements Gathering</h2>
<p><span>Always clarify requirements before coding. Interviewers intentionally leave specifications ambiguous to test your requirement-gathering skills.</span></p>
<h3 id="questions-to-ask-the-interviewer">Questions to Ask the Interviewer</h3>
<div>
<div>
<div>
<div>Functional Requirements</div>
<ul>
<li>Which operations: get, put, delete, peek?</li>
<li>Does updating a key reset its recency?</li>
<li>Return value for missing keys: -1, null, Optional, throw?</li>
<li>Support for TTL (time-to-live) expiration?</li>
<li>Support for eviction callbacks/listeners?</li>
<li>Key/value type constraints (int only vs generic)?</li>
</ul>
</div>
<div>
<div>Non-Functional Requirements</div>
<ul>
<li>Expected capacity range (hundreds vs millions)?</li>
<li>Thread-safety for concurrent access?</li>
<li>Single machine or distributed system?</li>
<li>Read/write ratio (90/10 vs 50/50)?</li>
<li>Memory constraints?</li>
<li>Latency requirements (p99)?</li>
</ul>
</div>
</div>
</div>
<h3 id="interview-questions-requirements">Interview Questions: Requirements</h3>
<div>
<div>L1: Why is requirements gathering important for LRU Cache?</div>
<div>It determines API design, thread-safety needs, and whether simple LRU suffices or if variants (LRU-K, ARC, LIRS) are needed.</div>
<div>
<div>L2: How would TTL support change your implementation?</div>
<div>Requires storing timestamps per entry, periodic cleanup (background thread or lazy eviction on access), and potentially a separate data structure like a min-heap ordered by expiration time.</div>
<div>
<div>L3: Compare lazy vs eager TTL eviction. When would you choose each?</div>
<div>Lazy eviction (check on access) has no background overhead but may hold stale data and shows inconsistent memory usage. Eager eviction (background sweeper) maintains consistent memory but requires threading and adds latency spikes during cleanup. Choose lazy for simple cases, eager when memory predictability matters (e.g., embedded systems, strict SLAs).</div>
</div>
</div>
</div>
<hr />
<h2 id="core-architecture">Core Architecture</h2>
<h3 id="the-hashmap--doubly-linked-list-synergy">The HashMap + Doubly Linked List Synergy</h3>
<p><span>The key architectural insight: HashMap provides O(1) lookup by key, while the doubly linked list maintains access order with O(1) insertion, deletion, and reordering when we have a direct node reference.</span></p>
<div>
<h4>LRU Cache Internal Architecture</h4>
<div>
<div>
<div>HashMap: Key to Node Reference Mapping</div>
<div>
<div>
<div>"user:123"</div>
<div>ptr: 0x7f2a</div>
</div>
<div>
<div>"session:abc"</div>
<div>ptr: 0x7f3b</div>
</div>
<div>
<div>"config:db"</div>
<div>ptr: 0x7f4c</div>
</div>
</div>
<div>HashMap stores direct memory references to nodes, enabling O(1) node access without list traversal</div>
</div>
<div>
<div>Pointers reference nodes directly</div>
</div>
<div>
<div>Doubly Linked List: Recency Order (Head = Most Recent)</div>
<div>
<div>
<div>HEAD</div>
<div>sentinel</div>
</div>
<div>&#8644;</div>
<div>
<div>0x7f2a</div>
<div>"user:123"</div>
<div>MRU</div>
</div>
<div>&#8644;</div>
<div>
<div>0x7f3b</div>
<div>"session:abc"</div>
</div>
<div>&#8644;</div>
<div>
<div>0x7f4c</div>
<div>"config:db"</div>
<div>LRU</div>
</div>
<div>&#8644;</div>
<div>
<div>TAIL</div>
<div>sentinel</div>
</div>
</div>
<div>Each node has prev/next pointers; sentinel nodes eliminate null checks</div>
</div>
</div>
</div>
<h3 id="why-doubly-linked-list-not-singly">Why Doubly Linked List (Not Singly)?</h3>
<p><span>To remove a node from a singly linked list, you need its predecessor. Finding the predecessor requires O(n) traversal. With a doubly linked list, the predecessor is directly accessible via <code>node.prev</code>, enabling O(1) removal.</span></p>
<table>
<thead>
<tr>
<th>Operation</th>
<th>Singly Linked</th>
<th>Doubly Linked</th>
</tr>
</thead>
<tbody>
<tr>
<td>Remove node (given pointer)</td>
<td>O(n) - must find prev</td>
<td>O(1) - prev directly available</td>
</tr>
<tr>
<td>Insert after node</td>
<td>O(1)</td>
<td>O(1)</td>
</tr>
<tr>
<td>Insert before node</td>
<td>O(n)</td>
<td>O(1)</td>
</tr>
<tr>
<td>Move to front</td>
<td>O(n)</td>
<td>O(1)</td>
</tr>
</tbody>
</table>
<h3 id="why-sentinel-nodes">Why Sentinel Nodes?</h3>
<p>Sentinel (dummy) head and tail nodes eliminate edge cases:</p>
<pre><code class="language-python"># WITHOUT sentinels - many null checks needed
def remove(node):
    if node.prev:
        node.prev.next = node.next
    else:
        self.head = node.next  # Special case: removing head

    if node.next:
        node.next.prev = node.prev
    else:
        self.tail = node.prev  # Special case: removing tail

# WITH sentinels - clean, uniform code
def remove(node):
    node.prev.next = node.next
    node.next.prev = node.prev
    # Always works - sentinels are never null
</code></pre>
<h3 id="class-structure">Class Structure</h3>
<div>
<div>
<div>
<div>Node</div>
<div>
<div>
<div>Fields</div>
<div>
<div>- key: K</div>
<div>- value: V</div>
<div>- prev: Node</div>
<div>- next: Node</div>
</div>
</div>
</div>
</div>
<div>&#8594;</div>
<div>
<div>LRUCache</div>
<div>
<div>
<div>Fields</div>
<div>
<div>- capacity: int</div>
<div>- size: int</div>
<div>- cache: HashMap&lt;K, Node&gt;</div>
<div>- head: Node (sentinel)</div>
<div>- tail: Node (sentinel)</div>
</div>
</div>
<div>
<div>Methods</div>
<div>
<div>+ get(key): V</div>
<div>+ put(key, value): void</div>
<div>- addToFront(node): void</div>
<div>- removeNode(node): void</div>
<div>- moveToFront(node): void</div>
<div>- evictLRU(): void</div>
</div>
</div>
</div>
</div>
</div>
</div>
<h3 id="interview-questions-architecture">Interview Questions: Architecture</h3>
<div>
<div>L1: Why can't we use just a HashMap for LRU Cache?</div>
<div>HashMap provides O(1) lookup but has no inherent ordering. We need to track access recency, which requires a separate ordered structure. HashMap iteration order is either undefined or insertion-order (LinkedHashMap), not access-order.</div>
<div>
<div>L2: Could we use a LinkedHashMap (access-order mode) instead of building from scratch?</div>
<div>Yes! Java's LinkedHashMap with accessOrder=true maintains access order. Override removeEldestEntry() to enforce capacity. However, interviewers want to see you implement the mechanism manually to demonstrate understanding.</div>
<div>
<div>L3: What are the internal differences between your implementation and LinkedHashMap?</div>
<div>LinkedHashMap uses a single doubly-linked list threaded through the HashMap entries themselves (each Entry has before/after pointers), avoiding separate node allocation. It also handles iteration, serialization, and has optimized memory layout. Our implementation is cleaner conceptually but less memory-efficient. LinkedHashMap also handles null keys/values with special treatment.</div>
</div>
</div>
</div>
<hr />
<h2 id="o1-operations-deep-dive">O(1) Operations: Deep Dive</h2>
<h3 id="get-operation">GET Operation</h3>
<div>
<div>GET(key) - Retrieve value and update recency</div>
<div>
<div>
<div>1</div>
<div>
<div><strong>HashMap Lookup:</strong> cache.get(key) - O(1) average</div>
<div>Returns null/None if key doesn't exist, otherwise returns Node reference</div>
</div>
</div>
<div>
<div>2</div>
<div>
<div><strong>Not Found Path:</strong> Return -1 (or throw, based on API contract)</div>
</div>
</div>
<div>
<div>3</div>
<div>
<div><strong>Remove from Current Position:</strong> Unlink node by updating neighbor pointers - O(1)</div>
<div>node.prev.next = node.next; node.next.prev = node.prev</div>
</div>
</div>
<div>
<div>4</div>
<div>
<div><strong>Insert After Head:</strong> Add node as most recently used - O(1)</div>
<div>Update 4 pointers: node.prev, node.next, head.next.prev, head.next</div>
</div>
</div>
<div>
<div>5</div>
<div>
<div><strong>Return Value:</strong> Return node.value</div>
</div>
</div>
</div>
</div>
<h3 id="put-operation">PUT Operation</h3>
<div>
<div>PUT(key, value) - Insert or update entry</div>
<div>
<div>
<div>1</div>
<div>
<div><strong>Check Existence:</strong> Does key already exist in cache?</div>
</div>
</div>
<div>
<div>
<div>Key EXISTS (Update Path)</div>
<div>
  2a. Update node.value<br>
  2b. Move node to front (same as GET)<br>
  2c. Return
</div>
</div>
<div>
<div>Key NOT EXISTS (Insert Path)</div>
<div>
  2a. If size == capacity: evict LRU<br>
  2b. Create new Node(key, value)<br>
  2c. Add to HashMap<br>
  2d. Insert after head<br>
  2e. Increment size
</div>
</div>
</div>
<div>
<div>!</div>
<div>
<div><strong>Eviction:</strong> Remove tail.prev from list AND delete its key from HashMap</div>
<div>Common bug: forgetting to remove from HashMap causes memory leak and size mismatch</div>
</div>
</div>
</div>
</div>
<h3 id="pointer-update-sequence-critical">Pointer Update Sequence (Critical)</h3>
<p><span>The order of pointer updates matters! Updating in the wrong order can lose references.</span></p>
<pre><code>        ```python
        # CORRECT: Insert node after head
        def add_to_front(self, node):
        # First, set the new node's pointers (doesn't modify existing structure)
        node.prev = self.head
        node.next = self.head.next

        # Then, update existing nodes to point to new node
        self.head.next.prev = node  # Old first node points back to new node
        self.head.next = node       # Head points forward to new node

        # WRONG: If we do head.next = node first, we lose reference to old first node!
        def add_to_front_WRONG(self, node):
        self.head.next = node       # OOPS! Lost reference to old head.next
        node.prev = self.head
        node.next = ???             # Can't access old first node anymore!
        ```
</code></pre>
<h3 id="interview-questions-o1-operations">Interview Questions: O(1) Operations</h3>
<div>
<div>L1: Walk me through what happens internally when we call get(key) on an existing key.</div>
<div>HashMap lookup returns node pointer (O(1)). Node is unlinked from current position by making its neighbors point to each other (O(1)). Node is then inserted right after head sentinel by updating 4 pointers (O(1)). Finally, return node.value.</div>
<div>
<div>L2: Is HashMap lookup truly O(1)? When might it degrade?</div>
<div>HashMap is O(1) average case but O(n) worst case when all keys hash to the same bucket. This happens with pathological hash functions or adversarial inputs. Modern HashMaps (Java 8+) convert long chains to balanced trees (O(log n) worst case). Load factor and resizing also affect performance.</div>
<div>
<div>L3: How does HashMap resizing interact with LRU Cache performance? What's the amortized complexity?</div>
<div>HashMap resize is O(n) when triggered (rehash all entries). However, since resize doubles capacity and only triggers when load factor exceeded, each element is rehashed O(1) times amortized. For LRU Cache, resizes are rare after warmup since size is bounded by capacity. Initial puts might trigger resizes; consider initializing HashMap with expected capacity to avoid this. Amortized complexity remains O(1).</div>
</div>
</div>
</div>
<hr />
<h2 id="code-implementation">Code Implementation</h2>
<h3 id="python-implementation">Python Implementation</h3>
<pre><code>        ```python
        from typing import Optional, Dict

        class Node:
        &quot;&quot;&quot;
        Doubly linked list node storing key-value pair.

        Key is stored because we need it during eviction to remove
        from HashMap. Without the key in the node, we'd need O(n)
        HashMap traversal to find the entry to delete.
        &quot;&quot;&quot;
        __slots__ = ['key', 'value', 'prev', 'next']  # Memory optimization

        def __init__(self, key: int = 0, value: int = 0):
        self.key = key
        self.value = value
        self.prev: Optional['Node'] = None
        self.next: Optional['Node'] = None


        class LRUCache:
        &quot;&quot;&quot;
        LRU Cache with O(1) get and put operations.

        Architecture:
        - HashMap (dict): key -&gt; Node reference for O(1) lookup
        - Doubly Linked List: maintains access order (head = MRU, tail = LRU)
        - Sentinel nodes: eliminate edge case handling

        Invariants:
        - len(cache) == number of data nodes in list
        - head.next is MRU, tail.prev is LRU
        - Every node in list has corresponding cache entry and vice versa

        Time: O(1) for get, put
        Space: O(capacity) for storage + O(1) per operation
        &quot;&quot;&quot;

        def __init__(self, capacity: int):
        if capacity &lt;= 0:
        raise ValueError(&quot;Capacity must be positive&quot;)

        self.capacity = capacity
        self.cache: Dict[int, Node] = {}

        # Sentinel nodes - never store data, just simplify logic
        self.head = Node()  # Dummy head (before MRU)
        self.tail = Node()  # Dummy tail (after LRU)
        self.head.next = self.tail
        self.tail.prev = self.head

        def _add_to_front(self, node: Node) -&gt; None:
        &quot;&quot;&quot;
        Insert node immediately after head sentinel.

        Before: head &lt;-&gt; A &lt;-&gt; ... &lt;-&gt; tail
        After:  head &lt;-&gt; node &lt;-&gt; A &lt;-&gt; ... &lt;-&gt; tail

        Pointer update order matters to avoid losing references!
        &quot;&quot;&quot;
        node.prev = self.head
        node.next = self.head.next
        # Update existing nodes AFTER setting new node's pointers
        self.head.next.prev = node
        self.head.next = node

        def _remove_node(self, node: Node) -&gt; None:
        &quot;&quot;&quot;
        Remove node from its current position.

        Before: ... &lt;-&gt; A &lt;-&gt; node &lt;-&gt; B &lt;-&gt; ...
        After:  ... &lt;-&gt; A &lt;-&gt; B &lt;-&gt; ...

        Does not delete from HashMap - caller's responsibility.
        &quot;&quot;&quot;
        prev_node = node.prev
        next_node = node.next
        prev_node.next = next_node
        next_node.prev = prev_node

        def _move_to_front(self, node: Node) -&gt; None:
        &quot;&quot;&quot;Move existing node to MRU position.&quot;&quot;&quot;
        self._remove_node(node)
        self._add_to_front(node)

        def _evict_lru(self) -&gt; None:
        &quot;&quot;&quot;
        Remove least recently used entry (node before tail).

        Critical: Must remove from BOTH list AND HashMap.
        The node stores its key specifically for this operation.
        &quot;&quot;&quot;
        lru_node = self.tail.prev
        self._remove_node(lru_node)
        del self.cache[lru_node.key]  # Don't forget this!

        def get(self, key: int) -&gt; int:
        &quot;&quot;&quot;
        Retrieve value by key, marking it as recently used.

        Returns -1 if key not found (per LeetCode convention).
        Consider returning Optional[int] or raising KeyError
        for production code.
        &quot;&quot;&quot;
        if key not in self.cache:
        return -1

        node = self.cache[key]
        self._move_to_front(node)  # Mark as recently used
        return node.value

        def put(self, key: int, value: int) -&gt; None:
        &quot;&quot;&quot;
        Insert or update key-value pair.

        If key exists: update value and move to front.
        If key doesn't exist:
        - If at capacity: evict LRU first
        - Create new node and add to front
        &quot;&quot;&quot;
        if key in self.cache:
        # Update existing
        node = self.cache[key]
        node.value = value
        self._move_to_front(node)
        else:
        # Insert new
        if len(self.cache) &gt;= self.capacity:
        self._evict_lru()

        new_node = Node(key, value)
        self.cache[key] = new_node
        self._add_to_front(new_node)

        def delete(self, key: int) -&gt; bool:
        &quot;&quot;&quot;
        Explicitly remove a key (extension beyond LeetCode spec).
        Returns True if key existed and was removed.
        &quot;&quot;&quot;
        if key not in self.cache:
        return False

        node = self.cache[key]
        self._remove_node(node)
        del self.cache[key]
        return True

        def peek(self, key: int) -&gt; int:
        &quot;&quot;&quot;
        Get value WITHOUT updating recency (extension).
        Useful for debugging or special access patterns.
        &quot;&quot;&quot;
        if key not in self.cache:
        return -1
        return self.cache[key].value

        def __len__(self) -&gt; int:
        return len(self.cache)

        def __contains__(self, key: int) -&gt; bool:
        return key in self.cache

        def __repr__(self) -&gt; str:
        &quot;&quot;&quot;Debug representation showing order.&quot;&quot;&quot;
        items = []
        curr = self.head.next
        while curr != self.tail:
        items.append(f&quot;{curr.key}:{curr.value}&quot;)
        curr = curr.next
        return f&quot;LRUCache({len(self)}/{self.capacity})[{' -&gt; '.join(items)}]&quot;


        # Verification
        if __name__ == &quot;__main__&quot;:
        cache = LRUCache(2)

        cache.put(1, 1)
        cache.put(2, 2)
        print(f&quot;Initial: {cache}&quot;)

        assert cache.get(1) == 1  # Returns 1, moves 1 to front
        print(f&quot;After get(1): {cache}&quot;)

        cache.put(3, 3)  # Evicts key 2 (LRU)
        print(f&quot;After put(3,3): {cache}&quot;)

        assert cache.get(2) == -1  # Returns -1 (not found)

        cache.put(4, 4)  # Evicts key 1
        print(f&quot;After put(4,4): {cache}&quot;)

        assert cache.get(1) == -1
        assert cache.get(3) == 3
        assert cache.get(4) == 4

        print(&quot;All assertions passed!&quot;)
        ```
</code></pre>
<h3 id="java-implementation-thread-safe-version">Java Implementation (Thread-Safe Version)</h3>
<pre><code>        ```java
        import java.util.HashMap;
        import java.util.Map;
        import java.util.concurrent.locks.ReentrantReadWriteLock;

        /**
        * Thread-safe LRU Cache implementation.
        *
        * Uses ReentrantReadWriteLock for concurrent access:
        * - Multiple readers can access simultaneously
        * - Writers have exclusive access
        *
        * Note: get() requires write lock because it modifies recency order.
        * For read-heavy workloads, consider lock-free alternatives.
        */
        public class LRUCache&lt;K, V&gt; {

          private static class Node&lt;K, V&gt; {
            K key;
            V value;
            Node&lt;K, V&gt; prev, next;

              Node(K key, V value) {
              this.key = key;
              this.value = value;
              }

              Node() {} // Sentinel constructor
              }

              private final int capacity;
              private final Map&lt;K, Node&lt;K, V&gt;&gt; cache;
                private final Node&lt;K, V&gt; head, tail;
                  private final ReentrantReadWriteLock lock;

                  public LRUCache(int capacity) {
                  if (capacity &lt;= 0) {
                  throw new IllegalArgumentException(&quot;Capacity must be positive&quot;);
                  }

                  this.capacity = capacity;
                  this.cache = new HashMap&lt;&gt;(capacity, 1.0f); // Load factor 1.0, pre-sized
                  this.lock = new ReentrantReadWriteLock();

                  // Initialize sentinel nodes
                  this.head = new Node&lt;&gt;();
                  this.tail = new Node&lt;&gt;();
                  head.next = tail;
                  tail.prev = head;
                  }

                  private void addToFront(Node&lt;K, V&gt; node) {
                    node.prev = head;
                    node.next = head.next;
                    head.next.prev = node;
                    head.next = node;
                    }

                    private void removeNode(Node&lt;K, V&gt; node) {
                      node.prev.next = node.next;
                      node.next.prev = node.prev;
                      }

                      private void moveToFront(Node&lt;K, V&gt; node) {
                        removeNode(node);
                        addToFront(node);
                        }

                        private void evictLRU() {
                        Node&lt;K, V&gt; lru = tail.prev;
                          removeNode(lru);
                          cache.remove(lru.key);
                          }

                          /**
                          * Get value by key. Returns null if not found.
                          *
                          * Requires write lock because it modifies access order.
                          */
                          public V get(K key) {
                          lock.writeLock().lock();
                          try {
                          Node&lt;K, V&gt; node = cache.get(key);
                            if (node == null) {
                            return null;
                            }
                            moveToFront(node);
                            return node.value;
                            } finally {
                            lock.writeLock().unlock();
                            }
                            }

                            /**
                            * Peek at value without updating recency.
                            *
                            * Can use read lock since it doesn't modify structure.
                            */
                            public V peek(K key) {
                            lock.readLock().lock();
                            try {
                            Node&lt;K, V&gt; node = cache.get(key);
                              return node != null ? node.value : null;
                              } finally {
                              lock.readLock().unlock();
                              }
                              }

                              /**
                              * Insert or update key-value pair.
                              */
                              public void put(K key, V value) {
                              lock.writeLock().lock();
                              try {
                              Node&lt;K, V&gt; node = cache.get(key);

                                if (node != null) {
                                // Update existing
                                node.value = value;
                                moveToFront(node);
                                } else {
                                // Insert new
                                if (cache.size() &gt;= capacity) {
                                evictLRU();
                                }

                                Node&lt;K, V&gt; newNode = new Node&lt;&gt;(key, value);
                                  cache.put(key, newNode);
                                  addToFront(newNode);
                                  }
                                  } finally {
                                  lock.writeLock().unlock();
                                  }
                                  }

                                  public int size() {
                                  lock.readLock().lock();
                                  try {
                                  return cache.size();
                                  } finally {
                                  lock.readLock().unlock();
                                  }
                                  }

                                  public boolean containsKey(K key) {
                                  lock.readLock().lock();
                                  try {
                                  return cache.containsKey(key);
                                  } finally {
                                  lock.readLock().unlock();
                                  }
                                  }
                                  }
                                  ```
</code></pre>
<h3 id="go-implementation">Go Implementation</h3>
<pre><code>                                  ```go
                                  package lru

                                  import (
                                  &quot;container/list&quot;
                                  &quot;sync&quot;
                                  )

                                  // entry stores key-value pair in the list
                                  // Key is needed to delete from map during eviction
                                  type entry[K comparable, V any] struct {
                                  key   K
                                  value V
                                  }

                                  // Cache is a thread-safe LRU cache with O(1) operations
                                  type Cache[K comparable, V any] struct {
                                  capacity int
                                  cache    map[K]*list.Element
                                  list     *list.List // Front = MRU, Back = LRU
                                  mu       sync.RWMutex
                                  zero     V // Zero value for type V
                                  }

                                  // New creates a new LRU Cache with given capacity
                                  func New[K comparable, V any](capacity int) *Cache[K, V] {
                                  if capacity &lt;= 0 {
                                  panic(&quot;capacity must be positive&quot;)
                                  }
                                  return &amp;Cache[K, V]{
                                  capacity: capacity,
                                  cache:    make(map[K]*list.Element, capacity),
                                  list:     list.New(),
                                  }
                                  }

                                  // Get retrieves value by key, returns (value, true) or (zero, false)
                                  func (c *Cache[K, V]) Get(key K) (V, bool) {
                                  c.mu.Lock()
                                  defer c.mu.Unlock()

                                  elem, ok := c.cache[key]
                                  if !ok {
                                  return c.zero, false
                                  }

                                  // Move to front (MRU position)
                                  c.list.MoveToFront(elem)
                                  return elem.Value.(*entry[K, V]).value, true
                                  }

                                  // Put inserts or updates key-value pair
                                  func (c *Cache[K, V]) Put(key K, value V) {
                                  c.mu.Lock()
                                  defer c.mu.Unlock()

                                  if elem, ok := c.cache[key]; ok {
                                  // Update existing
                                  elem.Value.(*entry[K, V]).value = value
                                  c.list.MoveToFront(elem)
                                  return
                                  }

                                  // Evict if at capacity
                                  if c.list.Len() &gt;= c.capacity {
                                  c.evictLRU()
                                  }

                                  // Insert new
                                  ent := &amp;entry[K, V]{key: key, value: value}
                                  elem := c.list.PushFront(ent)
                                  c.cache[key] = elem
                                  }

                                  // evictLRU removes least recently used entry (must hold lock)
                                  func (c *Cache[K, V]) evictLRU() {
                                  back := c.list.Back()
                                  if back == nil {
                                  return
                                  }

                                  ent := back.Value.(*entry[K, V])
                                  delete(c.cache, ent.key)
                                  c.list.Remove(back)
                                  }

                                  // Peek gets value without updating recency
                                  func (c *Cache[K, V]) Peek(key K) (V, bool) {
                                  c.mu.RLock()
                                  defer c.mu.RUnlock()

                                  elem, ok := c.cache[key]
                                  if !ok {
                                  return c.zero, false
                                  }
                                  return elem.Value.(*entry[K, V]).value, true
                                  }

                                  // Len returns current number of entries
                                  func (c *Cache[K, V]) Len() int {
                                  c.mu.RLock()
                                  defer c.mu.RUnlock()
                                  return c.list.Len()
                                  }

                                  // Delete removes a key from the cache
                                  func (c *Cache[K, V]) Delete(key K) bool {
                                  c.mu.Lock()
                                  defer c.mu.Unlock()

                                  elem, ok := c.cache[key]
                                  if !ok {
                                  return false
                                  }

                                  delete(c.cache, key)
                                  c.list.Remove(elem)
                                  return true
                                  }
                                  ```
</code></pre>
<hr />
<h2 id="thread-safety-deep-dive">Thread Safety Deep Dive</h2>
<h3 id="the-challenge">The Challenge</h3>
<p><span>LRU Cache operations appear to be simple reads (get) and writes (put), but get actually MUTATES the data structure by updating access order. This makes thread-safe implementation non-trivial.</span></p>
<div>
<div>Race Condition Example</div>
<div>
<div>
<div>
<div>Thread A: get(key1)</div>
<div>
  1. Read node from map<br>
  2. Remove node from list<br>
<span>--CONTEXT SWITCH--</span><br>
  3. Add node to front
</div>
</div>
</div>
<div>
<div>
<div>Thread B: put(key2)</div>
<div>
<span>--RUNS DURING SWITCH--</span><br>
  1. Check capacity (full)<br>
  2. Evict tail.prev<br>
<span>BUG: May evict wrong node!</span>
</div>
</div>
</div>
</div>
<div>
<strong>Result:</strong> List structure is corrupted because Thread A left the node in a partially removed state.
</div>
</div>
<h3 id="synchronization-strategies">Synchronization Strategies</h3>
<div>
<div>
<div>
<div>1. Global Mutex (Simple)</div>
<div>
<div>Single lock protects all operations</div>
<div>
<div>+ Simple to implement correctly</div>
<div>+ No deadlock risk</div>
<div>- No concurrent reads</div>
<div>- Bottleneck under high load</div>
</div>
</div>
</div>
<div>
<div>2. Read-Write Lock</div>
<div>
<div>Readers share, writers exclusive</div>
<div>
<div>+ Concurrent reads for peek()</div>
<div>- get() needs write lock (updates order)</div>
<div>- Limited benefit for LRU</div>
<div>See: Java ReentrantReadWriteLock</div>
</div>
</div>
</div>
<div>
<div>3. Segmented/Striped Locks</div>
<div>
<div>Partition cache into segments, each with own lock</div>
<div>
<div>+ Parallel access to different segments</div>
<div>+ Used by ConcurrentHashMap</div>
<div>- Per-segment LRU, not global</div>
<div>- Complex implementation</div>
</div>
</div>
</div>
<div>
<div>4. Lock-Free (Advanced)</div>
<div>
<div>CAS operations, concurrent data structures</div>
<div>
<div>+ Maximum throughput</div>
<div>+ No blocking</div>
<div>- Extremely complex</div>
<div>- Often approximate LRU</div>
</div>
</div>
</div>
</div>
</div>
<h3 id="production-approaches">Production Approaches</h3>
<p><strong>Caffeine (Java)</strong> uses a sophisticated approach:<br />
- Window TinyLFU admission policy (better than pure LRU)<br />
- Concurrent hash table with compare-and-swap<br />
- Buffer writes to avoid contention<br />
- Background thread processes access order updates</p>
<p><strong>Guava Cache</strong> uses:<br />
- Segmented design (like ConcurrentHashMap)<br />
- Each segment has its own LRU order<br />
- Global LRU is approximated, not exact</p>
<p><span>In interviews, implement simple global lock first, then discuss trade-offs. Mentioning Caffeine/Guava shows production awareness.</span></p>
<h3 id="interview-questions-thread-safety">Interview Questions: Thread Safety</h3>
<div>
<div>L1: Why does get() need a write lock in thread-safe LRU Cache?</div>
<div>Because get() modifies the data structure by moving the accessed node to the front of the list. This is a write operation on the linked list, even though it appears to be a "read" from the caller's perspective.</div>
<div>
<div>L2: How would you optimize for a read-heavy workload?</div>
<div>Options: (1) Add peek() that doesn't update order (can use read lock). (2) Buffer access events and batch-update order periodically. (3) Use approximate LRU with probabilistic promotion (not every read moves to front). (4) Segmented cache where each segment has its own LRU. (5) Consider CLOCK algorithm which approximates LRU with less contention.</div>
<div>
<div>L3: Explain the buffered writes approach used by Caffeine. What are the trade-offs?</div>
<div>Caffeine uses read/write buffers backed by concurrent queues. Access events are appended to buffer (fast, non-blocking). A maintenance thread periodically drains buffers and updates the eviction policy. Trade-offs: (1) Access order is eventually consistent, not immediately reflected. (2) Memory overhead for buffers. (3) Under extreme write bursts, buffers may overflow (Caffeine drops oldest events). (4) Eviction decisions based on slightly stale information. This is acceptable because perfect LRU is often overkill - near-LRU with high throughput is better in practice.</div>
</div>
</div>
</div>
<hr />
<h2 id="cache-eviction-policies-comparison">Cache Eviction Policies Comparison</h2>
<p><span>LRU is just one eviction policy. Understanding alternatives shows depth and helps choose the right tool for specific workloads.</span></p>
<div>
<table>
  <thead>
<tr>
<th>Policy</th>
<th>Evicts</th>
<th>Pros</th>
<th>Cons</th>
<th>Use Case</th>
</tr>
  </thead>
  <tbody>
<tr>
<td>LRU</td>
<td>Least recently accessed</td>
<td>Simple, O(1), good locality</td>
<td>Scan pollution, ignores frequency</td>
<td>General purpose, web caching</td>
</tr>
<tr>
<td>LFU</td>
<td>Least frequently accessed</td>
<td>Keeps popular items</td>
<td>Old popular items never evicted, O(log n) naive</td>
<td>CDN, long-running caches</td>
</tr>
<tr>
<td>FIFO</td>
<td>First inserted</td>
<td>Simplest, no access tracking</td>
<td>Ignores actual usage</td>
<td>Write buffers, simple queues</td>
</tr>
<tr>
<td>CLOCK</td>
<td>Approximates LRU</td>
<td>Less metadata than LRU, fast</td>
<td>Only approximation</td>
<td>OS page replacement</td>
</tr>
<tr>
<td>ARC</td>
<td>Adaptive (recency + frequency)</td>
<td>Self-tuning, scan-resistant</td>
<td>Complex, IBM patent (expired)</td>
<td>ZFS, databases</td>
</tr>
<tr>
<td>TinyLFU</td>
<td>Combines LRU window + LFU main</td>
<td>Best hit rates, scan-resistant</td>
<td>Complex, approximate</td>
<td>Caffeine, modern caches</td>
</tr>
  </tbody>
</table>
</div>
<h3 id="lru-scan-pollution-problem">LRU Scan Pollution Problem</h3>
<div>
<div>Scan Pollution Example</div>
<div>
  Consider a database with a hot set (frequently accessed) and a full table scan:
</div>
<div>
<div>
<div>Before Scan</div>
<div>Cache: [A, B, C, D, E] (all hot, frequently accessed)</div>
</div>
<div>
<div>Full Table Scan Reads: X, Y, Z, W, V (each accessed once)</div>
<div>Cache: [X, Y, Z, W, V] (all cold, never accessed again!)</div>
</div>
<div>
<div>Result</div>
<div>Hot data evicted by one-time scan. Cache becomes useless until hot data reloaded.</div>
</div>
</div>
</div>
<h3 id="lfu-implementation-insight">LFU Implementation Insight</h3>
<p>See <a href="/topic/machine-coding/lfu-cache">[LFU Cache]</a> for detailed implementation. Key difference: LFU uses frequency counts and requires O(1) access to minimum frequency bucket.</p>
<h3 id="interview-questions-eviction-policies">Interview Questions: Eviction Policies</h3>
<div>
<div>L1: When would LFU be better than LRU?</div>
<div>When access patterns have clear frequency differences. Example: A popular API endpoint called 1000x/sec should stay cached even if a batch job scans through rarely-accessed data. LRU would evict the popular endpoint; LFU keeps it.</div>
<div>
<div>L2: What's the main problem with pure LFU?</div>
<div>Cache pollution from historical frequency. Items popular in the past but no longer relevant (e.g., yesterday's trending topic) can't be evicted because they accumulated high frequency counts. Solutions: decay frequency over time, use windowed LFU, or hybrid approaches like TinyLFU.</div>
<div>
<div>L3: Explain how TinyLFU solves both LRU's scan pollution and LFU's history problem.</div>
<div>TinyLFU uses a small LRU "window" (typically 1% of cache) as an admission filter. New items enter the window first. To enter the main cache, an item must "win" against a potential eviction victim by having higher estimated frequency. Frequency is tracked using a Count-Min Sketch (probabilistic, constant space) that's periodically halved (aging). The main cache uses Segmented LRU (protected + probationary segments). This combines recency (window), frequency (admission filter), and aging (sketch decay), achieving near-optimal hit rates while resisting both scan pollution and stale frequency.</div>
</div>
</div>
</div>
<hr />
<h2 id="distributed-caching">Distributed Caching</h2>
<p><span>Scaling LRU Cache beyond a single machine introduces fundamental distributed systems challenges: consistency, partition tolerance, and coordination overhead.</span></p>
<h3 id="architecture-patterns">Architecture Patterns</h3>
<div>
<div>
<div>
<div>Replicated Cache</div>
<div>
<div>
<div>
<div>Node A<br><span>Full Copy</span></div>
<div>Node B<br><span>Full Copy</span></div>
<div>Node C<br><span>Full Copy</span></div>
</div>
</div>
<div>
<div>+ Fast reads (local)</div>
<div>+ High availability</div>
<div>- Write amplification</div>
<div>- Limited by smallest node</div>
</div>
</div>
</div>
<div>
<div>Partitioned/Sharded Cache</div>
<div>
<div>
<div>
<div>Node A<br><span>Keys 0-33%</span></div>
<div>Node B<br><span>Keys 34-66%</span></div>
<div>Node C<br><span>Keys 67-100%</span></div>
</div>
</div>
<div>
<div>+ Scales capacity linearly</div>
<div>+ Per-shard LRU works well</div>
<div>- Network hop for remote keys</div>
<div>- Rebalancing complexity</div>
</div>
</div>
</div>
<div>
<div>Tiered Cache (L1 + L2)</div>
<div>
<div>
<div>
<div>L1: Local (per-instance, small)</div>
<div>miss</div>
<div>L2: Distributed (shared, large)</div>
</div>
</div>
<div>
<div>+ Best latency for hot data</div>
<div>+ Reduces L2 load</div>
<div>- Consistency between tiers</div>
<div>- Cache invalidation complexity</div>
</div>
</div>
</div>
</div>
</div>
<h3 id="consistent-hashing-for-sharding">Consistent Hashing for Sharding</h3>
<p><span>Consistent hashing minimizes key redistribution when nodes are added/removed. Instead of rehashing all keys, only keys in the affected range move.</span></p>
<p>See <a href="/topic/system-design/consistent-hashing">[Consistent Hashing]</a> for detailed explanation.</p>
<p>Key insight for LRU: Each shard maintains its own LRU order independently. There's no global LRU across the cluster - this is acceptable because:</p>
<ol>
<li>Global LRU would require cross-node coordination (slow)</li>
<li>Per-shard LRU approximates global LRU well enough</li>
<li>Hot keys naturally stay cached on their designated shards</li>
</ol>
<h3 id="cache-invalidation">Cache Invalidation</h3>
<div>
<div>"There are only two hard things in Computer Science: cache invalidation and naming things." - Phil Karlton</div>
</div>
<p><strong>Invalidation Strategies:</strong></p>
<table>
<thead>
<tr>
<th>Strategy</th>
<th>Mechanism</th>
<th>Trade-off</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>TTL (Time-to-Live)</strong></td>
<td>Entries expire after fixed duration</td>
<td>Simple but may serve stale data</td>
</tr>
<tr>
<td><strong>Write-Through</strong></td>
<td>Update cache on every write</td>
<td>Consistent but slower writes</td>
</tr>
<tr>
<td><strong>Write-Behind</strong></td>
<td>Async cache update after write</td>
<td>Fast writes, brief inconsistency</td>
</tr>
<tr>
<td><strong>Pub/Sub Invalidation</strong></td>
<td>Broadcast invalidation messages</td>
<td>Consistent, requires messaging infra</td>
</tr>
<tr>
<td><strong>Version/ETag</strong></td>
<td>Check version on read</td>
<td>Reduces bandwidth, adds latency</td>
</tr>
</tbody>
</table>
<h3 id="real-world-systems">Real-World Systems</h3>
<p><strong>Redis:</strong><br />
- Single-threaded (no lock contention)<br />
- Approximates LRU with sampling (configurable sample size)<br />
- <code>maxmemory-policy</code>: allkeys-lru, volatile-lru, allkeys-lfu, etc.<br />
- Cluster mode: sharded by key hash slot</p>
<p><strong>Memcached:</strong><br />
- LRU per slab class (items grouped by size)<br />
- No native clustering (client-side sharding)<br />
- Simple protocol, very fast</p>
<p><strong>Caffeine (JVM):</strong><br />
- Window TinyLFU policy<br />
- Near-optimal hit rates<br />
- Async maintenance for high throughput</p>
<h3 id="interview-questions-distributed-caching">Interview Questions: Distributed Caching</h3>
<div>
<div>L1: How would you distribute an LRU cache across multiple servers?</div>
<div>Use consistent hashing to partition keys across nodes. Each node maintains its own LRU cache for its assigned keys. Clients hash the key to determine which node to contact. This scales capacity linearly with nodes.</div>
<div>
<div>L2: What happens when a node fails or is added?</div>
<div>With consistent hashing, only keys mapped to the failed/new node are affected. For failure: those keys become cache misses until the node recovers or is replaced. For addition: some keys from neighboring nodes remap to the new node (cold start). Use virtual nodes to distribute load more evenly. Consider replication for high availability.</div>
<div>
<div>L3: How do you handle cache stampede when a popular key expires or node fails?</div>
<div>Cache stampede: multiple requests simultaneously try to recompute/fetch a missing popular key, overwhelming the backend. Solutions: (1) Locking - only one request recomputes, others wait or get stale data. (2) Probabilistic early expiration - randomly refresh before TTL to avoid synchronized expiration. (3) Background refresh - proactively refresh popular keys before expiration. (4) Request coalescing - combine identical in-flight requests. (5) Circuit breaker - limit concurrent backend requests. For node failure specifically, implement warm-up procedures to gradually restore the cache.</div>
</div>
</div>
</div>
<hr />
<h2 id="edge-cases-and-common-bugs">Edge Cases and Common Bugs</h2>
<div>
<div>Common Implementation Bugs</div>
<table>
  <thead>
<tr>
<th>Bug</th>
<th>Symptom</th>
<th>Fix</th>
</tr>
  </thead>
  <tbody>
<tr>
<td>Forget to delete from HashMap on eviction</td>
<td>Memory leak, size never decreases</td>
<td>Always delete from both structures</td>
</tr>
<tr>
<td>Update doesn't move to front</td>
<td>Updated items evicted unexpectedly</td>
<td>Call moveToFront on update</td>
</tr>
<tr>
<td>Wrong pointer update order</td>
<td>Corrupted list, lost nodes</td>
<td>Set new node pointers first</td>
</tr>
<tr>
<td>Confuse sentinel with data nodes</td>
<td>Return dummy values, crash</td>
<td>Clear separation, never store data in sentinels</td>
</tr>
<tr>
<td>Capacity 0 not handled</td>
<td>Division by zero, infinite loop</td>
<td>Validate capacity> 0 in constructor</td>
</tr>
<tr>
<td>Node key not stored</td>
<td>Can't remove from HashMap on evict</td>
<td>Store key in Node class</td>
</tr>
  </tbody>
</table>
</div>
<h3 id="critical-edge-cases-to-test">Critical Edge Cases to Test</h3>
<pre><code>                                                          ```python
                                                          def test_edge_cases():
                                                          # Capacity 1 - every put after first evicts
                                                          cache = LRUCache(1)
                                                          cache.put(1, 1)
                                                          cache.put(2, 2)  # Evicts 1
                                                          assert cache.get(1) == -1
                                                          assert cache.get(2) == 2

                                                          # Update same key multiple times
                                                          cache = LRUCache(2)
                                                          cache.put(1, 1)
                                                          cache.put(1, 10)  # Update, not insert
                                                          cache.put(1, 100) # Update again
                                                          assert cache.get(1) == 100
                                                          assert len(cache.cache) == 1  # Still only 1 entry

                                                          # Get non-existent key shouldn't crash or modify state
                                                          cache = LRUCache(2)
                                                          assert cache.get(999) == -1
                                                          cache.put(1, 1)
                                                          assert cache.get(999) == -1
                                                          assert cache.get(1) == 1  # Original still works

                                                          # Access pattern affects eviction
                                                          cache = LRUCache(3)
                                                          cache.put(1, 1)
                                                          cache.put(2, 2)
                                                          cache.put(3, 3)
                                                          cache.get(1)      # 1 is now MRU
                                                          cache.put(4, 4)   # Evicts 2 (not 1!)
                                                          assert cache.get(2) == -1
                                                          assert cache.get(1) == 1

                                                          # Empty cache operations
                                                          cache = LRUCache(2)
                                                          assert cache.get(1) == -1  # No crash

                                                          # Large capacity
                                                          cache = LRUCache(10000)
                                                          for i in range(10000):
                                                          cache.put(i, i)
                                                          assert cache.get(0) == 0  # First item still there
                                                          cache.put(10000, 10000)   # Now evicts 1 (not 0, we just accessed 0)
                                                          assert cache.get(1) == -1
                                                          ```
</code></pre>
<hr />
<h2 id="interview-execution-guide">Interview Execution Guide</h2>
<h3 id="45-minute-timeline">45-Minute Timeline</h3>
<div>
<div>
<div>
<div>0-5 min</div>
<div>
<div>Clarify Requirements</div>
<div>Operations? Thread-safety? Capacity range? Return type for missing keys?</div>
</div>
</div>
<div>
<div>5-12 min</div>
<div>
<div>Design Discussion</div>
<div>Draw HashMap + Doubly Linked List. Explain why each is needed. Walk through get/put.</div>
</div>
</div>
<div>
<div>12-35 min</div>
<div>
<div>Implementation</div>
<div>Node class -> Helper methods (add, remove, move) -> get() -> put(). Comment as you go.</div>
</div>
</div>
<div>
<div>35-40 min</div>
<div>
<div>Testing & Verification</div>
<div>Dry run example. Trace through edge cases verbally. Check pointer updates.</div>
</div>
</div>
<div>
<div>40-45 min</div>
<div>
<div>Extensions Discussion</div>
<div>Thread-safety approach. TTL addition. LRU vs LFU. Distributed caching.</div>
</div>
</div>
</div>
</div>
<h3 id="what-differentiates-candidates">What Differentiates Candidates</h3>
<table>
<thead>
<tr>
<th>Level</th>
<th>Expectation</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Junior</strong></td>
<td>Completes working solution with prompting, handles basic cases</td>
</tr>
<tr>
<td><strong>Mid</strong></td>
<td>Clean code with helper methods, discusses trade-offs, handles edge cases</td>
</tr>
<tr>
<td><strong>Senior</strong></td>
<td>Thread-safety discussion, production considerations, alternative algorithms</td>
</tr>
<tr>
<td><strong>Staff+</strong></td>
<td>Distributed design, cache coherence, real-world system references</td>
</tr>
</tbody>
</table>
<h3 id="follow-up-questions-to-prepare">Follow-up Questions to Prepare</h3>
<ol>
<li><strong>Thread-safety</strong>: &quot;How would you make this thread-safe?&quot; (locks, concurrent data structures)</li>
<li><strong>TTL</strong>: &quot;Add expiration support&quot; (timestamp per entry, lazy vs eager eviction)</li>
<li><strong>Delete operation</strong>: &quot;Add explicit delete&quot; (trivial addition)</li>
<li><strong>Distributed</strong>: &quot;Scale to multiple machines&quot; (consistent hashing, per-shard LRU)</li>
<li><strong>LFU comparison</strong>: &quot;When would LFU be better?&quot; (frequency matters, scan resistance)</li>
<li><strong>Memory</strong>: &quot;Reduce memory usage&quot; (intrusive list, object pooling)</li>
<li><strong>Generics</strong>: &quot;Support any key/value type&quot; (generics, comparable keys)</li>
</ol>
<hr />
<h2 id="time-and-space-complexity">Time and Space Complexity</h2>
<div>
<table>
  <thead>
<tr>
<th>Operation</th>
<th>Time (Average)</th>
<th>Time (Worst)</th>
<th>Notes</th>
</tr>
  </thead>
  <tbody>
<tr>
<td>get(key)</td>
<td>O(1)</td>
<td>O(n)*</td>
<td>*HashMap worst case with collisions</td>
</tr>
<tr>
<td>put(key, value)</td>
<td>O(1)</td>
<td>O(n)*</td>
<td>*HashMap resize amortized O(1)</td>
</tr>
<tr>
<td>delete(key)</td>
<td>O(1)</td>
<td>O(n)*</td>
<td>Same as get + unlink</td>
</tr>
<tr>
<td>Space</td>
<td colspan="2"><span>O(capacity)</span></td>
<td>HashMap + List nodes</td>
</tr>
  </tbody>
</table>
</div>
<p><strong>Per-Entry Memory (approximate, 64-bit system):</strong><br />
- HashMap entry: 32-48 bytes (key, value ref, hash, next)<br />
- List node: 24-40 bytes (key copy, value, prev, next)<br />
- Total: ~60-90 bytes per entry (varies by language/implementation)</p>
<hr />
<h2 id="related-topics">Related Topics</h2>
<pre><code>                                                          - [[Hash Map]](/topic/data-structures/hash-map) - Underlying lookup structure
                                                          - [[Doubly Linked List]](/topic/data-structures/doubly-linked-list) - Underlying order structure
                                                          - [[LFU Cache]](/topic/machine-coding/lfu-cache) - Frequency-based alternative
                                                          - [[Consistent Hashing]](/topic/system-design/consistent-hashing) - Distribution strategy
                                                          - [[Redis]](/topic/system-design/redis) - Production cache implementation
                                                          - [[Caching Strategies]](/topic/system-design/caching) - Broader caching patterns
</code></pre>
<hr />
<h2 id="summary">Summary</h2>
<div>
<div>Key Takeaways</div>
<ul>
<li><strong>Architecture</strong>: HashMap (O(1) lookup) + Doubly Linked List (O(1) reorder) = O(1) for both operations</li>
<li><strong>Why doubly linked</strong>: Removal needs predecessor access; singly linked would be O(n)</li>
<li><strong>Sentinel nodes</strong>: Eliminate null checks, simplify edge cases</li>
<li><strong>Store key in node</strong>: Required to delete from HashMap during eviction</li>
<li><strong>Thread-safety</strong>: get() modifies structure, needs write lock; consider buffered writes for high throughput</li>
<li><strong>Distributed</strong>: Per-shard LRU with consistent hashing; no global LRU needed</li>
<li><strong>Alternatives</strong>: LFU for frequency-heavy workloads; TinyLFU for best hit rates</li>
</ul>
</div>
