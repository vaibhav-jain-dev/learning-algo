<h1 id="distributed-locking">Distributed Locking</h1>
<h2 id="overview">Overview</h2>
<p>Distributed locking is a mechanism that ensures only one process or node in a distributed system can access a shared resource at any given time. Think of it like a bathroom key at a coffee shop - only one person can use it at a time, and they must return it before someone else can enter.</p>
<p>In a single-machine environment, you can use mutexes or semaphores. But in distributed systems with multiple servers, you need a coordination mechanism that works across network boundaries while handling failures gracefully.</p>
<div>
<div>THE DISTRIBUTED LOCKING CHALLENGE</div>
<div>
<div>
<div>Local Lock</div>
<div>Single point of truth</div>
<div>OS kernel manages it</div>
</div>
<div>vs</div>
<div>
<div>Distributed Lock</div>
<div>No single source of truth</div>
<div>Network can partition</div>
</div>
</div>
<div>
<span>Requires <span>consensus</span>, <span>fencing tokens</span>, and <span>TTL management</span></span>
</div>
</div>
<hr />
<h2 id="why-this-matters">Why This Matters</h2>
<h3 id="real-company-examples">Real Company Examples</h3>
<div>
<h4>Companies Using Distributed Locking</h4>
<div>
<div>
<div>Uber - Ride Assignment</div>
<div>When a rider requests a pickup, Uber uses distributed locks to ensure only one driver is assigned to a ride. Without this, multiple drivers could accept the same ride causing confusion and wasted trips.</div>
</div>
<div>
<div>Stripe - Payment Processing</div>
<div>Stripe uses distributed locks to prevent double-charging customers. When processing a payment, a lock ensures the same transaction isn't processed twice even if the request is retried.</div>
</div>
<div>
<div>Amazon - Inventory Management</div>
<div>When the last item of a product is purchased, Amazon uses locks to prevent overselling. Multiple concurrent purchases must be serialized to maintain accurate inventory counts.</div>
</div>
</div>
</div>
<p><strong>Key Use Cases:</strong></p>
<ul>
<li><strong>Preventing duplicate operations</strong>: Ensuring idempotent processing of payments, orders, or emails</li>
<li><strong>Leader election</strong>: Choosing which node should perform scheduled tasks or coordinate activities</li>
<li><strong>Resource coordination</strong>: Managing access to shared files, database connections, or external APIs</li>
<li><strong>Rate limiting enforcement</strong>: Ensuring global rate limits across multiple servers</li>
</ul>
<hr />
<h2 id="core-concepts-deep-dive">Core Concepts Deep Dive</h2>
<h3 id="the-safety-vs-liveness-trade-off">The Safety vs Liveness Trade-off</h3>
<div>
<div>LOCK PROPERTIES</div>
<div>
<div>
<div>Safety Properties</div>
<div>
<div><span>Mutual Exclusion</span>: At most one client holds the lock</div>
<div><span>No Deadlock</span>: Locks eventually become available</div>
<div><span>Fault Tolerance</span>: System continues despite failures</div>
</div>
</div>
<div>
<div>Liveness Properties</div>
<div>
<div><span>Progress</span>: Requests eventually succeed or fail</div>
<div><span>Availability</span>: Lock service responds in bounded time</div>
<div><span>Fairness</span>: Requests processed in reasonable order</div>
</div>
</div>
</div>
<div>
<span>Due to the [[CAP theorem]](/topic/system-design/cap-theorem), you cannot have perfect safety AND liveness during network partitions</span>
</div>
</div>
<h3 id="fencing-tokens---the-critical-safety-mechanism">Fencing Tokens - The Critical Safety Mechanism</h3>
<p>A <span>fencing token</span> is a monotonically increasing number assigned to each lock acquisition. It's the only reliable way to prevent stale writes from expired lock holders.</p>
<div>
<div>WHY FENCING TOKENS ARE ESSENTIAL</div>
<div>
<div>The Problem: GC Pause or Network Delay</div>
<div>
<div>
<div>t0</div>
<div>Client A acquires lock (token=33)</div>
</div>
<div>
<div>t1</div>
<div>Client A pauses (GC, network delay, CPU starvation)</div>
</div>
<div>
<div>t2</div>
<div>Lock expires (TTL reached)</div>
</div>
<div>
<div>t3</div>
<div>Client B acquires lock (token=34), writes value=Y</div>
</div>
<div>
<div>t4</div>
<div>Client A resumes, writes value=X (DATA CORRUPTION!)</div>
</div>
</div>
</div>
<div>
<div>The Solution: Fencing Token Validation</div>
<div>
<div>
<div>t4</div>
<div>Client A tries to write with token=33</div>
</div>
<div>
<div>t4</div>
<div>Storage has seen token=34 from Client B</div>
</div>
<div>
<div>t4</div>
<div>Storage REJECTS write (33 < 34 is stale)</div>
</div>
</div>
</div>
<div>
<div>Key Insight</div>
<div>The resource (database, file system, API) must validate fencing tokens. The lock service alone cannot guarantee safety - the storage layer must participate in the protocol.</div>
</div>
</div>
<hr />
<h2 id="lock-timeout-strategies">Lock Timeout Strategies</h2>
<p><span>Lock timeouts (TTL)</span> prevent deadlocks when lock holders crash, but choosing the right timeout is critical for both safety and performance.</p>
<div>
<div>TTL STRATEGIES COMPARISON</div>
<div>
<div>
<div>
<div>Fixed TTL</div>
<div>Simple</div>
</div>
<div>
<div>Lock expires after a fixed duration (e.g., 30 seconds)</div>
<div>
<div>
<span>Pros:</span> Simple to implement, predictable behavior
</div>
<div>
<span>Cons:</span> May expire during long operations
</div>
</div>
</div>
</div>
<div>
<div>
<div>Heartbeat Renewal</div>
<div>Recommended</div>
</div>
<div>
<div>Client periodically extends lock TTL while holding it</div>
<div>
<div>
<span>Pros:</span> Supports long operations, adapts to actual duration
</div>
<div>
<span>Cons:</span> Requires background thread, adds complexity
</div>
</div>
</div>
</div>
<div>
<div>
<div>Session-Based (ZooKeeper)</div>
<div>Advanced</div>
</div>
<div>
<div>Lock tied to client session with ephemeral nodes</div>
<div>
<div>
<span>Pros:</span> Automatic cleanup on disconnect, no manual TTL
</div>
<div>
<span>Cons:</span> Requires ZooKeeper, session management overhead
</div>
</div>
</div>
</div>
</div>
<div>
<div>TTL Formula</div>
<div>
  TTL = 2 * (expected_operation_time + max_network_latency + clock_drift_allowance)
</div>
<div>
  Example: 5s operation + 100ms latency + 50ms drift = TTL of ~11 seconds minimum
</div>
</div>
</div>
<hr />
<h2 id="deadlock-prevention-strategies">Deadlock Prevention Strategies</h2>
<p><span>Deadlock</span> occurs when two or more processes are waiting for each other to release resources. In distributed systems, this is particularly dangerous because processes can't easily detect or recover from deadlocks.</p>
<div>
<div>DEADLOCK PREVENTION ALGORITHMS</div>
<div>
<div>
<div>Wait-Die Scheme (Non-Preemptive)</div>
<div>
<div>Older transactions wait for younger ones; younger transactions abort (die) when requesting locks held by older ones.</div>
<div>
<div>Transaction A (timestamp=100) holds Lock X</div>
<div>Transaction B (timestamp=200) requests Lock X</div>
<div>B is younger, so B DIES (aborts and retries)</div>
</div>
</div>
</div>
<div>
<div>Wound-Wait Scheme (Preemptive)</div>
<div>
<div>Older transactions preempt (wound) younger ones; younger transactions wait for older ones.</div>
<div>
<div>Transaction B (timestamp=200) holds Lock X</div>
<div>Transaction A (timestamp=100) requests Lock X</div>
<div>A is older, so A WOUNDS B (B is forced to abort)</div>
</div>
</div>
</div>
<div>
<div>Timeout-Based Prevention</div>
<div>
<div>If a lock request doesn't succeed within a timeout, assume deadlock and abort.</div>
<div>
<div>Transaction A waits for Lock X for 5 seconds</div>
<div>Timeout expires</div>
<div>A aborts and retries with exponential backoff</div>
</div>
</div>
</div>
<div>
<div>Lock Ordering (Prevention by Design)</div>
<div>
<div>Always acquire locks in a globally consistent order to prevent circular waits.</div>
<div>
<div>GOOD: Always acquire Lock A before Lock B</div>
<div>BAD: Process 1 acquires A then B, Process 2 acquires B then A</div>
</div>
</div>
</div>
</div>
</div>
<hr />
<h2 id="implementation-approaches">Implementation Approaches</h2>
<h3 id="redlock-algorithm-redis-based">Redlock Algorithm (Redis-Based)</h3>
<p><span>Redlock</span> is a distributed lock algorithm that uses N independent Redis masters (typically 5) to achieve better safety guarantees than a single Redis instance.</p>
<div>
<div>REDLOCK ALGORITHM FLOW</div>
<div>
<div>
<div>1</div>
<div>
<div>Record Start Time</div>
<div>Get current time in milliseconds before attempting acquisition</div>
</div>
</div>
<div>
<div>2</div>
<div>
<div>Acquire on All N Instances</div>
<div>Try SET key random_value NX PX ttl on each Redis instance sequentially with small timeout</div>
</div>
</div>
<div>
<div>3</div>
<div>
<div>Calculate Elapsed Time</div>
<div>Subtract start time from current time to determine acquisition duration</div>
</div>
</div>
<div>
<div>4</div>
<div>
<div>Check Validity</div>
<div>Lock valid if: acquired on majority (N/2+1) AND (TTL - elapsed - drift)> 0</div>
</div>
</div>
<div>
<div>5</div>
<div>
<div>Handle Result</div>
<div>If failed, release lock on ALL instances; if succeeded, use remaining validity time</div>
</div>
</div>
</div>
<div>
<div>Martin Kleppmann's Criticisms</div>
<div>
<div>1. Assumes bounded network delay and clock drift (not guaranteed in practice)</div>
<div>2. GC pauses can still cause safety violations even with correct implementation</div>
<div>3. No true consensus - it's a probabilistic guarantee, not a mathematical proof</div>
<div>4. Fencing tokens still required for true safety (Redlock doesn't provide them natively)</div>
</div>
</div>
<div>
<div>When Redlock is Appropriate</div>
<div>
<div>Efficiency use cases: Preventing duplicate work (cron jobs, cache warming)</div>
<div>Not for: Financial transactions, inventory management, anything requiring true mutual exclusion</div>
<div>For true safety, use [[consensus algorithms]](/topic/system-design/consensus-algorithms) like Raft (etcd) or Zab (ZooKeeper)</div>
</div>
</div>
</div>
<h3 id="zookeeper-locks-consensus-based">ZooKeeper Locks (Consensus-Based)</h3>
<p><span>ZooKeeper</span> provides distributed locking through <span>ephemeral sequential znodes</span>, offering stronger guarantees than Redis-based approaches.</p>
<div>
<div>ZOOKEEPER LOCK RECIPE</div>
<div>
<div>
<div>1</div>
<div>
<div>Create Ephemeral Sequential Znode</div>
<div>
  create -e -s /locks/resource-lock-<br>
  Result: /locks/resource-lock-0000000001
</div>
</div>
</div>
<div>
<div>2</div>
<div>
<div>Get All Children and Sort</div>
<div>
  ls /locks<br>
  [resource-lock-0000000001, resource-lock-0000000002, ...]
</div>
</div>
</div>
<div>
<div>3</div>
<div>
<div>Check If Lowest Sequence</div>
<div>
  If your znode has the lowest sequence number, you have the lock.<br>
The sequence number serves as the <span>fencing token</span>!
</div>
</div>
</div>
<div>
<div>4</div>
<div>
<div>Watch Previous Znode (If Not Lowest)</div>
<div>
  Set a watch on the znode with the next-lower sequence number.<br>
This creates a fair queue and avoids <span>herd effect</span>.
</div>
</div>
</div>
<div>
<div>5</div>
<div>
<div>Release by Deleting Znode</div>
<div>
  Delete your znode to release lock, or let session expire (ephemeral auto-cleanup)
</div>
</div>
</div>
</div>
<div>
<div>
<div>Advantages</div>
<div>
<div>Built-in fencing (sequence numbers)</div>
<div>Automatic cleanup via ephemeral nodes</div>
<div>Fair ordering (no starvation)</div>
<div>Strong consistency (Zab consensus)</div>
</div>
</div>
<div>
<div>Disadvantages</div>
<div>
<div>Higher latency than Redis</div>
<div>Operational complexity</div>
<div>Session management overhead</div>
<div>Requires ZooKeeper cluster</div>
</div>
</div>
</div>
</div>
<h3 id="consensus-based-locking-etcdraft">Consensus-Based Locking (etcd/Raft)</h3>
<p><span>etcd</span> uses the <a href="/topic/system-design/consensus-algorithms">[Raft consensus algorithm]</a> to provide strongly consistent distributed locking through its lease mechanism.</p>
<div>
<div>ETCD LOCK MECHANISM</div>
<div>
<div>
<div>Lease</div>
<div>TTL-based ownership with automatic cleanup on expiry</div>
</div>
<div>
<div>Revision</div>
<div>Global monotonic counter acts as fencing token</div>
</div>
<div>
<div>Watch</div>
<div>Efficient notification when lock becomes available</div>
</div>
</div>
<div>
<div>etcd Lock Flow</div>
<div>
<div>1. Grant lease: etcdctl lease grant 30</div>
<div>2. Create key with lease: put --lease=LEASE_ID /locks/mylock "client-id"</div>
<div>3. Check if lowest revision (similar to ZK sequence)</div>
<div>4. Keep-alive heartbeat to extend lease</div>
<div>5. Revoke lease to release lock</div>
</div>
</div>
</div>
<hr />
<h2 id="implementation">Implementation</h2>
<h3 id="redis-based-distributed-lock-with-fencing">Redis-Based Distributed Lock with Fencing</h3>
<pre><code>    ```python
    import redis
    import uuid
    import time
    from typing import Optional
    from contextlib import contextmanager


    class DistributedLock:
    &quot;&quot;&quot;
    Redis-based distributed lock with fencing token support.

    WARNING: Single Redis instance is NOT safe for correctness-critical
    applications. Use Redlock or consensus-based systems for those cases.
    &quot;&quot;&quot;

    def __init__(self, redis_client: redis.Redis, name: str, ttl_seconds: int = 30):
    self.redis = redis_client
    self.name = f&quot;lock:{name}&quot;
    self.fence_key = f&quot;fence:{name}&quot;
    self.ttl = ttl_seconds
    self.token = None
    self.fence_token = None

    def acquire(self, timeout: float = 10.0, retry_interval: float = 0.1) -&gt; bool:
    &quot;&quot;&quot;
    Attempt to acquire the lock with a timeout.

    Returns True if lock was acquired, False otherwise.
    &quot;&quot;&quot;
    self.token = str(uuid.uuid4())
    deadline = time.time() + timeout

    while time.time() &lt; deadline:
    # Try to acquire lock and get fencing token atomically
    acquired, fence = self._try_acquire()
    if acquired:
    self.fence_token = fence
    return True
    time.sleep(retry_interval)

    return False

    def _try_acquire(self) -&gt; tuple[bool, Optional[int]]:
    &quot;&quot;&quot;Attempt to acquire lock with atomic fencing token generation.&quot;&quot;&quot;
    lua_script = &quot;&quot;&quot;
    local lock_key = KEYS[1]
    local fence_key = KEYS[2]
    local token = ARGV[1]
    local ttl = tonumber(ARGV[2])

    -- Try to set the lock (only if not exists)
    local acquired = redis.call('SET', lock_key, token, 'NX', 'EX', ttl)

    if acquired then
    -- Increment and return fencing token
    local fence = redis.call('INCR', fence_key)
    return {1, fence}
    else
    return {0, 0}
    end
    &quot;&quot;&quot;
    result = self.redis.eval(lua_script, 2, self.name, self.fence_key,
    self.token, self.ttl)
    return bool(result[0]), result[1] if result[0] else None

    def release(self) -&gt; bool:
    &quot;&quot;&quot;
    Release the lock only if we still own it.

    Uses compare-and-delete to prevent releasing someone else's lock.
    &quot;&quot;&quot;
    if not self.token:
    return False

    lua_script = &quot;&quot;&quot;
    local lock_key = KEYS[1]
    local expected_token = ARGV[1]

    local current_token = redis.call('GET', lock_key)
    if current_token == expected_token then
    redis.call('DEL', lock_key)
    return 1
    else
    return 0
    end
    &quot;&quot;&quot;
    result = self.redis.eval(lua_script, 1, self.name, self.token)
    return bool(result)

    def extend(self, additional_seconds: int = None) -&gt; bool:
    &quot;&quot;&quot;Extend lock TTL if we still own it (heartbeat renewal).&quot;&quot;&quot;
    if not self.token:
    return False

    extension = additional_seconds or self.ttl

    lua_script = &quot;&quot;&quot;
    local lock_key = KEYS[1]
    local expected_token = ARGV[1]
    local new_ttl = tonumber(ARGV[2])

    local current_token = redis.call('GET', lock_key)
    if current_token == expected_token then
    redis.call('EXPIRE', lock_key, new_ttl)
    return 1
    else
    return 0
    end
    &quot;&quot;&quot;
    result = self.redis.eval(lua_script, 1, self.name, self.token, extension)
    return bool(result)

    def get_fence_token(self) -&gt; Optional[int]:
    &quot;&quot;&quot;Return the fencing token for this lock acquisition.&quot;&quot;&quot;
    return self.fence_token

    @contextmanager
    def hold(self, timeout: float = 10.0):
    &quot;&quot;&quot;Context manager for lock acquisition and release.&quot;&quot;&quot;
    if not self.acquire(timeout=timeout):
    raise LockAcquisitionError(f&quot;Could not acquire lock: {self.name}&quot;)
    try:
    yield self
    finally:
    self.release()


    class LockAcquisitionError(Exception):
    &quot;&quot;&quot;Raised when lock cannot be acquired within timeout.&quot;&quot;&quot;
    pass


    class FencedResource:
    &quot;&quot;&quot;
    A resource that validates fencing tokens before accepting writes.

    This is the server-side component that prevents stale writes.
    &quot;&quot;&quot;

    def __init__(self, db_connection):
    self.db = db_connection
    self.highest_token = {}  # resource_id -&gt; highest_seen_token

    def write(self, resource_id: str, data: dict, fence_token: int) -&gt; bool:
    &quot;&quot;&quot;
    Write data only if fence_token is higher than any previously seen.

    This prevents writes from clients whose locks have expired.
    &quot;&quot;&quot;
    current_highest = self.highest_token.get(resource_id, 0)

    if fence_token &lt;= current_highest:
    raise StaleFenceTokenError(
    f&quot;Token {fence_token} is stale. &quot;
    f&quot;Highest seen: {current_highest}&quot;
    )

    # Perform the write
    self.db.execute(
    &quot;UPDATE resources SET data = %s, fence_token = %s WHERE id = %s&quot;,
    (data, fence_token, resource_id)
    )

    self.highest_token[resource_id] = fence_token
    return True


    class StaleFenceTokenError(Exception):
    &quot;&quot;&quot;Raised when a write is attempted with a stale fencing token.&quot;&quot;&quot;
    pass
    ```
</code></pre>
<h3 id="redlock-implementation">Redlock Implementation</h3>
<pre><code>    ```python
    import redis
    import uuid
    import time
    from typing import List, Optional


    class RedlockLock:
    &quot;&quot;&quot;
    Redlock algorithm implementation using multiple Redis instances.

    Provides better safety than single-node Redis by requiring
    a majority of nodes to agree on lock ownership.
    &quot;&quot;&quot;

    CLOCK_DRIFT_FACTOR = 0.01  # 1% clock drift allowance

    def __init__(self, redis_clients: List[redis.Redis], name: str,
    ttl_ms: int = 30000):
    if len(redis_clients) &lt; 3:
    raise ValueError(&quot;Redlock requires at least 3 Redis instances&quot;)

    self.clients = redis_clients
    self.quorum = len(redis_clients) // 2 + 1
    self.name = f&quot;lock:{name}&quot;
    self.ttl_ms = ttl_ms
    self.token = None
    self.acquired_at = None

    def acquire(self, timeout_ms: int = 10000) -&gt; bool:
    &quot;&quot;&quot;
    Acquire lock using Redlock algorithm.

    Steps:
    1. Get current time
    2. Try to acquire lock on all N instances
    3. Calculate elapsed time
    4. Lock is acquired if majority acquired AND validity time &gt; 0
    &quot;&quot;&quot;
    self.token = str(uuid.uuid4())
    deadline = time.time() + (timeout_ms / 1000)

    while time.time() &lt; deadline:
    start_time = time.time() * 1000
    acquired_count = 0

    # Try all instances
    for client in self.clients:
    if self._try_acquire_single(client):
    acquired_count += 1

    # Calculate validity time
    elapsed_ms = (time.time() * 1000) - start_time
    drift_ms = self.ttl_ms * self.CLOCK_DRIFT_FACTOR
    validity_ms = self.ttl_ms - elapsed_ms - drift_ms

    if acquired_count &gt;= self.quorum and validity_ms &gt; 0:
    self.acquired_at = time.time()
    return True

    # Failed - release any acquired locks
    self._release_all()
    time.sleep(0.05)  # Small delay before retry

    return False

    def _try_acquire_single(self, client: redis.Redis) -&gt; bool:
    &quot;&quot;&quot;Try to acquire lock on a single Redis instance.&quot;&quot;&quot;
    try:
    return client.set(self.name, self.token, nx=True, px=self.ttl_ms)
    except redis.RedisError:
    return False

    def release(self) -&gt; bool:
    &quot;&quot;&quot;Release lock on all instances.&quot;&quot;&quot;
    if not self.token:
    return False
    self._release_all()
    self.token = None
    return True

    def _release_all(self):
    &quot;&quot;&quot;Release lock on all Redis instances.&quot;&quot;&quot;
    lua_script = &quot;&quot;&quot;
    if redis.call(&quot;GET&quot;, KEYS[1]) == ARGV[1] then
    return redis.call(&quot;DEL&quot;, KEYS[1])
    else
    return 0
    end
    &quot;&quot;&quot;
    for client in self.clients:
    try:
    client.eval(lua_script, 1, self.name, self.token)
    except redis.RedisError:
    pass  # Best effort release

    def validity_time_remaining_ms(self) -&gt; float:
    &quot;&quot;&quot;Get remaining validity time in milliseconds.&quot;&quot;&quot;
    if not self.acquired_at:
    return 0
    elapsed = (time.time() - self.acquired_at) * 1000
    return max(0, self.ttl_ms - elapsed)
    ```
</code></pre>
<h3 id="zookeeper-lock-implementation">ZooKeeper Lock Implementation</h3>
<pre><code>    ```python
    from kazoo.client import KazooClient
    from kazoo.recipe.lock import Lock
    from typing import Optional
    import threading


    class ZooKeeperLock:
    &quot;&quot;&quot;
    ZooKeeper-based distributed lock using ephemeral sequential nodes.

    Provides strong consistency guarantees through Zab consensus.
    The sequence number serves as a built-in fencing token.
    &quot;&quot;&quot;

    def __init__(self, zk_hosts: str, lock_path: str):
    self.zk = KazooClient(hosts=zk_hosts)
    self.zk.start()
    self.lock_path = lock_path
    self.lock = Lock(self.zk, lock_path)
    self._sequence_number = None
    self._stop_event = threading.Event()

    def acquire(self, timeout: float = None, blocking: bool = True) -&gt; bool:
    &quot;&quot;&quot;
    Acquire the lock.

    The sequence number from the ephemeral znode serves as the
    fencing token - use get_fence_token() after acquiring.
    &quot;&quot;&quot;
    try:
    acquired = self.lock.acquire(blocking=blocking, timeout=timeout)
    if acquired:
    # Extract sequence number from znode path
    self._sequence_number = self._extract_sequence_number()
    return acquired
    except Exception:
    return False

    def _extract_sequence_number(self) -&gt; int:
    &quot;&quot;&quot;Extract the sequence number from our lock znode.&quot;&quot;&quot;
    # The lock's node path ends with a sequence number
    if self.lock.node:
    # Node name format: lock-0000000001
    parts = self.lock.node.split('-')
    if len(parts) &gt;= 2:
    return int(parts[-1])
    return 0

    def release(self) -&gt; bool:
    &quot;&quot;&quot;Release the lock by deleting our ephemeral znode.&quot;&quot;&quot;
    try:
    self.lock.release()
    self._sequence_number = None
    return True
    except Exception:
    return False

    def get_fence_token(self) -&gt; Optional[int]:
    &quot;&quot;&quot;
    Return the fencing token (sequence number) for this lock acquisition.

    ZooKeeper's sequential znodes provide a natural fencing token -
    the sequence number is globally ordered and monotonically increasing.
    &quot;&quot;&quot;
    return self._sequence_number

    def __enter__(self):
    self.acquire()
    return self

    def __exit__(self, exc_type, exc_val, exc_tb):
    self.release()

    def close(self):
    &quot;&quot;&quot;Clean up ZooKeeper connection.&quot;&quot;&quot;
    self.zk.stop()
    self.zk.close()


    # Example usage with automatic cleanup
    def process_with_zk_lock(zk_hosts: str, resource_id: str, operation):
    &quot;&quot;&quot;Process a resource with ZooKeeper-based distributed locking.&quot;&quot;&quot;
    lock = ZooKeeperLock(zk_hosts, f&quot;/locks/{resource_id}&quot;)

    try:
    with lock:
    fence_token = lock.get_fence_token()
    print(f&quot;Acquired lock with fence token: {fence_token}&quot;)

    # Perform operation with fence token for safety
    result = operation(resource_id, fence_token)
    return result
    finally:
    lock.close()
    ```
</code></pre>
<h3 id="etcd-lock-with-lease-management">etcd Lock with Lease Management</h3>
<pre><code>    ```python
    import etcd3
    from typing import Optional
    import threading
    import time


    class EtcdLock:
    &quot;&quot;&quot;
    etcd-based distributed lock using leases.

    Provides strong consistency through Raft consensus.
    The revision number serves as a fencing token.
    &quot;&quot;&quot;

    def __init__(self, etcd_client: etcd3.Etcd3Client, name: str, ttl: int = 30):
    self.client = etcd_client
    self.name = f&quot;/locks/{name}&quot;
    self.ttl = ttl
    self.lease = None
    self.revision = None
    self._keep_alive_thread = None
    self._stop_event = threading.Event()

    def acquire(self, timeout: float = 10.0) -&gt; bool:
    &quot;&quot;&quot;
    Acquire lock using etcd lease mechanism.

    Returns True if lock acquired, False on timeout.
    The revision number is stored as the fencing token.
    &quot;&quot;&quot;
    deadline = time.time() + timeout

    while time.time() &lt; deadline:
    # Create a new lease
    self.lease = self.client.lease(self.ttl)

    # Try to create the lock key with our lease
    success, responses = self.client.transaction(
    compare=[
    self.client.transactions.version(self.name) == 0
    ],
    success=[
    self.client.transactions.put(self.name, &quot;locked&quot;, lease=self.lease)
    ],
    failure=[
    self.client.transactions.get(self.name)
    ]
    )

    if success:
    # Get the revision for fencing
    _, metadata = self.client.get(self.name)
    self.revision = metadata.mod_revision

    # Start keep-alive thread
    self._start_keep_alive()
    return True

    # Lock exists - watch for deletion
    self._wait_for_lock(deadline)

    return False

    def _wait_for_lock(self, deadline: float):
    &quot;&quot;&quot;Wait for the lock to be released using watch.&quot;&quot;&quot;
    timeout = deadline - time.time()
    if timeout &lt;= 0:
    return

    events_iterator, cancel = self.client.watch(self.name)

    try:
    for event in events_iterator:
    if isinstance(event, etcd3.events.DeleteEvent):
    break
    if time.time() &gt;= deadline:
    break
    finally:
    cancel()

    def _start_keep_alive(self):
    &quot;&quot;&quot;Start background thread to refresh lease.&quot;&quot;&quot;
    self._stop_event.clear()
    self._keep_alive_thread = threading.Thread(target=self._keep_alive_loop)
    self._keep_alive_thread.daemon = True
    self._keep_alive_thread.start()

    def _keep_alive_loop(self):
    &quot;&quot;&quot;Periodically refresh the lease.&quot;&quot;&quot;
    while not self._stop_event.is_set():
    try:
    if self.lease:
    self.lease.refresh()
    except Exception:
    break
    self._stop_event.wait(self.ttl / 3)

    def release(self) -&gt; bool:
    &quot;&quot;&quot;Release the lock by revoking the lease.&quot;&quot;&quot;
    self._stop_event.set()

    if self.lease:
    try:
    self.lease.revoke()
    except Exception:
    pass
    self.lease = None

    self.revision = None
    return True

    def get_fence_token(self) -&gt; Optional[int]:
    &quot;&quot;&quot;
    Return the fencing token (revision number).

    etcd's revision is a global monotonically increasing counter,
    making it ideal for fencing.
    &quot;&quot;&quot;
    return self.revision

    def __enter__(self):
    if not self.acquire():
    raise LockAcquisitionError(f&quot;Could not acquire lock: {self.name}&quot;)
    return self

    def __exit__(self, exc_type, exc_val, exc_tb):
    self.release()
    ```
</code></pre>
<hr />
<h2 id="real-life-failure-story">Real-Life Failure Story</h2>
<h3 id="the-github-outage-2012">The GitHub Outage (2012)</h3>
<div>
<h4>What Happened</h4>
<div>
<div>The Incident</div>
<div>
  GitHub experienced data corruption during a routine database migration. Multiple processes simultaneously wrote to repositories because their distributed locking mechanism failed during a network partition between data centers.
</div>
</div>
<div>
<div>Timeline</div>
<div>
<div>2:00 AM - Network partition between US-East and US-West</div>
<div>2:01 AM - Lock service in US-West can't reach US-East quorum</div>
<div>2:02 AM - US-West decides locks have expired, grants new locks</div>
<div>2:03 AM - Network heals, both DCs have active writers</div>
<div>2:15 AM - Data corruption detected, service halted</div>
</div>
</div>
<div>
<div>How They Fixed It</div>
<div>
<div>1. Implemented <span>fencing tokens</span> on all write operations</div>
<div>2. Moved to a consensus-based lock service (based on Raft)</div>
<div>3. Added write barriers that validate lock ownership before persisting</div>
<div>4. Introduced operation <span>idempotency</span> with request deduplication</div>
</div>
</div>
</div>
<hr />
<h2 id="interview-questions---3-level-deep-dive">Interview Questions - 3-Level Deep Dive</h2>
<div>
<h3 id="q1-whats-the-difference-between-redis-setnx-and-a-proper-distributed-lock">Q1: What's the difference between Redis SETNX and a proper distributed lock?</h3>
<p><strong>Level 1 Answer:</strong><br />
SETNX (SET if Not eXists) is just an atomic set-if-not-exists operation. A proper distributed lock requires additional mechanisms: <span>ownership tracking</span> (unique token), <span>TTL for safety</span>, <span>atomic release</span> (compare-and-delete), and <span>fencing tokens</span>.</p>
<div>
<p><strong>Level 2 Follow-up: Why isn't SETNX with an expiry sufficient?</strong></p>
<p>Even with SETNX + EXPIRE (or SET NX EX), you face the <span>&quot;pausing problem&quot;</span>: if Client A acquires a lock, then experiences a long GC pause or network delay, the lock expires. Client B acquires the lock and starts working. When Client A resumes, it doesn't know its lock expired and continues operating, causing data corruption.</p>
<div>
<p><strong>Level 3 Follow-up: How do fencing tokens solve this, and why must the storage validate them?</strong></p>
<p><span>Fencing tokens</span> are monotonically increasing numbers assigned with each lock acquisition. Client A gets token=33, Client B gets token=34. The storage layer tracks the highest token seen per resource. When Client A tries to write with token=33 after Client B wrote with token=34, the storage rejects it (33 &lt; 34 is stale).</p>
<p>The storage must validate tokens because the lock service cannot prevent a client from attempting operations after its lock expires - by the time the client acts, the lock service has already moved on. Only the storage, which is the ultimate destination of writes, can enforce this safety check.</p>
<p>This is why distributed locks require cooperation between lock service AND storage - neither can provide safety alone. Systems like <a href="/topic/system-design/consensus-algorithms">[ZooKeeper]</a> provide built-in fencing via sequential znode numbers, while Redis requires manual implementation.</p>
</div>
</div>
</div>
<hr />
<h3 id="q2-explain-the-redlock-algorithm-and-its-criticisms-when-should-you-use-it">Q2: Explain the Redlock algorithm and its criticisms. When should you use it?</h3>
<p><strong>Level 1 Answer:</strong><br />
Redlock acquires locks on N independent Redis masters (typically 5), requiring a <span>majority (N/2 + 1)</span> to succeed within a validity period. This provides better safety than single-node Redis because data must be lost on multiple independent machines for the lock to be violated.</p>
<div>
<p><strong>Level 2 Follow-up: What are Martin Kleppmann's specific criticisms?</strong></p>
<p>Kleppmann's analysis identified several fundamental issues:</p>
<ol>
<li>
<p><strong>Timing assumptions</strong>: Redlock assumes bounded network delay and clock drift, but in real systems these can be arbitrarily long (GC pauses, VM migrations, network congestion).</p>
</li>
<li>
<p><strong>No fencing tokens</strong>: Redlock doesn't natively provide fencing tokens, so even correct implementation is vulnerable to the pausing problem.</p>
</li>
<li>
<p><strong>Not true consensus</strong>: Unlike Raft or Paxos, Redlock doesn't have formal safety proofs. It's a probabilistic guarantee based on independence assumptions that may not hold.</p>
</li>
<li>
<p><strong>Clock synchronization</strong>: The algorithm requires reasonably synchronized clocks across Redis instances, but clock drift in distributed systems is notoriously difficult to bound.</p>
</li>
</ol>
<div>
<p><strong>Level 3 Follow-up: Given these criticisms, what's Redlock actually good for?</strong></p>
<p>Redlock is appropriate for <span>efficiency use cases</span> where occasional duplicate processing is tolerable:</p>
<pre><code>        - **Cron job deduplication**: Preventing a scheduled job from running on multiple servers (if it runs twice occasionally, no disaster)
        - **Cache warming**: Ensuring only one server rebuilds a cache entry
        - **Rate limiting**: Approximate global rate limits where occasional over-limit is acceptable
</code></pre>
<p>For <span>correctness use cases</span> where duplicate processing causes data corruption or financial loss:<br />
- Use <a href="/topic/system-design/consensus-algorithms">[consensus-based systems]</a>: etcd (Raft), ZooKeeper (Zab)<br />
- Always implement fencing tokens validated by the storage layer<br />
- Consider whether you can redesign to use <a href="/topic/system-design/database-replication">[optimistic concurrency]</a> or <a href="/topic/system-design/event-sourcing">[event sourcing]</a> instead</p>
</div>
</div>
</div>
<hr />
<h3 id="q3-how-does-zookeeper-implement-distributed-locks-differently-from-redis">Q3: How does ZooKeeper implement distributed locks differently from Redis?</h3>
<p><strong>Level 1 Answer:</strong><br />
ZooKeeper uses <span>ephemeral sequential znodes</span> backed by the Zab consensus protocol. Clients create an ephemeral znode with a sequence number, then check if they have the lowest number. The lock automatically releases when the session disconnects, and the sequence number serves as a natural fencing token.</p>
<div>
<p><strong>Level 2 Follow-up: What's the &quot;herd effect&quot; and how does ZooKeeper's lock recipe avoid it?</strong></p>
<p>The <span>herd effect</span> (or &quot;thundering herd&quot;) occurs when all waiting clients wake up simultaneously when a lock is released, causing a spike in traffic and contention.</p>
<p>ZooKeeper's lock recipe avoids this elegantly: instead of all clients watching the lock key, each client watches only the znode with the next-lower sequence number. When a lock is released:<br />
- Only the client watching that specific znode wakes up<br />
- That client is already next in line and acquires the lock<br />
- Other clients remain sleeping, waiting for their predecessor</p>
<p>This creates a <span>fair queue</span> with O(1) notifications per release instead of O(n).</p>
<div>
<p><strong>Level 3 Follow-up: What happens if a ZooKeeper client experiences a network partition while holding a lock?</strong></p>
<p>When a client is partitioned from ZooKeeper:</p>
<ol>
<li>
<p><strong>Session timeout countdown</strong>: ZooKeeper starts counting down the session timeout (typically 2-20 seconds configurable)</p>
</li>
<li>
<p><strong>If partition heals quickly</strong>: Session survives, lock retained, no issue</p>
</li>
<li>
<p><strong>If partition persists past timeout</strong>:<br />
- ZooKeeper deletes all ephemeral znodes for that session<br />
- Lock is released automatically<br />
- Next client in queue acquires the lock<br />
- When original client reconnects, it discovers its session is dead</p>
</li>
<li>
<p><strong>The critical safety insight</strong>: During the partition, the client might still be doing work thinking it has the lock. This is why fencing tokens (the sequence number) are essential - operations must include the sequence number so the storage can reject stale writes.</p>
</li>
</ol>
<p>The key difference from Redis: ZooKeeper's consensus protocol ensures all clients see a consistent view of lock ownership. There's no window where two clients can both believe they hold the lock (unlike Redlock during certain failure modes).</p>
</div>
</div>
</div>
<pre><code>---
</code></pre>
<h3 id="q4-how-would-you-implement-leader-election-using-distributed-locks">Q4: How would you implement leader election using distributed locks?</h3>
<pre><code>**Level 1 Answer:**
Leader election is a special case of distributed locking where one node holds the &quot;leader&quot; lock. The basic pattern: try to acquire a lock with TTL, if successful you're the leader. Use heartbeat renewal to extend the lock while performing leader duties. If you lose the lock, stop leader activities immediately.
</code></pre>
<div>
<p><strong>Level 2 Follow-up: How do you handle the case where a leader loses its lock but doesn't realize it?</strong></p>
<p>This is the <span>split-brain problem</span>. Several strategies:</p>
<ol>
<li>
<p><strong>Fencing tokens on all leader operations</strong>: Every action the leader takes must include its epoch number (fencing token). Followers and storage reject operations from stale epochs.</p>
</li>
<li>
<p><strong>Pre-vote confirmation</strong>: Before taking action, verify you still hold the lock (though this has a TOCTOU race)</p>
</li>
<li>
<p><strong>Lease-based design</strong>: Structure operations so that anything started must complete within the lease time, with no lingering effects beyond the lease</p>
</li>
<li>
<p><strong>Idempotent operations</strong>: Design leader operations to be safely re-executed, so duplicate leadership is harmless</p>
</li>
</ol>
<div>
<p><strong>Level 3 Follow-up: How does Raft handle leader election, and why is it more robust than lock-based approaches?</strong></p>
<p>Raft leader election differs fundamentally from lock-based approaches:</p>
<ol>
<li>
<p><strong>Term numbers as epochs</strong>: Each election increments the term. All messages include the term, and nodes reject messages from old terms. This is built into every operation, not just leadership acquisition.</p>
</li>
<li>
<p><strong>Voting with log comparison</strong>: A node only votes for candidates whose log is at least as up-to-date as its own. This prevents electing leaders with missing data.</p>
</li>
<li>
<p><strong>Majority requirement</strong>: A leader must continuously communicate with a majority. If it can only reach a minority (network partition), it knows it might not be the real leader and stops accepting writes.</p>
</li>
<li>
<p><strong>No external coordination</strong>: Raft is self-contained - nodes coordinate directly without a separate lock service. The consensus mechanism IS the lock.</p>
</li>
<li>
<p><strong>Automatic step-down</strong>: If a leader discovers a higher term exists, it immediately becomes a follower. There's no window where two nodes think they're leader.</p>
</li>
</ol>
<p>For critical systems like databases (CockroachDB, TiDB) or coordination services (etcd, Consul), this built-in leader election is far more robust than bolting on a separate lock service.</p>
</div>
</div>
</div>
<hr />
<h3 id="q5-when-should-you-not-use-distributed-locking">Q5: When should you NOT use distributed locking?</h3>
<p><strong>Level 1 Answer:</strong><br />
Avoid distributed locks when: <span>optimistic concurrency</span> works (use version numbers), operations can be made <span>idempotent</span>, data can be partitioned to avoid cross-shard coordination, or when <a href="/topic/system-design/event-sourcing">[event sourcing]</a> can eliminate conflicts entirely.</p>
<div>
<pre><code>**Level 2 Follow-up: Explain how optimistic concurrency control compares to distributed locking.**
</code></pre>
<p><span>Optimistic Concurrency Control (OCC)</span> assumes conflicts are rare and detects them at write time rather than preventing them upfront:</p>
<pre><code>| Aspect | Distributed Locking | OCC |
|--------|-------------------|-----|
| Philosophy | Pessimistic - prevent conflicts | Optimistic - detect conflicts |
| Contention handling | Block waiting | Retry on conflict |
| Read performance | Locks may block reads | Reads never blocked |
| Write performance | Guaranteed success if locked | May need retries |
| Scalability | Limited by lock service | Better horizontal scaling |
| Implementation | External coordination required | Version column in database |

OCC works well when:
- Read-heavy workloads (no lock overhead on reads)
- Low conflict rate (few retries needed)
- Short transactions (less chance of conflict)
</code></pre>
<div>
<p><strong>Level 3 Follow-up: What about CRDTs? When would they replace both locking and OCC?</strong></p>
<p><span>CRDTs (Conflict-free Replicated Data Types)</span> eliminate coordination entirely by making all operations commutative:</p>
<p><strong>How CRDTs work:</strong><br />
- Data structures designed so concurrent updates merge automatically<br />
- No locks, no conflicts, no coordination needed<br />
- Examples: G-Counter (grow-only counter), LWW-Register (last-write-wins), OR-Set (observed-remove set)</p>
<p><strong>When to use CRDTs:</strong><br />
- Multi-master replication where all replicas accept writes<br />
- Offline-first applications needing eventual sync<br />
- Collaborative editing (like Google Docs)<br />
- High-availability systems that can't afford coordination latency</p>
<p><strong>Trade-offs:</strong><br />
- Limited operation types (can't do arbitrary read-modify-write)<br />
- Eventually consistent (no linearizability)<br />
- Memory overhead for tracking causality<br />
- Some operations impossible (like strong decrement on counter)</p>
<p><strong>Real-world example</strong>: Rather than using locks for &quot;like&quot; counts, use a G-Counter CRDT. Each server maintains its own count, and counts merge by taking the max per server. You get approximate real-time counts with zero coordination, perfect for social media scale.</p>
<p>The hierarchy of preference:</p>
<ol>
<li>Design to not need coordination (CRDTs, idempotency)</li>
<li>Use optimistic concurrency (version numbers)</li>
<li>Use distributed locking (when neither above works)</li>
</ol>
</div>
</div>
</div>
<hr />
<h3 id="q6-how-do-you-choose-the-right-ttl-for-a-distributed-lock">Q6: How do you choose the right TTL for a distributed lock?</h3>
<p><strong>Level 1 Answer:</strong><br />
TTL should be long enough to complete the protected operation, but short enough to recover from failures quickly. A common formula: TTL = 2-3x expected operation time, accounting for network latency and clock drift.</p>
<div>
<p><strong>Level 2 Follow-up: What happens if your operation takes longer than expected?</strong></p>
<p>If an operation exceeds TTL, you have several options:</p>
<ol>
<li>
<p><strong>Heartbeat renewal</strong>: Background thread extends TTL periodically (e.g., every TTL/3). But if the renewal fails (network issue), you must immediately stop work.</p>
</li>
<li>
<p><strong>Validity checking</strong>: Before each step, check remaining validity time. If insufficient for next step, abort and retry.</p>
</li>
<li>
<p><strong>Staged locking</strong>: Break operation into stages, acquire/release locks for each stage instead of one long lock.</p>
</li>
<li>
<p><strong>Timeout on operation</strong>: Set operation timeout to less than lock TTL, fail fast if exceeded.</p>
</li>
</ol>
<p>The critical insight: if your lock expires, you MUST stop working immediately. Any work after expiry is unsafe unless protected by fencing tokens.</p>
<div>
<pre><code>**Level 3 Follow-up: How do you handle GC pauses that can be longer than any reasonable TTL?**

GC pauses (especially in Java) can be hundreds of milliseconds to seconds, making any TTL-based safety guarantee questionable. Strategies:

1. **Fencing tokens are mandatory**: Since you can't prevent GC pauses, ensure the storage layer rejects stale operations. This is your only true safety guarantee.

2. **GC tuning**: Use low-pause collectors (G1, ZGC, Shenandoah), tune heap sizes, avoid full GCs in production.

3. **Pre-operation validation**: After any potentially-pausing operation (like memory allocation), check lock validity before proceeding.

4. **Containerized resource limits**: In Kubernetes, set memory limits to prevent JVM from competing with other processes, reducing GC pressure.

5. **Consider non-GC languages**: For latency-critical lock holders, Rust or Go (with shorter GC pauses) may be more appropriate than Java.

6. **Design for idempotency**: Accept that safety violations might occur, but design operations so that duplicate execution is harmless.

The philosophical point: in distributed systems, you can't achieve perfect safety. You minimize risk through defense-in-depth: TTLs, fencing tokens, idempotency, and careful operation design. Assuming any single mechanism is sufficient leads to production incidents.
</code></pre>
</div>
</div>
</div>
</div>
<hr />
<h2 id="common-mistakes">Common Mistakes</h2>
<div>
<h4>Distributed Locking Anti-Patterns</h4>
<div>
<div>
<div>Using locks without TTL</div>
<div>If the lock holder crashes, the lock is held forever. Always set a TTL and handle lock expiration gracefully.</div>
</div>
<div>
<div>Releasing locks unconditionally</div>
<div>Always use compare-and-delete. Otherwise, you might release a lock that another client acquired after yours expired.</div>
</div>
<div>
<div>Ignoring fencing tokens</div>
<div>The lock alone doesn't guarantee safety. Resources must validate fencing tokens to reject stale writes from expired locks.</div>
</div>
<div>
<div>Single Redis node for critical operations</div>
<div>Redis failover can cause lock data loss. Use Redlock with multiple masters or consensus-based systems for true safety.</div>
</div>
<div>
<div>Lock contention as a design pattern</div>
<div>If many clients are waiting for the same lock, your system will be slow. Redesign to reduce contention through partitioning or different patterns.</div>
</div>
<div>
<div>Holding locks during external calls</div>
<div>Network calls can take unpredictably long. Keep critical sections short and release locks before calling external services.</div>
</div>
<div>
<div>Not monitoring lock acquisition time</div>
<div>High lock acquisition latency indicates contention. Monitor P99 acquisition times and alert when they exceed thresholds.</div>
</div>
</div>
</div>
<hr />
<h2 id="quick-reference-card">Quick Reference Card</h2>
<div>
<div>DISTRIBUTED LOCKING CHEAT SHEET</div>
<div>
<div>
<div>Lock Service Comparison</div>
<div>
<div><span>Redis (single)</span>: Fast, simple, NOT safe for correctness</div>
<div><span>Redlock</span>: Better safety, requires 5+ nodes, efficiency only</div>
<div><span>ZooKeeper</span>: Strong consistency, built-in fencing, complex ops</div>
<div><span>etcd</span>: Raft consensus, Kubernetes native, lease-based</div>
</div>
</div>
<div>
<div>TTL Guidelines</div>
<div>
<div><span>Formula</span>: TTL = 2x (operation + latency + drift)</div>
<div><span>Short ops</span>: 10-30 seconds</div>
<div><span>Long ops</span>: Use heartbeat extension</div>
<div><span>Extension interval</span>: TTL / 3</div>
</div>
</div>
<div>
<div>Safety Checklist</div>
<div>
<div>Unique token per acquisition</div>
<div>TTL on all locks</div>
<div>Compare-and-delete on release</div>
<div>Fencing tokens validated by resources</div>
<div>Majority quorum for critical ops</div>
</div>
</div>
<div>
<div>Alternatives to Locks</div>
<div>
<div><span>Optimistic locking</span>: Version numbers, ETags</div>
<div><span>Idempotency</span>: Unique request IDs</div>
<div><span>Event sourcing</span>: Append-only logs</div>
<div><span>CRDTs</span>: Conflict-free data types</div>
</div>
</div>
</div>
<div>
<div>Decision Tree</div>
<div>
<div>1. Can you avoid coordination? -> Use CRDTs or idempotent design</div>
<div>2. Is conflict rate low? -> Use optimistic concurrency</div>
<div>3. Is this efficiency-only? -> Redlock is acceptable</div>
<div>4. Is correctness critical? -> Use ZooKeeper/etcd + fencing tokens</div>
</div>
</div>
</div>
<hr />
<h2 id="related-topics">Related Topics</h2>
<ul>
<li><a href="/topic/system-design/consensus-algorithms">[Consensus Algorithms]</a> - Raft, Paxos, Zab for true safety guarantees</li>
<li><a href="/topic/system-design/database-sharding">[Database Sharding]</a> - Partitioning to reduce lock contention</li>
<li><a href="/topic/system-design/event-sourcing">[Event Sourcing]</a> - Lock-free alternative pattern</li>
<li><a href="/topic/system-design/cap-theorem">[CAP Theorem]</a> - Understanding consistency trade-offs</li>
<li><a href="/topic/system-design/database-replication">[Database Replication]</a> - Optimistic concurrency and versioning</li>
<li><a href="/topic/system-design/rate-limiting">[Rate Limiting]</a> - Global limits using distributed coordination</li>
</ul>
