<h1 id="caching">Caching</h1>
<div class="tldr-box">
    <div class="tldr-header">TL;DR</div>
    <ul class="tldr-list">
        <li>Caching stores frequently accessed data in fast memory (100-1000x faster than disk/network)</li>
        <li>Common strategies: Cache-Aside (lazy load), Write-Through (sync), Write-Behind (async)</li>
        <li>Eviction policies: LRU (most popular), LFU, FIFO, TTL-based</li>
        <li>Prevent stampede: use jittered TTLs, locking, or stale-while-revalidate</li>
        <li>Best for read-heavy workloads (10:1 read/write ratio) with acceptable staleness</li>
    </ul>
</div>
<div class="concept-section type-definition">
<h2 id="overview">Overview</h2>
<p>Caching is a technique that stores copies of frequently accessed data in a faster storage layer (like memory) to reduce latency and decrease load on the primary data source. Think of it as keeping your most-used tools on your desk instead of walking to the storage room every time you need them.</p>
</div>
<hr />
<div class="concept-section type-important">
<h2 id="why-this-matters">Why This Matters</h2>
<div>
<h4>Real Company Examples</h4>
<div>
<div>
<div>Netflix</div>
<div>Caches movie metadata and thumbnails at edge servers. Reduced origin requests by 95% and serves 200+ million users globally.</div>
</div>
<div>
<div>Facebook</div>
<div>Uses Memcached clusters caching 75% of all reads. Handles billions of requests per second with sub-millisecond latency.</div>
</div>
<div>
<div>Amazon</div>
<div>Every 100ms of latency costs 1% in sales. Caching product pages and recommendations saves millions in revenue.</div>
</div>
</div>
</div>
<p><strong>Why caching is essential:</strong></p>
<ul>
<li><strong>Speed</strong>: Memory access is ~100x faster than disk, ~1000x faster than network</li>
<li><strong>Cost Reduction</strong>: Serve more requests with fewer database servers</li>
<li><strong>Scalability</strong>: Handle traffic spikes without scaling expensive backend resources</li>
<li><strong>User Experience</strong>: Users abandon sites that take more than 3 seconds to load</li>
</ul>
</div>
<hr />
<div class="concept-section type-definition">
<h2 id="core-concepts">Core Concepts</h2>
<h3 id="the-library-analogy">The Library Analogy</h3>
<div>
<div>
<p>Imagine a <strong>university library</strong>:</p>
<div>
<div>
<div>Without Caching</div>
<ul>
<li>Every student walks to the archive basement</li>
<li>Finds the book in the catalog</li>
<li>Retrieves it from storage</li>
<li>Walks back to their desk</li>
<li><strong>Time: 10 minutes per book</strong></li>
</ul>
</div>
<div>
<div>With Caching</div>
<ul>
<li>Popular books kept on a "reserve shelf" near entrance</li>
<li>Students check reserve shelf first</li>
<li>If found (cache hit): grab and go</li>
<li>If not found (cache miss): go to archive</li>
<li><strong>Time: 30 seconds (hit) or 10 min (miss)</strong></li>
</ul>
</div>
</div>
</div>
</div>
<h3 id="cache-terminology">Cache Terminology</h3>
<table>
<thead>
<tr>
<th>Term</th>
<th>Library Analogy</th>
<th>Technical Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Cache Hit</strong></td>
<td>Book found on reserve shelf</td>
<td>Data found in cache</td>
</tr>
<tr>
<td><strong>Cache Miss</strong></td>
<td>Book not on shelf, go to archive</td>
<td>Data not in cache, fetch from source</td>
</tr>
<tr>
<td><strong>TTL</strong></td>
<td>Books returned to archive after 7 days</td>
<td>Time before cached data expires</td>
</tr>
<tr>
<td><strong>Eviction</strong></td>
<td>Remove least-used books when shelf is full</td>
<td>Remove data when cache is full</td>
</tr>
<tr>
<td><strong>Invalidation</strong></td>
<td>Remove outdated edition when new one arrives</td>
<td>Remove stale data after update</td>
</tr>
</tbody>
</table>
</div>
<hr />
<div class="concept-section type-definition">
<h2 id="how-it-works">How It Works</h2>
<h3 id="cache-hierarchy">Cache Hierarchy</h3>
<div>
<h4>THE CACHING PYRAMID</h4>
<div>
<div>
<div>Browser</div>
<div>~0ms</div>
</div>
<div>|</div>
<div>
<div>CDN Edge</div>
<div>~20ms</div>
</div>
<div>|</div>
<div>
<div>Application Cache (Redis)</div>
<div>~1ms</div>
</div>
<div>|</div>
<div>
<div>Database Query Cache</div>
<div>~10ms</div>
</div>
<div>|</div>
<div>
<div>Database (Disk)</div>
<div>~50-100ms</div>
</div>
</div>
<div>
    Faster at top, more capacity at bottom
</div>
</div>
<h3 id="caching-strategies">Caching Strategies</h3>
<div>
<h4>CACHE-ASIDE (LAZY LOADING)</h4>
<div>
<div>
<div>Application</div>
</div>
<div>
<div>1. Check</div>
<div>--></div>
</div>
<div>
<div>Cache</div>
<div>Hit? Return!</div>
</div>
<div>
<div>2. Miss?</div>
<div>--></div>
</div>
<div>
<div>Database</div>
</div>
</div>
<div>
    3. Store result in cache for next time
</div>
</div>
<h3 id="strategy-comparison">Strategy Comparison</h3>
<div>
<div>
<table>
  <thead>
<tr>
<th>Strategy</th>
<th>Read Path</th>
<th>Write Path</th>
<th>Best For</th>
</tr>
  </thead>
  <tbody>
<tr>
<td>Cache-Aside</td>
<td>App checks cache, then DB</td>
<td>Update DB, invalidate cache</td>
<td>General purpose</td>
</tr>
<tr>
<td>Write-Through</td>
<td>Read from cache</td>
<td>Write to cache AND DB together</td>
<td>Strong consistency needed</td>
</tr>
<tr>
<td>Write-Behind</td>
<td>Read from cache</td>
<td>Write to cache, async DB write</td>
<td>Write-heavy workloads</td>
</tr>
<tr>
<td>Read-Through</td>
<td>Cache fetches from DB on miss</td>
<td>N/A</td>
<td>Simplified read logic</td>
</tr>
  </tbody>
</table>
</div>
</div>
<h3 id="cache-eviction-policies">Cache Eviction Policies</h3>
<div>
<h4>WHEN THE CACHE IS FULL, WHO GETS EVICTED?</h4>
<div>
<div>
<div>LRU (Least Recently Used)</div>
<div>Evict item not accessed longest. Most popular choice.</div>
<div>"Haven't used this in ages? Out!"</div>
</div>
<div>
<div>LFU (Least Frequently Used)</div>
<div>Evict item accessed fewest times overall.</div>
<div>"Only used twice ever? Goodbye!"</div>
</div>
<div>
<div>FIFO (First In First Out)</div>
<div>Evict oldest item regardless of usage.</div>
<div>"You were here first, now leave first."</div>
</div>
<div>
<div>TTL (Time To Live)</div>
<div>Items expire after set time period.</div>
<div>"Your time is up!"</div>
</div>
</div>
</div>
</div>
<hr />
<div class="concept-section type-warning">
<h2 id="real-life-failure-story">Real-Life Failure Story</h2>
<div>
<h4>The Facebook Cache Stampede (2010)</h4>
<div>
<p><strong>What Happened:</strong> Facebook experienced a major outage when a bug caused their entire Memcached cluster to invalidate simultaneously. When cache entries expired at the same time:</p>
<ol>
<li>Millions of requests found empty caches (cache miss)</li>
<li>All requests hit the database simultaneously</li>
<li>Database servers were overwhelmed and crashed</li>
<li>Even after restart, the stampede repeated</li>
</ol>
<p><strong>The Fix:</strong></p>
<ul>
<li><strong>Jittered TTLs:</strong> Added random variation to expiration times (e.g., 3600s +/- 300s)</li>
<li><strong>Locking:</strong> Only one request regenerates cache, others wait</li>
<li><strong>Stale-while-revalidate:</strong> Serve stale data while refreshing in background</li>
</ul>
<p><strong>Lesson:</strong> Never let cache entries expire at the same time. Add randomization to everything in distributed systems.</p>
</div>
</div>
</div>
<hr />
<div class="concept-section type-warning">
<h2 id="what-to-watch-out-for">What to Watch Out For</h2>
<div>
<h4>Common Pitfalls</h4>
<div>
<div>
<div>Cache Stampede</div>
<div>Many requests hit database when cache expires. Use locking or jittered TTLs.</div>
</div>
<div>
<div>Stale Data</div>
<div>Cache shows outdated information. Implement proper invalidation or use short TTLs.</div>
</div>
<div>
<div>Cache Penetration</div>
<div>Queries for non-existent data always miss cache. Cache negative results too.</div>
</div>
<div>
<div>Hot Key Problem</div>
<div>One popular key overwhelms a single cache node. Replicate hot keys or use local caching.</div>
</div>
<div>
<div>Memory Pressure</div>
<div>Cache grows unbounded. Set memory limits and monitor eviction rates.</div>
</div>
</div>
</div>
</div>
<hr />
<div class="concept-section type-important">
<h2 id="interview-deep-dive">Interview Deep Dive</h2>
<div>
<h4>Common Interview Questions</h4>
<div>
<div>Q: How do you handle cache invalidation in a distributed system?</div>
<div>
<strong>A:</strong> Multiple approaches: (1) <strong>TTL-based</strong> - set expiration, accept some staleness. (2) <strong>Event-driven</strong> - publish invalidation events via Kafka/Redis Pub-Sub when data changes. (3) <strong>Version-based</strong> - include version in cache key, increment on updates. For strong consistency, use write-through with distributed locks.
</div>
</div>
<div>
<div>Q: Redis vs Memcached - when would you choose each?</div>
<div>
<strong>A:</strong> <strong>Redis</strong> when you need: data structures (lists, sets, sorted sets), persistence, pub/sub, Lua scripting, replication. <strong>Memcached</strong> when you need: simple key-value, multi-threaded performance, less memory overhead per key, or already have it in your stack. Redis is more versatile; Memcached is simpler and slightly faster for basic operations.
</div>
</div>
<div>
<div>Q: How do you prevent cache stampede?</div>
<div>
<strong>A:</strong> (1) <strong>Locking</strong> - only one request regenerates, others wait or return stale. (2) <strong>Probabilistic early expiration</strong> - randomly refresh before TTL. (3) <strong>Background refresh</strong> - async job refreshes popular keys. (4) <strong>Jittered TTLs</strong> - add randomness to prevent synchronized expiry.
</div>
</div>
<div>
<div>Q: When should you NOT use caching?</div>
<div>
<strong>A:</strong> (1) <strong>Highly dynamic data</strong> that changes every request. (2) <strong>Strong consistency requirements</strong> where stale reads are unacceptable (financial transactions). (3) <strong>Low-traffic endpoints</strong> where cache hit rate would be low. (4) <strong>Large, unique datasets</strong> that don't fit in memory. (5) <strong>Security-sensitive data</strong> that shouldn't persist outside the database.
</div>
</div>
</div>
</div>
<hr />
<div class="concept-section type-example">
<h2 id="code-implementation">Code Implementation</h2>
<h3 id="python---production-cache-with-stampede-prevention">Python - Production Cache with Stampede Prevention</h3>
<pre><code class="language-python">import time
import threading
import hashlib
import random
from typing import Optional, Callable, Any
from dataclasses import dataclass

@dataclass
class CacheEntry:
    value: Any
    expires_at: float
    created_at: float

class ProductionCache:
    &quot;&quot;&quot;
    Production-ready cache with:
    - TTL with jitter (prevents stampede)
    - Locking for cache regeneration
    - Stale-while-revalidate
    - Cache statistics
    &quot;&quot;&quot;

    def __init__(self, max_size: int = 10000, default_ttl: int = 3600):
        self.cache: dict[str, CacheEntry] = {}
        self.max_size = max_size
        self.default_ttl = default_ttl
        self.locks: dict[str, threading.Lock] = {}
        self.lock_mutex = threading.Lock()

        # Statistics
        self.hits = 0
        self.misses = 0

    def _get_lock(self, key: str) -&gt; threading.Lock:
        &quot;&quot;&quot;Get or create a lock for a specific key.&quot;&quot;&quot;
        with self.lock_mutex:
            if key not in self.locks:
                self.locks[key] = threading.Lock()
            return self.locks[key]

    def _add_jitter(self, ttl: int) -&gt; float:
        &quot;&quot;&quot;Add 10% random jitter to TTL.&quot;&quot;&quot;
        jitter = ttl * 0.1 * random.random()
        return ttl + jitter

    def get(self, key: str) -&gt; Optional[Any]:
        &quot;&quot;&quot;Get value from cache.&quot;&quot;&quot;
        entry = self.cache.get(key)

        if entry is None:
            self.misses += 1
            return None

        if time.time() &gt; entry.expires_at:
            self.misses += 1
            return None

        self.hits += 1
        return entry.value

    def set(self, key: str, value: Any, ttl: Optional[int] = None) -&gt; None:
        &quot;&quot;&quot;Set value in cache with jittered TTL.&quot;&quot;&quot;
        ttl = ttl or self.default_ttl
        jittered_ttl = self._add_jitter(ttl)

        # Evict if at capacity (simple LRU would be better)
        if len(self.cache) &gt;= self.max_size:
            oldest_key = min(self.cache, key=lambda k: self.cache[k].created_at)
            del self.cache[oldest_key]

        self.cache[key] = CacheEntry(
            value=value,
            expires_at=time.time() + jittered_ttl,
            created_at=time.time()
        )

    def get_or_set(
        self,
        key: str,
        loader: Callable[[], Any],
        ttl: Optional[int] = None,
        stale_ttl: int = 60
    ) -&gt; Any:
        &quot;&quot;&quot;
        Get from cache or load with stampede prevention.

        Uses locking to ensure only one request loads data.
        Others wait or return stale data.
        &quot;&quot;&quot;
        entry = self.cache.get(key)
        now = time.time()

        # Fresh cache hit
        if entry and now &lt; entry.expires_at:
            self.hits += 1
            return entry.value

        # Stale data available?
        stale_value = entry.value if entry else None
        stale_available = entry and now &lt; entry.expires_at + stale_ttl

        # Try to acquire lock
        lock = self._get_lock(key)
        acquired = lock.acquire(blocking=not stale_available)

        if not acquired:
            # Couldn't get lock, return stale if available
            self.hits += 1  # Serving stale
            return stale_value

        try:
            # Double-check after acquiring lock
            entry = self.cache.get(key)
            if entry and time.time() &lt; entry.expires_at:
                return entry.value

            # Load fresh data
            self.misses += 1
            value = loader()
            self.set(key, value, ttl)
            return value
        finally:
            lock.release()

    def invalidate(self, key: str) -&gt; bool:
        &quot;&quot;&quot;Remove key from cache.&quot;&quot;&quot;
        if key in self.cache:
            del self.cache[key]
            return True
        return False

    def invalidate_pattern(self, pattern: str) -&gt; int:
        &quot;&quot;&quot;Invalidate all keys matching pattern (simple prefix match).&quot;&quot;&quot;
        keys_to_delete = [k for k in self.cache if k.startswith(pattern)]
        for key in keys_to_delete:
            del self.cache[key]
        return len(keys_to_delete)

    def stats(self) -&gt; dict:
        &quot;&quot;&quot;Get cache statistics.&quot;&quot;&quot;
        total = self.hits + self.misses
        return {
            &quot;hits&quot;: self.hits,
            &quot;misses&quot;: self.misses,
            &quot;hit_rate&quot;: self.hits / total if total &gt; 0 else 0,
            &quot;size&quot;: len(self.cache),
            &quot;max_size&quot;: self.max_size
        }


# Usage Example
cache = ProductionCache(max_size=10000, default_ttl=3600)

def get_user(user_id: int) -&gt; dict:
    &quot;&quot;&quot;Get user with caching.&quot;&quot;&quot;
    cache_key = f&quot;user:{user_id}&quot;

    def load_from_db():
        # Simulate database query
        return {&quot;id&quot;: user_id, &quot;name&quot;: f&quot;User {user_id}&quot;}

    return cache.get_or_set(cache_key, load_from_db, ttl=300)

# First call - cache miss, loads from DB
user = get_user(123)

# Second call - cache hit
user = get_user(123)

# Check stats
print(cache.stats())
# {'hits': 1, 'misses': 1, 'hit_rate': 0.5, 'size': 1, 'max_size': 10000}
</code></pre>
<h3 id="python---distributed-cache-with-redis">Python - Distributed Cache with Redis</h3>
<pre><code class="language-python">import redis
import json
import time
from typing import Optional, Any, Callable

class RedisCache:
    &quot;&quot;&quot;Redis-backed distributed cache.&quot;&quot;&quot;

    def __init__(self, host: str = 'localhost', port: int = 6379, db: int = 0):
        self.client = redis.Redis(host=host, port=port, db=db, decode_responses=True)
        self.default_ttl = 3600

    def get(self, key: str) -&gt; Optional[Any]:
        &quot;&quot;&quot;Get value from Redis.&quot;&quot;&quot;
        value = self.client.get(key)
        if value:
            return json.loads(value)
        return None

    def set(self, key: str, value: Any, ttl: Optional[int] = None) -&gt; None:
        &quot;&quot;&quot;Set value in Redis with TTL.&quot;&quot;&quot;
        ttl = ttl or self.default_ttl
        self.client.setex(key, ttl, json.dumps(value))

    def get_or_set(
        self,
        key: str,
        loader: Callable[[], Any],
        ttl: Optional[int] = None
    ) -&gt; Any:
        &quot;&quot;&quot;Get from cache or load with distributed locking.&quot;&quot;&quot;
        # Try cache first
        value = self.get(key)
        if value is not None:
            return value

        # Distributed lock using SETNX
        lock_key = f&quot;lock:{key}&quot;
        lock_acquired = self.client.setnx(lock_key, &quot;1&quot;)

        if lock_acquired:
            self.client.expire(lock_key, 30)  # Lock timeout
            try:
                value = loader()
                self.set(key, value, ttl)
                return value
            finally:
                self.client.delete(lock_key)
        else:
            # Wait and retry
            time.sleep(0.1)
            return self.get(key) or loader()

    def invalidate(self, key: str) -&gt; bool:
        &quot;&quot;&quot;Delete key from Redis.&quot;&quot;&quot;
        return self.client.delete(key) &gt; 0

    def invalidate_pattern(self, pattern: str) -&gt; int:
        &quot;&quot;&quot;Delete keys matching pattern.&quot;&quot;&quot;
        keys = self.client.keys(pattern)
        if keys:
            return self.client.delete(*keys)
        return 0
</code></pre>
</div>
<hr />
<h2 id="quick-reference-card">Quick Reference Card</h2>
<div>
<h4>CACHING CHEAT SHEET</h4>
<div>
<div>
<div>When to Cache</div>
<ul>
<li>Read-heavy workloads (read:write> 10:1)</li>
<li>Expensive computations</li>
<li>Slow external API calls</li>
<li>Database query results</li>
<li>Session data</li>
</ul>
</div>
<div>
<div>When NOT to Cache</div>
<ul>
<li>Rapidly changing data</li>
<li>Write-heavy workloads</li>
<li>Unique queries (low hit rate)</li>
<li>Sensitive financial data</li>
<li>Real-time requirements</li>
</ul>
</div>
<div>
<div>TTL Guidelines</div>
<ul>
<li>Static assets: 1 year</li>
<li>User profiles: 1 hour</li>
<li>API responses: 5-15 minutes</li>
<li>Search results: 1-5 minutes</li>
<li>Real-time data: 10-30 seconds</li>
</ul>
</div>
<div>
<div>Key Metrics</div>
<ul>
<li>Hit rate> 90% (for hot data)</li>
<li>Latency p99 < 10ms</li>
<li>Memory usage < 80%</li>
<li>Eviction rate (should be low)</li>
<li>Connection count</li>
</ul>
</div>
</div>
<div>
<code>Cache Rule: "Cache data that is read often, written rarely, and can tolerate some staleness"</code>
</div>
</div>
<hr />
<h2 id="related-topics">Related Topics</h2>
<ul>
<li><a href="/topic/system-design/cdn">CDN</a> - Edge caching for static content</li>
<li><a href="/topic/system-design/database-replication">Database Replication</a> - Data redundancy</li>
<li><a href="/topic/system-design/load-balancing">Load Balancing</a> - Request distribution</li>
<li><a href="/topic/system-design/redis">Redis</a> - Popular caching solution</li>
</ul>
