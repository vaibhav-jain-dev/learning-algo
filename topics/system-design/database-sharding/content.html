<h1 id="database-sharding">Database Sharding</h1>
<div class="tldr-box">
    <div class="tldr-header">TL;DR</div>
    <ul class="tldr-list">
        <li>Sharding splits large databases horizontally across multiple servers (shards)</li>
        <li>Strategies: Range-based (age groups), Hash-based (userId % N), Geographic (region)</li>
        <li>Shard key choice is critical - poor keys cause hotspots and difficult resharding</li>
        <li>Challenges: cross-shard queries, distributed transactions, data rebalancing</li>
        <li>Use when single database can't handle load (usually 100GB+ or 100k+ QPS)</li>
    </ul>
</div>
<div class="concept-section type-definition">
<h2 id="overview">Overview</h2>
<p><span>Database sharding</span> is a horizontal scaling technique that partitions data across multiple database instances, where each instance (shard) holds a subset of the total data. Unlike <a href="/topic/system-design/database-replication">[database-replication]</a> which copies the same data everywhere, sharding divides data so each shard is responsible for different records.</p>
<p>Think of it like organizing a massive library: instead of one overwhelming building with millions of books, you create multiple specialized libraries - one for fiction, one for science, one for history. Each library is manageable on its own, and you just need to know which library to visit for your topic.</p>
<div>
<p><strong>The Core Insight</strong>: A single database has hard limits - disk space, CPU, memory, network bandwidth, and connection count. Sharding breaks through these limits by distributing data horizontally across multiple machines, enabling <span>linear scalability</span> for both storage and throughput.</p>
</div>
</div>
<hr />
<div class="concept-section type-definition">
<h2 id="visual-architecture">Visual Architecture</h2>
<div>
<div>SHARDED DATABASE ARCHITECTURE</div>
<div>
    <!-- Application Layer -->
<div>
<div>Application Layer</div>
<div>Query: SELECT * FROM users WHERE user_id = 12345</div>
</div>
<div>↓</div>
<pre><code>&lt;!-- Shard Router --&gt;
</code></pre>
<div>
<div>Shard Router / Coordinator</div>
<div>hash(12345) % 4 = 1 → Route to Shard 1</div>
</div>
<div>
<span>↙</span><span>↓</span><span>↓</span><span>↘</span>
</div>
<pre><code>&lt;!-- Shards --&gt;
</code></pre>
<div>
<div>
<div>Shard 0</div>
<div>Users 0-249K</div>
<div>Primary + 2 Replicas</div>
</div>
<div>
<div>Shard 1</div>
<div>Users 250K-499K</div>
<div>← Query routed here</div>
</div>
<div>
<div>Shard 2</div>
<div>Users 500K-749K</div>
<div>Primary + 2 Replicas</div>
</div>
<div>
<div>Shard 3</div>
<div>Users 750K-1M</div>
<div>Primary + 2 Replicas</div>
</div>
</div>
</div>
<div>
<span>Each shard operates independently - 4x write throughput, 4x storage capacity</span>
</div>
</div>
<hr />
<h2 id="horizontal-vs-vertical-sharding">Horizontal vs Vertical Sharding</h2>
<p>Understanding the difference between <span>horizontal sharding</span> (partitioning rows) and <span>vertical sharding</span> (partitioning columns) is fundamental to designing scalable database architectures.</p>
<div>
<div>HORIZONTAL vs VERTICAL SHARDING</div>
<div>
    <!-- Horizontal Sharding -->
<div>
<div>HORIZONTAL SHARDING</div>
<div>Split by ROWS</div>
<div>
<div>Original Table</div>
<div>
  | id | name | email | orders |
</div>
</div>
<div>↓ Split by user_id ↓</div>
<div>
<div>
<div>Shard A</div>
<div>Users 1-1000</div>
<div>All columns</div>
</div>
<div>
<div>Shard B</div>
<div>Users 1001-2000</div>
<div>All columns</div>
</div>
</div>
<div>
<div>✓ Scales writes linearly</div>
<div>✓ Each row is complete</div>
<div>✗ Cross-shard queries expensive</div>
</div>
</div>
<pre><code>&lt;!-- Vertical Sharding --&gt;
</code></pre>
<div>
<div>VERTICAL SHARDING</div>
<div>Split by COLUMNS</div>
<div>
<div>Original Table</div>
<div>
  | id | name | email | blob_data |
</div>
</div>
<div>↓ Split by column type ↓</div>
<div>
<div>
<div>Core DB</div>
<div>id, name, email</div>
<div>Fast queries</div>
</div>
<div>
<div>Blob Store</div>
<div>id, blob_data</div>
<div>Heavy data</div>
</div>
</div>
<div>
<div>✓ Separates hot/cold data</div>
<div>✓ Different storage tiers</div>
<div>✗ JOINs require network</div>
</div>
</div>
</div>
<div>
<span>In practice: Combine both! Vertically shard by domain, then horizontally shard hot tables.</span>
</div>
</div>
<h3 id="when-to-use-each">When to Use Each</h3>
<div>
<p><strong>Horizontal Sharding (Most Common)</strong>:</p>
<ul>
<li>When you have billions of rows in a single table</li>
<li>When write throughput exceeds single-machine capacity</li>
<li>When data naturally partitions by a key (user_id, tenant_id)</li>
<li>Examples: User data, transactions, social media posts</li>
</ul>
<p><strong>Vertical Sharding (Functional Partitioning)</strong>:</p>
<ul>
<li>When tables have very different access patterns</li>
<li>When some columns are accessed rarely but are large (BLOBs)</li>
<li>When you want to separate domains for microservices</li>
<li>Examples: User profiles vs user preferences, orders vs order_items</li>
</ul>
</div>
<hr />
<h2 id="why-this-matters-real-company-examples">Why This Matters: Real Company Examples</h2>
<div>
<div>
<div>
<div>Instagram - User Data Sharding</div>
<div>With 2+ billion users, Instagram shards by <span>user_id</span> across thousands of PostgreSQL instances. Each shard holds ~500K users. When you view a profile, the app calculates which shard to query: <code>shard_id = user_id % num_shards</code>. This enables independent scaling of the user graph.</div>
</div>
<div>
<div>Discord - Guild-Based Sharding</div>
<div>Discord shards messages by <span>guild_id</span> (server). This is brilliant because messages within a Discord server are always on the same shard - no cross-shard queries for conversation history. With 150M+ monthly users, each shard handles 10K-50K guilds independently.</div>
</div>
<div>
<div>Uber - Geographic Sharding</div>
<div>Uber shards trip data by <span>geographic region</span>. NYC trips hit different shards than San Francisco trips. This provides data locality (reduced latency) and failure isolation (NYC outage doesn't affect SF). They use a two-level sharding: city-level, then hash-based within city.</div>
</div>
<div>
<div>Slack - Workspace Sharding</div>
<div>Slack shards by <span>workspace_id</span>. Each company's Slack workspace lives on dedicated shards, providing data isolation (important for enterprise compliance), predictable performance, and simplified billing/quota management per tenant.</div>
</div>
</div>
</div>
<hr />
<h2 id="shard-key-selection">Shard Key Selection</h2>
<p>The <span>shard key</span> is the most critical decision in sharding. It determines how data is distributed and directly impacts query performance, data distribution, and operational complexity.</p>
<div>
<div>SHARD KEY SELECTION CRITERIA</div>
<div>
<div>
<div>1. High Cardinality</div>
<div>Many unique values enable even distribution. user_id (millions) is good; country (200) is bad.</div>
</div>
<div>
<div>2. Even Distribution</div>
<div>Values should spread uniformly. Random UUIDs good; sequential IDs cause hotspots on newest shard.</div>
</div>
<div>
<div>3. Query Alignment</div>
<div>Matches access patterns. If 90% of queries filter by user_id, shard by user_id.</div>
</div>
<div>
<div>4. Immutability</div>
<div>Value never changes. user_id is stable; email changes require data migration.</div>
</div>
</div>
</div>
<h3 id="good-vs-bad-shard-keys">Good vs Bad Shard Keys</h3>
<div>
<div>SHARD KEY DISTRIBUTION IMPACT</div>
  <!-- Good: user_id -->
<div>
<div>
<span>GOOD</span>
<span>user_id - High Cardinality, Uniform Distribution</span>
</div>
<div>
<div>
<span>25%</span>
</div>
<div>
<span>25%</span>
</div>
<div>
<span>25%</span>
</div>
<div>
<span>25%</span>
</div>
</div>
<div>Perfectly balanced - each shard gets equal load</div>
</div>
  <!-- Bad: country -->
<div>
<div>
<span>BAD</span>
<span>country - Low Cardinality, Skewed Distribution</span>
</div>
<div>
<div>
<span>US - 60% (HOTSPOT!)</span>
</div>
<div>
<span>UK</span>
</div>
<div>
<span>DE</span>
</div>
<div></div>
</div>
<div>Hotspot! US shard is overloaded while others are idle</div>
</div>
  <!-- Bad: created_at -->
<div>
<div>
<span>BAD</span>
<span>created_at - Temporal Hotspot</span>
</div>
<div>
<div>
<span>2022</span>
</div>
<div>
<span>2023</span>
</div>
<div>
<span>2024</span>
</div>
<div>
<span>2025 (ALL writes!)</span>
</div>
</div>
<div>All new data hits the latest shard - write bottleneck</div>
</div>
</div>
<h3 id="compound-shard-keys">Compound Shard Keys</h3>
<p>For complex access patterns, use <span>compound shard keys</span> that combine multiple fields:</p>
<pre><code class="language-python">def compute_compound_shard_key(tenant_id: str, user_id: str) -&gt; str:
    &quot;&quot;&quot;
    Compound shard key for multi-tenant SaaS.

    This enables:
    - All data for a tenant to be on same shard (tenant queries)
    - Even distribution within tenant (user queries)
    &quot;&quot;&quot;
    # First level: tenant determines shard cluster
    tenant_shard = hash(tenant_id) % NUM_SHARD_CLUSTERS

    # Second level: user determines shard within cluster
    user_shard = hash(user_id) % SHARDS_PER_CLUSTER

    return f&quot;cluster_{tenant_shard}_shard_{user_shard}&quot;

# Example: Slack's approach
# Workspace ID determines shard cluster (data isolation)
# User ID determines partition within cluster (performance)
</code></pre>
<hr />
<h2 id="sharding-strategies-deep-dive">Sharding Strategies Deep Dive</h2>
<div>
<div>SHARDING STRATEGIES COMPARISON</div>
<div>
<table>
  <thead>
<tr>
<th>Strategy</th>
<th>Distribution</th>
<th>Range Queries</th>
<th>Resharding</th>
<th>Complexity</th>
<th>Best For</th>
</tr>
  </thead>
  <tbody>
<tr>
<td>Range-Based</td>
<td><span>Uneven</span></td>
<td><span>Excellent</span></td>
<td><span>Medium</span></td>
<td><span>Low</span></td>
<td>Time-series, logs, analytics</td>
</tr>
<tr>
<td>Hash-Based</td>
<td><span>Even</span></td>
<td><span>Poor</span></td>
<td><span>Hard</span></td>
<td><span>Low</span></td>
<td>User data, key-value stores</td>
</tr>
<tr>
<td>Consistent Hash</td>
<td><span>Even</span></td>
<td><span>Poor</span></td>
<td><span>Easy</span></td>
<td><span>Medium</span></td>
<td>Dynamic scaling, caches</td>
</tr>
<tr>
<td>Directory-Based</td>
<td><span>Flexible</span></td>
<td><span>Good</span></td>
<td><span>Easy</span></td>
<td><span>High</span></td>
<td>Custom routing, multi-tenant</td>
</tr>
<tr>
<td>Geo-Based</td>
<td><span>Varies</span></td>
<td><span>Regional</span></td>
<td><span>Medium</span></td>
<td><span>Medium</span></td>
<td>Global apps, CDN-like</td>
</tr>
  </tbody>
</table>
</div>
</div>
<h3 id="1-range-based-sharding">1. Range-Based Sharding</h3>
<div>
<div>RANGE-BASED SHARDING FLOW</div>
<div>
<div>
<span>Query: user_id = 1,500,000</span>
</div>
<div>↓</div>
<div>
<div>Range Lookup Table</div>
<div>
<div>
<span>0 - 999,999</span>
<span>→ Shard 1</span>
</div>
<div>
<span>1,000,000 - 1,999,999</span>
<span>→ Shard 2 ✓</span>
</div>
<div>
<span>2,000,000 - 2,999,999</span>
<span>→ Shard 3</span>
</div>
<div>
<span>3,000,000+</span>
<span>→ Shard 4</span>
</div>
</div>
</div>
<div>↓</div>
<div>
<span>Execute on Shard 2</span>
</div>
</div>
<div>
<div>
<div>Advantages</div>
<div>Range queries are efficient (e.g., all orders from last week)</div>
</div>
<div>
<div>Watch Out</div>
<div>Sequential keys create hotspots on the "newest" shard</div>
</div>
</div>
</div>
<h3 id="2-hash-based-sharding">2. Hash-Based Sharding</h3>
<pre><code class="language-python">import hashlib

def get_shard_by_hash(key: str, num_shards: int) -&gt; int:
    &quot;&quot;&quot;
    Simple hash-based sharding.

    Pros: Even distribution regardless of key patterns
    Cons: Adding shards requires moving ~100% of data
    &quot;&quot;&quot;
    hash_value = int(hashlib.sha256(str(key).encode()).hexdigest(), 16)
    return hash_value % num_shards

# The problem with simple hashing:
# With 4 shards: hash(&quot;user_123&quot;) % 4 = 2
# With 5 shards: hash(&quot;user_123&quot;) % 5 = 3  &lt;- Different shard!
# Adding one shard moves ~80% of data (N-1/N)
</code></pre>
<h3 id="3-consistent-hashing-recommended">3. Consistent Hashing (Recommended)</h3>
<p><span>Consistent hashing</span> is the industry standard for dynamic sharding because it minimizes data movement when adding or removing shards.</p>
<div>
<div>CONSISTENT HASHING RING</div>
<div>
    Both keys and nodes hash to positions on a ring (0 to 2^32). Keys belong to the first node clockwise from their position.
</div>
<div>
<div>
<div>
<div>
<span>A</span>
</div>
<div>Position: 0°</div>
</div>
<div>→</div>
<div>
<div>
<span>B</span>
</div>
<div>Position: 90°</div>
</div>
<div>→</div>
<div>
<div>
<span>C</span>
</div>
<div>Position: 180°</div>
</div>
<div>→</div>
<div>
<div>
<span>D</span>
</div>
<div>Position: 270°</div>
</div>
<div>→ (back to A)</div>
</div>
</div>
<div>
<div>Key Lookup Example</div>
<div>
key "user_123" hashes to position <span>45°</span><br>
Walk clockwise → first node is <span>B (at 90°)</span><br>
  → Route to Node B
</div>
</div>
<div>
<div>Adding Node E at 67°</div>
<div>
<div><strong>Before:</strong> Keys 0°-90° → Node B</div>
<div><strong>After:</strong> Keys 0°-67° → <span>Node E (new)</span>, Keys 67°-90° → Node B</div>
<div>Only ~1/N of keys move (not all!)</div>
</div>
</div>
</div>
<pre><code>```python
import hashlib
from bisect import bisect_right
from typing import Optional, List

class ConsistentHashRing:
&quot;&quot;&quot;
Consistent hashing implementation with virtual nodes.

Virtual nodes improve distribution by placing multiple points
per physical node on the ring.
&quot;&quot;&quot;

def __init__(self, nodes: List[str] = None, virtual_nodes: int = 150):
self.virtual_nodes = virtual_nodes
self.ring: List[int] = []  # Sorted hash positions
self.hash_to_node: dict[int, str] = {}

for node in (nodes or []):
self.add_node(node)

def _hash(self, key: str) -&gt; int:
&quot;&quot;&quot;Hash a key to a position on the ring (0 to 2^32).&quot;&quot;&quot;
return int(hashlib.sha256(key.encode()).hexdigest(), 16) % (2**32)

def add_node(self, node: str) -&gt; None:
&quot;&quot;&quot;Add a node with virtual nodes for better distribution.&quot;&quot;&quot;
for i in range(self.virtual_nodes):
virtual_key = f&quot;{node}:vn{i}&quot;
hash_val = self._hash(virtual_key)
self.ring.append(hash_val)
self.hash_to_node[hash_val] = node
self.ring.sort()

def remove_node(self, node: str) -&gt; None:
&quot;&quot;&quot;Remove node - only its keys redistribute to next node.&quot;&quot;&quot;
for i in range(self.virtual_nodes):
virtual_key = f&quot;{node}:vn{i}&quot;
hash_val = self._hash(virtual_key)
self.ring.remove(hash_val)
del self.hash_to_node[hash_val]

def get_node(self, key: str) -&gt; Optional[str]:
&quot;&quot;&quot;Find the node responsible for this key.&quot;&quot;&quot;
if not self.ring:
return None

hash_val = self._hash(key)
idx = bisect_right(self.ring, hash_val)

# Wrap around if past the end of the ring
if idx == len(self.ring):
idx = 0

return self.hash_to_node[self.ring[idx]]

def get_nodes_for_key(self, key: str, replicas: int = 3) -&gt; List[str]:
&quot;&quot;&quot;Get multiple nodes for replication (walk clockwise).&quot;&quot;&quot;
if not self.ring or replicas &lt;= 0:
return []

hash_val = self._hash(key)
idx = bisect_right(self.ring, hash_val)

nodes = []
seen = set()

while len(nodes) &lt; replicas and len(seen) &lt; len(self.hash_to_node):
if idx &gt;= len(self.ring):
idx = 0

node = self.hash_to_node[self.ring[idx]]
if node not in seen:
nodes.append(node)
seen.add(node)
idx += 1

return nodes


# Usage
ring = ConsistentHashRing([&quot;shard1&quot;, &quot;shard2&quot;, &quot;shard3&quot;, &quot;shard4&quot;])

# Route a key
shard = ring.get_node(&quot;user:12345&quot;)  # -&gt; &quot;shard2&quot;

# Add new shard - only ~25% of keys move
ring.add_node(&quot;shard5&quot;)

# Get replication targets
replicas = ring.get_nodes_for_key(&quot;user:12345&quot;, replicas=3)  # -&gt; [&quot;shard2&quot;, &quot;shard3&quot;, &quot;shard4&quot;]
```

---
</code></pre>
<h2 id="cross-shard-queries">Cross-Shard Queries</h2>
<p>Cross-shard queries are one of the biggest challenges in sharded databases. When a query cannot be routed to a single shard, you need <span>scatter-gather</span> or other strategies.</p>
<div>
<div>SCATTER-GATHER PATTERN</div>
<div>
  <!-- Query -->
<div>
<span>Query without shard key:</span>
<span> SELECT * FROM orders WHERE total> 1000</span>
</div>
  <!-- Coordinator -->
<div>
<div>Coordinator</div>
<div>Manages query distribution</div>
</div>
  <!-- Scatter -->
<div>1. SCATTER (parallel fan-out)</div>
<div>
<span>↙</span><span>↓</span><span>↓</span><span>↘</span>
</div>
  <!-- Shards executing -->
<div>
<div>
<div>Shard 1</div>
<div>47 rows found</div>
<div>23ms</div>
</div>
<div>
<div>Shard 2</div>
<div>31 rows found</div>
<div>18ms</div>
</div>
<div>
<div>Shard 3</div>
<div>52 rows found</div>
<div>31ms</div>
</div>
<div>
<div>Shard 4</div>
<div>19 rows found</div>
<div>15ms</div>
</div>
</div>
  <!-- Gather -->
<div>
<span>↘</span><span>↓</span><span>↓</span><span>↙</span>
</div>
<div>2. GATHER (merge results)</div>
  <!-- Result -->
<div>
<div>149 rows merged</div>
<div>Total latency: 31ms (slowest shard) + 5ms (merge)</div>
</div>
</div>
<div>
<span>Performance Note:</span>
<span> Latency = max(shard latencies) + merge time. The slowest shard determines response time.</span>
</div>
</div>
<h3 id="cross-shard-query-strategies">Cross-Shard Query Strategies</h3>
<div>
<p><strong>1. Scatter-Gather</strong> (shown above)<br />
- Query all shards in parallel, merge results<br />
- Use for: Analytics, search, aggregations<br />
- Cost: N network calls, slowest shard dominates latency</p>
<p><strong>2. Global Secondary Index</strong><br />
- Maintain a separate index mapping query fields to shard locations<br />
- Use for: Frequent lookups by non-shard-key fields<br />
- Cost: Index maintenance overhead, storage</p>
<p><strong>3. Reference Tables</strong><br />
- Replicate small lookup tables (countries, categories) to all shards<br />
- Use for: JOINs with static reference data<br />
- Cost: Storage duplication, sync complexity</p>
<p><strong>4. Denormalization</strong><br />
- Store related data together on the same shard<br />
- Use for: Frequently joined data<br />
- Cost: Data duplication, update complexity</p>
<p><strong>5. Application-Level Joins</strong><br />
- Query each shard separately, join in application code<br />
- Use for: Complex joins that can't be avoided<br />
- Cost: Application complexity, memory usage</p>
</div>
<pre><code>```python
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any

class CrossShardQueryExecutor:
&quot;&quot;&quot;Execute queries across multiple shards with scatter-gather.&quot;&quot;&quot;

def __init__(self, shard_connections: Dict[str, Any]):
self.shards = shard_connections
self.executor = ThreadPoolExecutor(max_workers=len(shard_connections))

def scatter_gather(
self,
query: str,
params: tuple = (),
merge_func = None,
timeout: float = 30.0
) -&gt; List[Dict]:
&quot;&quot;&quot;
Execute query on all shards and merge results.

Args:
query: SQL query to execute
params: Query parameters
merge_func: Optional function to merge/aggregate results
timeout: Maximum time to wait for all shards

Returns:
Merged results from all shards
&quot;&quot;&quot;
futures = {}

# Scatter: submit query to all shards
for shard_name, connection in self.shards.items():
future = self.executor.submit(
self._execute_on_shard,
connection,
query,
params
)
futures[future] = shard_name

# Gather: collect results
results = []
errors = []

for future in as_completed(futures, timeout=timeout):
shard_name = futures[future]
try:
shard_results = future.result()
results.extend(shard_results)
except Exception as e:
errors.append((shard_name, str(e)))

if errors:
# Decide: fail fast or return partial results
print(f&quot;Shard errors: {errors}&quot;)

# Apply merge function if provided (sorting, aggregation, etc.)
if merge_func:
return merge_func(results)

return results

def _execute_on_shard(self, connection, query: str, params: tuple) -&gt; List[Dict]:
&quot;&quot;&quot;Execute query on a single shard.&quot;&quot;&quot;
cursor = connection.cursor()
cursor.execute(query, params)
columns = [desc[0] for desc in cursor.description]
return [dict(zip(columns, row)) for row in cursor.fetchall()]

def aggregate_count(self, table: str, where_clause: str = &quot;&quot;) -&gt; int:
&quot;&quot;&quot;Aggregate COUNT across all shards.&quot;&quot;&quot;
query = f&quot;SELECT COUNT(*) as cnt FROM {table}&quot;
if where_clause:
query += f&quot; WHERE {where_clause}&quot;

results = self.scatter_gather(query)
return sum(r['cnt'] for r in results)

def aggregate_sum(self, table: str, column: str, where_clause: str = &quot;&quot;) -&gt; float:
&quot;&quot;&quot;Aggregate SUM across all shards.&quot;&quot;&quot;
query = f&quot;SELECT SUM({column}) as total FROM {table}&quot;
if where_clause:
query += f&quot; WHERE {where_clause}&quot;

results = self.scatter_gather(query)
return sum(r['total'] or 0 for r in results)


# Usage
executor = CrossShardQueryExecutor(shard_connections)

# Simple scatter-gather
all_large_orders = executor.scatter_gather(
&quot;SELECT * FROM orders WHERE total &gt; %s ORDER BY created_at DESC LIMIT 100&quot;,
params=(1000,),
merge_func=lambda results: sorted(results, key=lambda x: x['created_at'], reverse=True)[:100]
)

# Aggregations
total_revenue = executor.aggregate_sum(&quot;orders&quot;, &quot;total&quot;, &quot;status = 'completed'&quot;)
order_count = executor.aggregate_count(&quot;orders&quot;, &quot;created_at &gt; '2024-01-01'&quot;)
```

---
</code></pre>
<h2 id="resharding-strategies">Resharding Strategies</h2>
<p><span>Resharding</span> is the process of redistributing data when adding or removing shards. It's one of the most complex operations in a sharded database.</p>
<div>
<div>ONLINE RESHARDING PROCESS</div>
<div>
  <!-- Step 1 -->
<div>
<div>1</div>
<div>
<div>Add Empty Shards</div>
<div>Deploy new shard infrastructure with empty databases. Update shard metadata but don't route traffic yet.</div>
</div>
</div>
  <!-- Step 2 -->
<div>
<div>2</div>
<div>
<div>Enable Double-Writes</div>
<div>For keys that will move, write to both old and new shard locations. This ensures new writes are captured during migration.</div>
</div>
</div>
  <!-- Step 3 -->
<div>
<div>3</div>
<div>
<div>Backfill Historical Data</div>
<div>Copy existing data from old shards to new shards in the background. Use checkpoints to track progress and enable resume on failure.</div>
</div>
</div>
  <!-- Step 4 -->
<div>
<div>4</div>
<div>
<div>Verify Data Consistency</div>
<div>Compare checksums between old and new locations. Run reconciliation jobs to find and fix discrepancies.</div>
</div>
</div>
  <!-- Step 5 -->
<div>
<div>5</div>
<div>
<div>Switch Read Traffic</div>
<div>Update routing to read from new shard locations. This can be done gradually with percentage-based rollout.</div>
</div>
</div>
  <!-- Step 6 -->
<div>
<div>6</div>
<div>
<div>Disable Double-Writes & Cleanup</div>
<div>Stop writing to old locations. After a safety period, delete migrated data from old shards.</div>
</div>
</div>
</div>
</div>
<h3 id="resharding-without-downtime">Resharding Without Downtime</h3>
<pre><code>```python
from enum import Enum
from typing import Dict, List, Optional
import threading
import time

class MigrationState(Enum):
NOT_STARTED = &quot;not_started&quot;
DOUBLE_WRITE = &quot;double_write&quot;
BACKFILLING = &quot;backfilling&quot;
VERIFYING = &quot;verifying&quot;
SWITCHING = &quot;switching&quot;
COMPLETED = &quot;completed&quot;

class OnlineReshardingManager:
&quot;&quot;&quot;
Manages online resharding with zero downtime.

Key principles:
1. Never stop serving traffic
2. Double-write during migration
3. Verify before switching
4. Support rollback at any stage
&quot;&quot;&quot;

def __init__(self, old_router, new_router, db_connections):
self.old_router = old_router
self.new_router = new_router
self.connections = db_connections
self.migration_state: Dict[str, MigrationState] = {}
self.state_lock = threading.Lock()

def start_migration(self, key_ranges: List[tuple]) -&gt; None:
&quot;&quot;&quot;Start migration for specified key ranges.&quot;&quot;&quot;
for start_key, end_key in key_ranges:
range_id = f&quot;{start_key}:{end_key}&quot;

with self.state_lock:
self.migration_state[range_id] = MigrationState.DOUBLE_WRITE

# Start background backfill
threading.Thread(
target=self._backfill_range,
args=(start_key, end_key)
).start()

def route_write(self, key: str, data: dict) -&gt; None:
&quot;&quot;&quot;Route write operation, respecting migration state.&quot;&quot;&quot;
range_id = self._get_range_for_key(key)
state = self.migration_state.get(range_id, MigrationState.COMPLETED)

if state in (MigrationState.DOUBLE_WRITE, MigrationState.BACKFILLING,
MigrationState.VERIFYING):
# Write to BOTH old and new locations
old_shard = self.old_router.get_shard(key)
new_shard = self.new_router.get_shard(key)

self._write_to_shard(old_shard, key, data)
self._write_to_shard(new_shard, key, data)

elif state == MigrationState.SWITCHING:
# Write only to new location
new_shard = self.new_router.get_shard(key)
self._write_to_shard(new_shard, key, data)

else:
# Normal operation: use appropriate router
shard = self.new_router.get_shard(key)
self._write_to_shard(shard, key, data)

def route_read(self, key: str) -&gt; dict:
&quot;&quot;&quot;Route read operation, respecting migration state.&quot;&quot;&quot;
range_id = self._get_range_for_key(key)
state = self.migration_state.get(range_id, MigrationState.COMPLETED)

if state in (MigrationState.DOUBLE_WRITE, MigrationState.BACKFILLING):
# Read from old location (source of truth during migration)
old_shard = self.old_router.get_shard(key)
return self._read_from_shard(old_shard, key)

else:
# Read from new location
new_shard = self.new_router.get_shard(key)
return self._read_from_shard(new_shard, key)

def _backfill_range(self, start_key: str, end_key: str) -&gt; None:
&quot;&quot;&quot;Backfill data from old shards to new shards.&quot;&quot;&quot;
range_id = f&quot;{start_key}:{end_key}&quot;

with self.state_lock:
self.migration_state[range_id] = MigrationState.BACKFILLING

# Scan old shards and copy data
# Use batching and checkpointing for large datasets
batch_size = 1000
last_key = start_key

while last_key &lt; end_key:
batch = self._scan_range(last_key, end_key, batch_size)

for record in batch:
new_shard = self.new_router.get_shard(record['key'])
self._write_to_shard(new_shard, record['key'], record['data'])
last_key = record['key']

# Checkpoint progress
self._save_checkpoint(range_id, last_key)

if len(batch) &lt; batch_size:
break

# Verify consistency
with self.state_lock:
self.migration_state[range_id] = MigrationState.VERIFYING

if self._verify_range(start_key, end_key):
with self.state_lock:
self.migration_state[range_id] = MigrationState.SWITCHING

# Wait for in-flight requests
time.sleep(5)

with self.state_lock:
self.migration_state[range_id] = MigrationState.COMPLETED

def _verify_range(self, start_key: str, end_key: str) -&gt; bool:
&quot;&quot;&quot;Verify data consistency between old and new shards.&quot;&quot;&quot;
# Compare checksums, row counts, sample records
old_checksum = self._compute_checksum(self.old_router, start_key, end_key)
new_checksum = self._compute_checksum(self.new_router, start_key, end_key)
return old_checksum == new_checksum
```

---
</code></pre>
<h2 id="real-life-failure-story">Real-Life Failure Story</h2>
<h3 id="the-notion-sharding-incident-2021">The Notion Sharding Incident (2021)</h3>
<div>
<h4>What Happened</h4>
<div>
<div>The Incident</div>
<div>
  Notion experienced a major outage when a shard containing popular templates became overloaded. The shard received 100x normal traffic when a viral template was shared widely. The imbalanced load caused cascading failures as the hot shard couldn't keep up.
</div>
</div>
<div>
<div>Root Cause</div>
<div>
<div>1. Sharding by workspace_id meant viral content created hotspots</div>
<div>2. No automatic shard splitting for hot shards</div>
<div>3. Connection pools exhausted on the hot shard</div>
<div>4. No caching layer for read-heavy template access</div>
</div>
</div>
<div>
<div>How They Fixed It</div>
<div>
<div>1. Implemented automatic hotspot detection and shard splitting</div>
<div>2. Added a CDN cache for public/shared content</div>
<div>3. Created separate "public content" shards for viral items</div>
<div>4. Implemented [[rate-limiting]](/topic/system-design/rate-limiting) per workspace</div>
<div>5. Added [[circuit-breaker]](/topic/design-patterns/circuit-breaker) patterns to prevent cascade</div>
</div>
</div>
</div>
<pre><code>---
</code></pre>
<h2 id="interview-questions---3-level-deep-dive">Interview Questions - 3-Level Deep Dive</h2>
<div>
<h3 id="q1-how-do-you-choose-a-shard-key-for-a-social-media-application">Q1: How do you choose a shard key for a social media application?</h3>
<div>
<p><strong>Level 1 Answer (Junior):</strong><br />
Choose user_id as the shard key because it has high cardinality and most queries are user-centric. Each user's data lives on one shard, making user profile and feed queries efficient.</p>
</div>
<div>
<p><strong>Level 2 Answer (Mid-Level):</strong><br />
User_id works for user-centric data, but social media has multiple access patterns:</p>
<ol>
<li><strong>User profile/feed</strong>: Shard by user_id - efficient single-shard queries</li>
<li><strong>Timeline (posts from followed users)</strong>: This is cross-shard! Options:
<ul>
<li>Fan-out on write: Copy posts to followers' shards (Instagram approach)</li>
<li>Fan-out on read: Query all followed users' shards (expensive)</li>
</ul>
</li>
<li><strong>Trending/search</strong>: Separate system, not sharded by user_id</li>
</ol>
<p>I'd use <span>compound sharding</span>: user_id for personal data, but maintain denormalized copies for cross-user features.</p>
</div>
<div>
<p><strong>Level 3 Answer (Senior):</strong><br />
The shard key decision requires analyzing the full data model:</p>
<p><strong>Primary entities and their sharding:</strong><br />
<code>users table        → shard by user_id (hash) posts table        → shard by author_id (co-locate with user) comments table     → shard by post_author_id (co-locate with post) likes table        → shard by post_author_id follows table      → COMPLEX - see below messages table     → shard by conversation_id</code></p>
<p><strong>The follows/timeline problem is the hardest:</strong><br />
- Option A: Store follows on follower's shard. Timeline = scatter-gather to all followed users' shards. High read latency.<br />
- Option B: Store follows on followee's shard. Efficient for &quot;who follows me?&quot; but timeline still scatter-gather.<br />
- Option C: Fan-out on write (Twitter/Instagram model):<br />
- When user posts, push to all followers' timeline shards<br />
- Trades write amplification for read efficiency<br />
- For celebrities (100M followers), use hybrid: don't fan-out, merge at read time</p>
<p><strong>Handling hotspots (celebrity accounts):</strong></p>
<ol>
<li>Secondary sharding for hot users: <code>shard = hash(user_id + date)</code> to spread load</li>
<li>Separate &quot;public&quot; content tier with caching</li>
<li>Rate limiting per user</li>
</ol>
<p><strong>Cross-shard consistency:</strong><br />
- Use <a href="/topic/system-design/cap-theorem">[eventual-consistency]</a> for timeline (seconds delay acceptable)<br />
- Use <a href="/topic/system-design/distributed-locking">[distributed-locking]</a> for critical operations (delete, account changes)</p>
</div>
</div>
<div>
<h3 id="q2-explain-consistent-hashing-and-why-virtual-nodes-are-important">Q2: Explain consistent hashing and why virtual nodes are important.</h3>
<div>
<p><strong>Level 1 Answer (Junior):</strong><br />
Consistent hashing maps both keys and servers to a ring. Keys belong to the first server clockwise from their position. When adding/removing servers, only keys adjacent to the change move, minimizing data redistribution. Virtual nodes are multiple positions per server to improve distribution.</p>
</div>
<div>
<p><strong>Level 2 Answer (Mid-Level):</strong><br />
Without consistent hashing, adding a shard requires rehashing all keys: <code>hash(key) % N</code> changes for most keys when N changes.</p>
<p>Consistent hashing fixes this by:</p>
<ol>
<li>Hashing servers and keys to the same ring (0 to 2^32)</li>
<li>Each key is assigned to the first server clockwise</li>
<li>Adding a server only affects keys in one segment</li>
</ol>
<p><strong>Virtual nodes solve the uneven distribution problem:</strong><br />
- With only 4 physical nodes, one might get 50% of the ring by chance<br />
- With 100 virtual nodes per physical node (400 total), distribution approaches uniform<br />
- Also helps during failures: one node's keys spread across many others, not just one</p>
<p><strong>Implementation detail:</strong> Virtual nodes are created by hashing <code>&quot;node_name:0&quot;</code>, <code>&quot;node_name:1&quot;</code>, etc.</p>
</div>
<div>
<p><strong>Level 3 Answer (Senior):</strong><br />
Consistent hashing is fundamental to distributed systems like DynamoDB, Cassandra, and Riak.</p>
<p><strong>Mathematical properties:</strong><br />
- Adding 1 node to N nodes moves only 1/(N+1) of keys<br />
- This is optimal - you can't do better without a directory<br />
- Ring positions use 32 or 64-bit hash space</p>
<p><strong>Virtual nodes serve multiple purposes:</strong></p>
<ol>
<li><strong>Load balancing</strong>: With K virtual nodes per physical node, standard deviation of load is O(1/sqrt(K))</li>
<li><strong>Heterogeneous hardware</strong>: Powerful nodes get more virtual nodes</li>
<li><strong>Graceful failure recovery</strong>: Failed node's load spreads across all others proportionally</li>
<li><strong>Incremental rebalancing</strong>: Can move virtual nodes one at a time</li>
</ol>
<p><strong>Replication strategy (Dynamo-style):</strong><br />
<code>python def get_preference_list(key, n_replicas=3): &quot;&quot;&quot;Return N distinct physical nodes, walking clockwise.&quot;&quot;&quot; nodes = [] pos = hash(key) while len(nodes) &lt; n_replicas: pos = next_virtual_node_clockwise(pos) physical = virtual_to_physical[pos] if physical not in nodes: nodes.append(physical) return nodes </code></p>
<p><strong>Weighted consistent hashing (for varying capacity):</strong><br />
- Node with 2x RAM gets 2x virtual nodes<br />
- But this complicates rebalancing when weights change<br />
- Alternative: Directory-based with weight metadata</p>
<p><strong>Jump consistent hash (Google, 2014):</strong><br />
- No memory overhead (no ring storage)<br />
- O(log n) computation<br />
- But only supports sequential node IDs and no removal</p>
</div>
</div>
<div>
<h3 id="q3-how-do-you-handle-cross-shard-transactions">Q3: How do you handle cross-shard transactions?</h3>
<div>
<p><strong>Level 1 Answer (Junior):</strong><br />
Cross-shard transactions are hard because you can't use a single database transaction. You need either two-phase commit (2PC) where a coordinator ensures all shards commit or rollback together, or saga pattern where you execute operations sequentially with compensating actions for rollback.</p>
</div>
<div>
<p><strong>Level 2 Answer (Mid-Level):</strong><br />
Cross-shard transactions violate the core benefit of sharding (independent operation). I'd recommend:</p>
<p><strong>1. Avoid them by design:</strong><br />
- Co-locate related data on the same shard<br />
- Denormalize to eliminate cross-shard joins<br />
- Accept eventual consistency where possible</p>
<p><strong>2. When unavoidable, choose based on requirements:</strong></p>
<p><strong>Two-Phase Commit (2PC):</strong><br />
- Strong consistency, but blocks on coordinator failure<br />
- High latency (2 round-trips minimum)<br />
- Use for: Financial transactions where correctness &gt; availability</p>
<p><strong>Saga Pattern:</strong><br />
- Eventual consistency with compensating transactions<br />
- Each step is a local transaction + event<br />
- If step fails, execute compensating actions for previous steps<br />
- Use for: Order processing, booking systems</p>
<p><strong>TCC (Try-Confirm-Cancel):</strong><br />
- Reserve resources (Try), then Confirm or Cancel<br />
- Like saga but with explicit reservation phase<br />
- Better for inventory, seat booking</p>
</div>
<div>
<p><strong>Level 3 Answer (Senior):</strong><br />
This is where theory meets practice. Let me break down the options with real trade-offs:</p>
<p><strong>Why cross-shard transactions are fundamentally hard:</strong><br />
- CAP theorem: Can't have consistency + availability during partition<br />
- 2PC is a consensus problem; consensus is expensive<br />
- Distributed transactions increase failure domain</p>
<p><strong>Production-ready approaches:</strong></p>
<p><strong>1. Choreography-based Saga (event-driven):</strong><br />
<code>OrderService                    PaymentService                  InventoryService |                                |                               | | OrderCreated                   |                               | |-------------------------------&gt;|                               | |                          PaymentProcessed                      | |                                |------------------------------&gt;| |                                                         InventoryReserved |&lt;---------------------------------------------------------------|</code><br />
- No central coordinator (no SPOF)<br />
- Complex to debug and monitor<br />
- Compensations must be idempotent</p>
<p><strong>2. Orchestration-based Saga:</strong><br />
```python<br />
class OrderSaga:<br />
steps = [<br />
(reserve_inventory, release_inventory),<br />
(process_payment, refund_payment),<br />
(confirm_order, cancel_order),<br />
]</p>
<pre><code>    def execute(self, order):
    completed = []
    for action, compensation in self.steps:
    try:
    action(order)
    completed.append(compensation)
    except Exception:
    for comp in reversed(completed):
    comp(order)
    raise
    ```
    - Easier to reason about and monitor
    - Coordinator is SPOF (need to persist saga state)
</code></pre>
<p><strong>3. Outbox Pattern (for reliable messaging):</strong><br />
- Write event to local outbox table in same transaction as data change<br />
- Separate process reads outbox and publishes to message queue<br />
- Guarantees at-least-once delivery</p>
<p><strong>4. For true ACID across shards (expensive but sometimes needed):</strong><br />
- Google Spanner: TrueTime + Paxos for global consistency<br />
- CockroachDB: Serializable isolation across nodes<br />
- Vitess: Supports cross-shard transactions with 2PC</p>
<p><strong>Design principle:</strong> Minimize cross-shard transactions by making shards the unit of consistency. Design domain boundaries around shard boundaries.</p>
</div>
</div>
<div>
<h3 id="q4-how-would-you-reshard-a-production-database-with-zero-downtime">Q4: How would you reshard a production database with zero downtime?</h3>
<div>
<p><strong>Level 1 Answer (Junior):</strong><br />
Use the double-write pattern: write to both old and new shard locations during migration, backfill historical data in background, then switch reads to new locations. Finally, stop writing to old locations and clean up.</p>
</div>
<div>
<p><strong>Level 2 Answer (Mid-Level):</strong><br />
Zero-downtime resharding requires careful orchestration:</p>
<p><strong>Phase 1 - Preparation:</strong><br />
- Deploy new shard infrastructure<br />
- Update routing logic to understand both old and new schemes<br />
- Enable feature flag for migration</p>
<p><strong>Phase 2 - Double-Write:</strong><br />
```python<br />
def write(key, data):<br />
old_shard = old_router.get_shard(key)<br />
new_shard = new_router.get_shard(key)</p>
<pre><code>    # Write to both (old is source of truth)
    write_to_shard(old_shard, key, data)
    write_to_shard(new_shard, key, data)  # Async is OK
    ```
</code></pre>
<p><strong>Phase 3 - Backfill:</strong><br />
- Scan old shards chronologically (or by key range)<br />
- Copy to new shards (skip if newer version exists from double-write)<br />
- Track progress with checkpoints</p>
<p><strong>Phase 4 - Verification:</strong><br />
- Compare row counts, checksums<br />
- Sample random records for deep comparison<br />
- Monitor for discrepancies</p>
<p><strong>Phase 5 - Cutover:</strong><br />
- Switch reads to new shards (gradually with % rollout)<br />
- Monitor error rates<br />
- Keep double-writes for safety buffer</p>
<p><strong>Phase 6 - Cleanup:</strong><br />
- Disable writes to old shards<br />
- Wait for in-flight requests<br />
- Archive or delete old data</p>
</div>
<div>
<p><strong>Level 3 Answer (Senior):</strong><br />
I've done this at scale. Here are the hard parts people don't mention:</p>
<p><strong>Challenge 1: Maintaining consistency during double-write</strong><br />
```python<br />
# Naive double-write has race conditions:<br />
# T1: read from old_shard (version 1)<br />
# T2: write to old_shard (version 2)<br />
# T2: write to new_shard (version 2)<br />
# T1: write to new_shard (version 1) &lt;- STALE!</p>
<pre><code>    # Solution: Include version/timestamp, use conditional writes
    def write_with_version(shard, key, data, version):
    # Only write if version is newer
    UPDATE table SET data = ?, version = ?
    WHERE key = ? AND version &lt; ?
    ```
</code></pre>
<p><strong>Challenge 2: Backfill with high write volume</strong><br />
- Backfill takes days for TB-scale data<br />
- Writes during backfill create moving target<br />
- Solution: Multiple passes with decreasing scope<br />
- Pass 1: Copy all data (some will be stale)<br />
- Pass 2: Copy only records modified since Pass 1 started<br />
- Pass 3: Copy only records modified since Pass 2 started<br />
- Continue until Pass N copies &lt; 1000 records</p>
<p><strong>Challenge 3: Handling schema differences</strong><br />
- New shard might have different schema<br />
- Need bidirectional transformation during migration<br />
```python<br />
def write_to_new(key, old_format_data):<br />
new_format = transform_v1_to_v2(old_format_data)<br />
write(new_shard, key, new_format)</p>
<pre><code>    def read_from_new_for_old_client(key):
    new_format = read(new_shard, key)
    return transform_v2_to_v1(new_format)
    ```
</code></pre>
<p><strong>Challenge 4: Rollback capability</strong><br />
- Keep old shards intact until fully verified<br />
- Maintain reverse routing capability<br />
- Test rollback procedure in staging</p>
<p><strong>Challenge 5: Cross-shard transactions during migration</strong><br />
- If transaction spans migrating + stable shards<br />
- Pause migration, complete transaction, resume<br />
- Or: use <a href="/topic/system-design/distributed-locking">[distributed-locking]</a> per-key during transition</p>
<p><strong>Tools we use:</strong><br />
- gh-ost (GitHub) for MySQL online schema changes<br />
- Vitess for managed MySQL sharding with resharding support<br />
- Custom state machine for migration orchestration<br />
- Extensive monitoring: lag, error rates, comparison mismatches</p>
</div>
</div>
<div>
<h3 id="q5-what-are-the-trade-offs-between-database-sharding-and-using-a-distributed-database-like-cockroachdb">Q5: What are the trade-offs between database sharding and using a distributed database like CockroachDB?</h3>
<div>
<p><strong>Level 1 Answer (Junior):</strong><br />
Manual sharding gives you more control but requires building routing, handling cross-shard queries, and managing resharding yourself. Distributed databases like CockroachDB handle this automatically but add latency for consensus and are more expensive.</p>
</div>
<div>
<p><strong>Level 2 Answer (Mid-Level):</strong></p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Manual Sharding</th>
<th>Distributed DB</th>
</tr>
</thead>
<tbody>
<tr>
<td>Control</td>
<td>Full control over data placement</td>
<td>Automatic, less predictable</td>
</tr>
<tr>
<td>Consistency</td>
<td>Choose per operation</td>
<td>Usually strong by default</td>
</tr>
<tr>
<td>Latency</td>
<td>Single-shard: ~1ms</td>
<td>All writes: 10-50ms (consensus)</td>
</tr>
<tr>
<td>Cross-shard</td>
<td>You build scatter-gather</td>
<td>Built-in SQL support</td>
</tr>
<tr>
<td>Resharding</td>
<td>Complex, manual process</td>
<td>Automatic rebalancing</td>
</tr>
<tr>
<td>Cost</td>
<td>Cheaper infrastructure</td>
<td>Higher license/complexity cost</td>
</tr>
<tr>
<td>Team skill</td>
<td>Requires deep expertise</td>
<td>Easier to operate</td>
</tr>
</tbody>
</table>
<p><strong>When to choose manual sharding:</strong><br />
- Predictable, simple access patterns<br />
- Very low latency requirements<br />
- Large team with database expertise<br />
- Cost-sensitive at scale</p>
<p><strong>When to choose distributed DB:</strong><br />
- Complex queries, joins across shards<br />
- Strong consistency requirements<br />
- Smaller team, less DB expertise<br />
- Rapid scaling needs</p>
</div>
<div>
<p><strong>Level 3 Answer (Senior):</strong><br />
This is a fundamental architecture decision. Let me share production experience with both:</p>
<p><strong>Manual Sharding (Instagram, Discord, Uber approach):</strong></p>
<p><em>Advantages:</em><br />
- Predictable latency: single-shard reads are ~1ms<br />
- Full control: can optimize for specific access patterns<br />
- Battle-tested: MySQL/PostgreSQL at scale is well-understood<br />
- Cost: Commodity hardware, open-source databases</p>
<p><em>Hidden costs:</em><br />
- Building shard router + query parser: 3-6 months engineering<br />
- On-call complexity: shard-aware debugging, rebalancing<br />
- Every new feature must consider sharding implications<br />
- Cross-shard transactions: build your own saga/2PC</p>
<p><strong>Distributed Database (Spanner, CockroachDB, TiDB):</strong></p>
<p><em>Advantages:</em><br />
- SQL semantics preserved (JOINs work across nodes)<br />
- Automatic rebalancing and resharding<br />
- Serializable isolation by default<br />
- Built-in HA with consensus replication</p>
<p><em>Hidden costs:</em><br />
- Write latency: 10-50ms minimum (consensus round-trips)<br />
- Tail latency: Cross-region writes can be 100ms+<br />
- Debugging: Distributed query plans are complex<br />
- Cost: 3-5x infrastructure cost vs manual sharding</p>
<p><strong>Hybrid approaches (what I'd recommend):</strong></p>
<ol>
<li>
<p><strong>Vitess (used by YouTube, Slack):</strong></p>
<ul>
<li>MySQL underneath (predictable)</li>
<li>Sharding layer handles routing</li>
<li>Supports cross-shard queries (scatter-gather)</li>
<li>Easier resharding than manual</li>
</ul>
</li>
<li>
<p><strong>Citus (PostgreSQL extension):</strong></p>
<ul>
<li>PostgreSQL syntax and tooling</li>
<li>Distributed tables for sharded data</li>
<li>Reference tables for small lookups</li>
<li>Co-located tables for related data</li>
</ul>
</li>
<li>
<p><strong>Start simple, evolve:</strong></p>
<ul>
<li>Begin with single database + read replicas</li>
<li>When hitting limits, first try vertical scaling</li>
<li>Then vertical sharding (split by domain/table)</li>
<li>Finally horizontal sharding for hot tables only</li>
</ul>
</li>
</ol>
<p><strong>Decision framework:</strong><br />
```<br />
if (p99_latency_requirement &lt; 10ms):<br />
manual_sharding()  # Consensus is too slow</p>
<pre><code>    elif (cross_shard_queries &gt; 20%):
    distributed_db()  # Manual scatter-gather is too complex

    elif (team.size &lt; 5 and team.db_expertise &lt; &quot;expert&quot;):
    managed_distributed_db()  # CockroachCloud, Spanner

    else:
    evaluate_based_on_cost_and_specific_patterns()
    ```
</code></pre>
</div>
</div>
<pre><code>---
</code></pre>
<h2 id="common-pitfalls">Common Pitfalls</h2>
<div>
<h3 id="1-wrong-shard-key-selection">1. Wrong Shard Key Selection</h3>
<p><strong>Problem</strong>: Chose low-cardinality key (country, status) causing hotspots.<br />
<strong>Solution</strong>: Use high-cardinality keys (user_id, UUID); compound keys for multi-dimensional access.</p>
<h3 id="2-cross-shard-joins-without-planning">2. Cross-Shard Joins Without Planning</h3>
<p><strong>Problem</strong>: Application evolved to need JOINs across shards, causing scatter-gather everywhere.<br />
<strong>Solution</strong>: Design schema upfront considering sharding. Denormalize or use reference tables.</p>
<h3 id="3-sequential-id-collisions">3. Sequential ID Collisions</h3>
<p><strong>Problem</strong>: Auto-increment IDs from different shards collide (shard1.id=1, shard2.id=1).<br />
<strong>Solution</strong>: Use UUIDs, Snowflake IDs, or shard-prefixed sequences: <code>shard_1_00001</code>.</p>
<h3 id="4-ignoring-hotspot-potential">4. Ignoring Hotspot Potential</h3>
<p><strong>Problem</strong>: Viral content or celebrity users overwhelm a single shard.<br />
<strong>Solution</strong>: Secondary sharding for hot entities, <a href="/topic/system-design/caching">[caching]</a>, rate limiting, or special &quot;hot&quot; shards.</p>
<h3 id="5-resharding-as-afterthought">5. Resharding as Afterthought</h3>
<p><strong>Problem</strong>: Started with simple hash sharding; now adding shards requires massive data movement.<br />
<strong>Solution</strong>: Use consistent hashing from day one. Build resharding capability before you need it.</p>
<h3 id="6-no-per-shard-observability">6. No Per-Shard Observability</h3>
<p><strong>Problem</strong>: Aggregate metrics hide individual shard problems until cascade failure.<br />
<strong>Solution</strong>: Dashboard per shard: CPU, memory, connections, query latency, replication lag.</p>
</div>
<pre><code>---
</code></pre>
<h2 id="best-practices">Best Practices</h2>
<div>
<ol>
<li>
<p><strong>Start with more shards than you need</strong> - 16 shards for 100M rows leaves room for 1B without resharding</p>
</li>
<li>
<p><strong>Use consistent hashing from day one</strong> - Even if you don't need dynamic scaling yet</p>
</li>
<li>
<p><strong>Include shard key in every related table</strong> - Enables co-located joins within shard</p>
</li>
<li>
<p><strong>Replicate each shard</strong> - Shards need <a href="/topic/system-design/database-replication">[database-replication]</a> too (primary + 2 replicas minimum)</p>
</li>
<li>
<p><strong>Monitor shard balance</strong> - Alert when data skew exceeds 20%; when hotspot detected</p>
</li>
<li>
<p><strong>Design for single-shard queries</strong> - 95%+ of queries should hit one shard</p>
</li>
<li>
<p><strong>Test resharding in staging</strong> - Before you need it in production</p>
</li>
<li>
<p><strong>Plan for failure</strong> - What happens when a shard is unavailable? Failover? Read-only mode?</p>
</li>
<li>
<p><strong>Document shard key in code</strong> - Make it explicit: <code>@ShardedBy(field = &quot;user_id&quot;)</code></p>
</li>
<li>
<p><strong>Implement circuit breakers</strong> - Prevent one failing shard from overwhelming others via <a href="/topic/design-patterns/circuit-breaker">[circuit-breaker]</a></p>
</li>
</ol>
</div>
<pre><code>---
</code></pre>
<h2 id="quick-reference-card">Quick Reference Card</h2>
<div>
<h4>Database Sharding Cheat Sheet</h4>
<div>
<div>
<div>Sharding Strategies</div>
<div>
<div><strong>Range:</strong> Good for scans, bad for hotspots</div>
<div><strong>Hash:</strong> Even distribution, poor resharding</div>
<div><strong>Consistent Hash:</strong> Best for dynamic clusters</div>
<div><strong>Directory:</strong> Maximum flexibility, extra lookup</div>
</div>
</div>
<div>
<div>Shard Key Properties</div>
<div>
<div>High cardinality (millions of values)</div>
<div>Even distribution (no hotspots)</div>
<div>Query-aligned (in WHERE clause)</div>
<div>Immutable (never changes)</div>
</div>
</div>
<div>
<div>Cross-Shard Strategies</div>
<div>
<div><strong>Scatter-Gather:</strong> Query all, merge results</div>
<div><strong>Denormalization:</strong> Co-locate related data</div>
<div><strong>Reference Tables:</strong> Replicate small tables</div>
<div><strong>Global Index:</strong> Secondary lookup table</div>
</div>
</div>
<div>
<div>ID Generation</div>
<div>
<div><strong>UUID:</strong> Random, no coordination</div>
<div><strong>Snowflake:</strong> Time + machine + sequence</div>
<div><strong>Shard-prefix:</strong> shard_1_00001</div>
<div><strong>ULID:</strong> Sortable UUID alternative</div>
</div>
</div>
</div>
</div>
<pre><code>---
</code></pre>
<h2 id="related-topics">Related Topics</h2>
<pre><code>- [[database-replication]](/topic/system-design/database-replication) - Each shard needs replicas for HA
- [[load-balancing]](/topic/system-design/load-balancing) - Consistent hashing algorithms
- [[cap-theorem]](/topic/system-design/cap-theorem) - Understanding consistency trade-offs
- [[distributed-locking]](/topic/system-design/distributed-locking) - Cross-shard coordination
- [[caching]](/topic/system-design/caching) - Reducing load on shards
- [[rate-limiting]](/topic/system-design/rate-limiting) - Protecting shards from overload
</code></pre>
