<h1 id="concurrency-patterns">Concurrency Patterns</h1>
<h2 id="overview">Overview</h2>
<p>Concurrency patterns are battle-tested solutions for coordinating multiple execution contexts competing for shared resources. Understanding these patterns at a deep level is essential for designing systems that are correct under all interleavings, performant under load, and debuggable when things go wrong.</p>
<div>
<div>The Fundamental Tension in Concurrent Systems</div>
<div>
<div>
<div>Safety</div>
<div>Nothing bad ever happens. No race conditions, no data corruption, no invariant violations.</div>
</div>
<div>
<div>Liveness</div>
<div>Something good eventually happens. No deadlocks, no starvation, progress is always made.</div>
</div>
<div>
<div>Performance</div>
<div>Good things happen quickly. Low latency, high throughput, efficient resource utilization.</div>
</div>
</div>
<div>Every concurrency design decision involves trading off between these three properties.</div>
</div>
<div>
<div>Critical Assumption</div>
<div>Concurrency bugs are non-deterministic. A program may run correctly thousands of times and fail on the thousand-and-first due to a specific thread interleaving. Testing alone cannot prove correctness. You must reason about all possible interleavings, use formal verification techniques where possible, and design defensively.</div>
</div>
<hr />
<h2 id="thread-pools">Thread Pools</h2>
<h3 id="internal-architecture-and-mechanisms">Internal Architecture and Mechanisms</h3>
<p>A thread pool maintains a collection of pre-created worker threads that pull tasks from a shared work queue. This amortizes thread creation overhead across many tasks and bounds resource consumption.</p>
<div>
<div>Thread Pool Internal Components</div>
<div>
<div>
<div>Work Queue</div>
<div>Holds pending tasks. Can be bounded (blocking on full) or unbounded (risk of OOM). Implementation choices: LinkedBlockingQueue, ArrayBlockingQueue, SynchronousQueue (direct handoff).</div>
</div>
<div>
<div>Worker Threads</div>
<div>Long-running threads in a loop: dequeue task, execute, repeat. Park when queue empty. Core threads stay alive; excess threads may terminate after idle timeout.</div>
</div>
<div>
<div>Thread Factory</div>
<div>Creates worker threads with proper naming (for debugging), daemon status, priority, and uncaught exception handlers. Critical for observability.</div>
</div>
<div>
<div>Rejection Handler</div>
<div>Invoked when queue is full and max threads reached. Policies: AbortPolicy (throw), CallerRunsPolicy (execute in submitting thread), DiscardPolicy, DiscardOldestPolicy.</div>
</div>
</div>
</div>
<h3 id="thread-pool-sizing-the-science">Thread Pool Sizing: The Science</h3>
<div>
<div>Optimal Thread Count Formulas</div>
<div>
<div>CPU-Bound Workloads</div>
<div>
  N<sub>threads</sub> = N<sub>cpu</sub> + 1
</div>
<div>The +1 accounts for occasional page faults or other stalls. More threads cause context switching overhead that degrades performance.</div>
</div>
<div>
<div>I/O-Bound Workloads (Little's Law Derivation)</div>
<div>
  N<sub>threads</sub> = N<sub>cpu</sub> x U<sub>target</sub> x (1 + W/C)
</div>
<div>
<strong>U<sub>target</sub></strong> = target CPU utilization (0.0-1.0, typically 0.8)<br/>
<strong>W</strong> = average wait time (blocking on I/O)<br/>
<strong>C</strong> = average compute time<br/>
  Example: 8 cores, 80% target utilization, 100ms wait, 10ms compute = 8 x 0.8 x (1 + 10) = 70 threads
</div>
</div>
<div>
<div>Critical Edge Case: Blocking Inside Tasks</div>
<div>If tasks make synchronous calls to other services/databases, the W/C ratio can spike unexpectedly. A downstream service degradation can exhaust your thread pool. Solution: Use separate pools for different latency tiers, or use async I/O.</div>
</div>
</div>
<h3 id="work-stealing-advanced-thread-pool-optimization">Work Stealing: Advanced Thread Pool Optimization</h3>
<p>Work stealing improves load balancing when tasks have variable execution times. Each worker maintains a local deque (double-ended queue). Workers push/pop from their own deque's tail (LIFO for cache locality), but steal from other workers' heads (FIFO for fairness).</p>
<div>
<div>Work Stealing Mechanics</div>
<div>
<div>
<div>Local Execution</div>
<div>Worker pops from own deque tail. LIFO order keeps hot data in cache. No contention with other threads.</div>
</div>
<div>
<div>Stealing</div>
<div>Idle worker steals from random victim's head. FIFO order steals oldest (largest) tasks. Uses CAS for lock-free operation.</div>
</div>
<div>
<div>Fork-Join Pattern</div>
<div>Parent task forks children to local deque. Children may be stolen. Parent joins by helping execute if not done.</div>
</div>
</div>
</div>
<h3 id="thread-pool-interview-questions-3-level-deep">Thread Pool Interview Questions (3-Level Deep)</h3>
<div>
<div>Level 1: How does a thread pool work?</div>
<div>
<div>Expected Answer:</div>
<div>A thread pool maintains a fixed set of worker threads and a queue of pending tasks. Instead of creating a new thread for each task (expensive: ~1MB stack allocation, OS scheduling overhead), tasks are submitted to the queue. Worker threads continuously pull tasks from the queue and execute them. This amortizes creation cost and bounds resource usage.</div>
</div>
<div>
<div>Level 2: What happens when tasks are submitted faster than workers can process them?</div>
<div>
<div>Expected Answer:</div>
<div>Tasks accumulate in the queue. With an unbounded queue, memory grows until OOM. With a bounded queue, behavior depends on the rejection policy: block the submitter, throw exception, discard the task, or run in the caller's thread. The CallerRunsPolicy provides natural backpressure by slowing down the producer when consumers are overwhelmed.</div>
</div>
<div>
<div>Level 3: How would you design a thread pool that handles both CPU-bound and I/O-bound tasks without one starving the other?</div>
<div>
<div>Expected Answer:</div>
<div>
<strong>Option 1: Separate pools.</strong> Dedicate N<sub>cpu</sub> threads for CPU work, larger pool for I/O. Requires task classification at submission time.<br/><br/>
<strong>Option 2: Managed blocking.</strong> Like ForkJoinPool's ManagedBlocker: before blocking, signal the pool to compensate by temporarily adding a thread. Prevents throughput collapse during blocking operations.<br/><br/>
<strong>Option 3: Async I/O.</strong> Convert I/O-bound work to non-blocking (CompletableFuture, coroutines). Single pool handles CPU work; I/O completion triggers continuations without blocking threads.<br/><br/>
<strong>Trade-off:</strong> Separate pools waste resources when workload mix changes. Managed blocking adds complexity. Async requires rewriting blocking code. Netflix uses separate pools per dependency to isolate failure domains (see [[bulkhead-pattern]](/topic/system-design/bulkhead-pattern)).
</div>
</div>
</div>
</div>
</div>
<hr />
<h2 id="producer-consumer-pattern">Producer-Consumer Pattern</h2>
<h3 id="internal-mechanisms">Internal Mechanisms</h3>
<p>The producer-consumer pattern decouples task generation from task processing using a shared buffer. Producers add items; consumers remove them. The buffer absorbs temporary rate mismatches between production and consumption.</p>
<div>
<div>Producer-Consumer Synchronization</div>
<div>
<div>
<div>Producer Logic</div>
<div>
  acquire(mutex)<br/>
  while (count == BUFFER_SIZE):<br/>
  &nbsp;&nbsp;wait(not_full, mutex)<br/>
  buffer[in] = item<br/>
  in = (in + 1) % BUFFER_SIZE<br/>
  count++<br/>
  signal(not_empty)<br/>
  release(mutex)
</div>
</div>
<div>
<div>Consumer Logic</div>
<div>
  acquire(mutex)<br/>
  while (count == 0):<br/>
  &nbsp;&nbsp;wait(not_empty, mutex)<br/>
  item = buffer[out]<br/>
  out = (out + 1) % BUFFER_SIZE<br/>
  count--<br/>
  signal(not_full)<br/>
  release(mutex)
</div>
</div>
</div>
<div>
<div>Why "while" instead of "if"? (Spurious Wakeups)</div>
<div>Condition variables can wake spuriously (no signal occurred) or wake multiple threads when only one can proceed. The while loop re-checks the condition after waking, ensuring correctness. This is a POSIX requirement and occurs in practice due to OS scheduler implementation details.</div>
</div>
</div>
<h3 id="buffer-implementation-choices">Buffer Implementation Choices</h3>
<div>
<div>
<div>
<div>Bounded Buffer (Ring Buffer)</div>
<div>
  Fixed-size circular array. O(1) enqueue/dequeue. Provides natural backpressure. Memory bounded.
</div>
<div>
<div>Use When:</div>
<div>Memory is constrained; you want automatic flow control; producer can afford to block or reject.</div>
</div>
</div>
<div>
<div>Unbounded Buffer (Linked List)</div>
<div>
  Grows dynamically. Producer never blocks. Risk of memory exhaustion if consumer falls behind.
</div>
<div>
<div>Use When:</div>
<div>Bursty traffic with eventual catch-up; producer latency is critical; you have memory headroom and monitoring.</div>
</div>
</div>
</div>
</div>
<h3 id="lock-free-alternatives-lmax-disruptor-pattern">Lock-Free Alternatives: LMAX Disruptor Pattern</h3>
<div>
<div>The Disruptor: Mechanical Sympathy in Action</div>
<div>
    The LMAX Disruptor achieves millions of operations per second through:
</div>
<div>
<div>
<div>Pre-allocated Ring Buffer</div>
<div>All entries pre-allocated. No GC pressure. Entries overwritten, not deallocated.</div>
</div>
<div>
<div>Cache-Line Padding</div>
<div>Sequences padded to avoid false sharing. Each sequence on its own cache line (64 bytes).</div>
</div>
<div>
<div>Memory Barriers</div>
<div>Volatile writes for sequence numbers. Consumers spin-wait on sequence, avoiding kernel transitions.</div>
</div>
<div>
<div>Batching</div>
<div>Consumer can process all available entries in one batch. Amortizes synchronization overhead.</div>
</div>
</div>
</div>
<h3 id="producer-consumer-interview-questions-3-level-deep">Producer-Consumer Interview Questions (3-Level Deep)</h3>
<div>
<div>Level 1: What is the producer-consumer pattern and why is it useful?</div>
<div>
<div>Expected Answer:</div>
<div>Producer-consumer decouples components that generate work from those that process it using a shared queue. Benefits: (1) Rate decoupling: producers and consumers can operate at different speeds. (2) Load balancing: multiple consumers share work. (3) Fault isolation: consumer crash doesn't affect producer. Used in message queues, logging systems, request processing pipelines.</div>
</div>
<div>
<div>Level 2: How do you prevent the producer from overwhelming the consumer?</div>
<div>
<div>Expected Answer:</div>
<div>
<strong>Backpressure mechanisms:</strong><br/>
1. <strong>Bounded queue + blocking:</strong> Producer blocks when queue full. Simple but can cause upstream latency spikes.<br/>
2. <strong>Rejection with retry:</strong> Return error to producer; it retries with exponential backoff.<br/>
3. <strong>Rate limiting:</strong> Token bucket at producer limits submission rate.<br/>
4. <strong>Reactive streams:</strong> Consumer signals demand; producer only sends what's requested (pull-based).<br/>
5. <strong>Sampling/dropping:</strong> For metrics/logs where completeness isn't critical, drop entries under pressure.
</div>
</div>
<div>
<div>Level 3: Your producer-consumer system has 100ms p99 latency requirements. During peak load, you see latency spikes to 5 seconds. How do you diagnose and fix this?</div>
<div>
<div>Expected Answer:</div>
<div>
<strong>Diagnosis:</strong><br/>
1. <strong>Queue depth monitoring:</strong> If queue grows unboundedly, consumers can't keep up. Check consumer processing rate vs producer submission rate.<br/>
2. <strong>Consumer profiling:</strong> Are consumers blocked on downstream calls? Check for lock contention, GC pauses, I/O waits.<br/>
3. <strong>Coordination overhead:</strong> Lock contention on the queue itself? Profile time spent acquiring locks vs doing work.<br/><br/>
<p><strong>Solutions (depending on root cause):</strong><br/></p>
<ol>
<li><strong>Queue saturation:</strong> Add consumers, increase queue size with alerting, implement load shedding (drop low-priority items).<br/></li>
<li><strong>Slow consumers:</strong> Profile and optimize hot paths, add caching, parallelize independent work within consumer.<br/></li>
<li><strong>Lock contention:</strong> Use lock-free queue (ConcurrentLinkedQueue, Disruptor), shard into multiple queues, batch submissions to reduce coordination frequency.<br/></li>
<li><strong>Head-of-line blocking:</strong> If one slow item blocks queue, use multiple queues with priority levels or task stealing.<br/><br/></li>
</ol>
<p><strong>Architectural change:</strong> If spikes are bursty, consider async processing with acknowledgment (message broker like <a href="/topic/system-design/message-queues">[message-queues]</a>), accepting higher latency but better throughput.</p>
</div>
</div>
</div>
</div>
</div>
<hr />
<h2 id="readers-writers-problem">Readers-Writers Problem</h2>
<h3 id="the-core-dilemma">The Core Dilemma</h3>
<p>Multiple readers can safely access shared data simultaneously (reads don't conflict), but writers need exclusive access (writes conflict with both reads and other writes). The readers-writers lock optimizes for read-heavy workloads.</p>
<div>
<div>Access Compatibility Matrix</div>
<div>
<table>
<tr>
<th></th>
<th>Reader Holding</th>
<th>Writer Holding</th>
</tr>
<tr>
<td>Reader Wants</td>
<td>ALLOWED</td>
<td>BLOCKED</td>
</tr>
<tr>
<td>Writer Wants</td>
<td>BLOCKED</td>
<td>BLOCKED</td>
</tr>
</table>
</div>
</div>
<h3 id="preference-policies-and-starvation">Preference Policies and Starvation</h3>
<div>
<div>
<div>
<div>Reader-Preference</div>
<div>
  New readers can acquire lock even if writer is waiting. Maximizes read throughput.
</div>
<div>
<div>Risk:</div>
<div>Writer starvation. Continuous reader stream means writer never executes.</div>
</div>
</div>
<div>
<div>Writer-Preference</div>
<div>
  Once writer is waiting, no new readers admitted. Writer gets lock when current readers finish.
</div>
<div>
<div>Risk:</div>
<div>Reader starvation. Continuous writer stream means readers queue indefinitely.</div>
</div>
</div>
<div>
<div>Fair (FIFO)</div>
<div>
  Service requests in arrival order. No starvation for either party.
</div>
<div>
<div>Trade-off:</div>
<div>Lower throughput than preference policies. Writer arrival stops admitting new readers.</div>
</div>
</div>
</div>
</div>
<h3 id="implementation-deep-dive">Implementation Deep Dive</h3>
<div>
<div>Writer-Preference RWLock Implementation</div>
<div>
<span>// State variables</span><br/>
int readers = 0;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>// Active readers</span><br/>
int writers_waiting = 0;&nbsp;<span>// Writers in queue</span><br/>
    bool writer_active = false;<br/>
    Mutex mutex;<br/>
    CondVar can_read, can_write;<br/><br/>
<p><span>read_lock():</span><br/><br />
  lock(mutex)<br/><br />
  <span>while</span> (writer_active || writers_waiting &gt; 0):<br/><br />
    wait(can_read, mutex)  <span>// Block if writer active OR waiting</span><br/><br />
  readers++<br/><br />
  unlock(mutex)<br/><br/></p>
<p><span>read_unlock():</span><br/><br />
  lock(mutex)<br/><br />
  readers--<br/><br />
  <span>if</span> (readers == 0):<br/><br />
    signal(can_write)     <span>// Wake one waiting writer</span><br/><br />
  unlock(mutex)<br/><br/></p>
<p><span>write_lock():</span><br/><br />
  lock(mutex)<br/><br />
  writers_waiting++<br/><br />
  <span>while</span> (writer_active || readers &gt; 0):<br/><br />
    wait(can_write, mutex)<br/><br />
  writers_waiting--<br/><br />
  writer_active = true<br/><br />
  unlock(mutex)<br/><br/></p>
<p><span>write_unlock():</span><br/><br />
  lock(mutex)<br/><br />
  writer_active = false<br/><br />
  <span>if</span> (writers_waiting &gt; 0):<br/><br />
    signal(can_write)     <span>// Prioritize next writer</span><br/><br />
  <span>else</span>:<br/><br />
    broadcast(can_read)   <span>// Wake all waiting readers</span><br/><br />
  unlock(mutex)</p>
</div>
</div>
<h3 id="readers-writers-interview-questions-3-level-deep">Readers-Writers Interview Questions (3-Level Deep)</h3>
<div>
<div>Level 1: When would you use a readers-writers lock instead of a mutex?</div>
<div>
<div>Expected Answer:</div>
<div>When you have read-heavy workloads (many more reads than writes) and reads are long enough that the overhead of the more complex RWLock is justified. Examples: configuration caches (read constantly, updated rarely), in-memory databases with mostly SELECT queries, routing tables. For short critical sections or balanced read/write ratios, a simple mutex may actually be faster due to lower overhead.</div>
</div>
<div>
<div>Level 2: Your RWLock-protected cache shows 99th percentile read latency of 500ms despite reads being fast (1ms). What's wrong?</div>
<div>
<div>Expected Answer:</div>
<div>
  Likely writer starvation causing reader starvation:<br/>
1. <strong>Writer-preference lock + frequent writes:</strong> Each write request blocks new readers. If writes arrive at high rate, readers queue behind each writer.<br/>
2. <strong>Long-running writer:</strong> A slow write (e.g., full cache refresh) blocks all readers for its duration.<br/><br/>
<strong>Diagnosis:</strong> Track lock acquisition wait times separately from operation time. Check writer frequency and duration. Monitor queue lengths for read vs write waiters.<br/><br/>
<strong>Solutions:</strong> (1) Switch to fair lock if writes are frequent. (2) Batch writes to reduce acquisition frequency. (3) Use copy-on-write: writers build new version, atomically swap pointer, readers never block.
</div>
</div>
<div>
<div>Level 3: Design a readers-writers solution that provides bounded wait times for both readers and writers, regardless of workload mix.</div>
<div>
<div>Expected Answer:</div>
<div>
<strong>Phase-Fair RWLock:</strong><br/>
  Divide time into "read phases" and "write phases". During a read phase, only readers are admitted (up to a limit or time bound). Then switch to write phase where queued writers execute. This provides bounded wait: readers wait at most one write phase, writers wait at most one read phase.<br/><br/>
<p><strong>Ticket-Based Fairness:</strong><br/><br />
Each request gets a ticket number. Service in ticket order, but batch consecutive readers together. Writer ticket N waits only for readers with tickets less than N. Provides FIFO guarantees while still allowing reader concurrency.<br/><br/></p>
<p><strong>Optimistic Concurrency (Best for Read-Heavy):</strong><br/><br />
Readers proceed without locking using a sequence counter. Read start: save sequence. Read end: check if sequence changed (writer intervened). If changed, retry. Writers increment sequence before and after. Readers never block writers; worst case is reader retry. See <a href="/topic/system-design/seqlocks">[seqlocks]</a>.<br/><br/></p>
<p><strong>Trade-off:</strong> Phase-fair adds complexity and may reduce throughput vs simpler policies. Optimistic concurrency requires idempotent reads and has retry overhead under write contention.</p>
</div>
</div>
</div>
</div>
</div>
<hr />
<h2 id="mutex-vs-semaphore">Mutex vs Semaphore</h2>
<h3 id="fundamental-differences">Fundamental Differences</h3>
<div>
<div>Mutex vs Semaphore Comparison</div>
<div>
<div>
<div>Mutex (Mutual Exclusion)</div>
<div>
<div><strong>State:</strong> Binary (locked/unlocked)</div>
<div><strong>Ownership:</strong> Yes. Only the locking thread can unlock.</div>
<div><strong>Semantics:</strong> Protects a critical section. Acquire-use-release pattern.</div>
<div><strong>Priority Inheritance:</strong> Supported (prevents priority inversion)</div>
<div><strong>Use Case:</strong> Protecting shared data from concurrent access</div>
</div>
</div>
<div>
<div>Semaphore</div>
<div>
<div><strong>State:</strong> Counter (0 to N)</div>
<div><strong>Ownership:</strong> No. Any thread can signal/wait.</div>
<div><strong>Semantics:</strong> Resource counting. P (wait/decrement) and V (signal/increment).</div>
<div><strong>Priority Inheritance:</strong> Not applicable (no ownership)</div>
<div><strong>Use Case:</strong> Limiting concurrent access, signaling between threads</div>
</div>
</div>
</div>
</div>
<div>
<div>Critical Distinction: Ownership</div>
<div>A mutex has an owner (the thread that locked it). This enables: (1) <strong>Recursive locking:</strong> Same thread can acquire multiple times without deadlock. (2) <strong>Priority inheritance:</strong> If high-priority thread waits for mutex held by low-priority thread, the holder's priority is boosted. (3) <strong>Debugging:</strong> Can identify which thread holds a contended lock. Semaphores have no owner, so none of these apply.</div>
</div>
<h3 id="binary-semaphore-vs-mutex-a-common-misconception">Binary Semaphore vs Mutex: A Common Misconception</h3>
<div>
<div>A Binary Semaphore is NOT a Mutex</div>
<div>
    A binary semaphore (count = 1) and mutex both provide mutual exclusion, but they differ semantically:
</div>
<div>
<div>
<div>Bug with Binary Semaphore</div>
<div>
  sem = Semaphore(1)<br/><br/>
<span>// Thread A</span><br/>
  sem.wait()<br/>
  critical_section()<br/>
<span>// Forgets to signal!</span><br/><br/>
<span>// Thread B accidentally</span><br/>
sem.signal()&nbsp;<span>// "Unlocks" for Thread A!</span>
</div>
</div>
<div>
<div>Mutex Prevents This</div>
<div>
  mtx = Mutex()<br/><br/>
<span>// Thread A</span><br/>
  mtx.lock()<br/>
  critical_section()<br/>
<span>// Forgets to unlock!</span><br/><br/>
<span>// Thread B</span><br/>
mtx.unlock()&nbsp;<span>// ERROR: Not owner!</span>
</div>
</div>
</div>
</div>
<h3 id="semaphore-use-cases">Semaphore Use Cases</h3>
<div>
<div>
<div>
<div>Resource Pool Limiting</div>
<div>
  Limit concurrent access to N identical resources (database connections, file handles).
</div>
<div>
  pool_sem = Semaphore(MAX_CONNECTIONS)<br/>
pool_sem.wait()&nbsp;&nbsp;<span>// Blocks if pool empty</span><br/>
  conn = pool.get()<br/>
  use(conn)<br/>
  pool.release(conn)<br/>
  pool_sem.signal()
</div>
</div>
<div>
<div>Event Signaling</div>
<div>
  One thread signals completion to another. No shared data, just coordination.
</div>
<div>
  ready = Semaphore(0)<br/><br/>
<span>// Producer</span><br/>
  prepare_data()<br/>
  ready.signal()<br/><br/>
<span>// Consumer</span><br/>
  ready.wait()<br/>
  use_data()
</div>
</div>
</div>
</div>
<h3 id="mutex-vs-semaphore-interview-questions-3-level-deep">Mutex vs Semaphore Interview Questions (3-Level Deep)</h3>
<div>
<div>Level 1: When would you use a semaphore instead of a mutex?</div>
<div>
<div>Expected Answer:</div>
<div>
  Use semaphore when: (1) You need to limit access to N resources (counting semaphore for connection pools). (2) You need signaling between threads without protecting shared data. (3) The thread that acquires doesn't need to be the one that releases.<br/><br/>
  Use mutex when: (1) Protecting a critical section with shared state. (2) You need ownership semantics (recursive locking, priority inheritance). (3) You want enforcement that only the holder can release.
</div>
</div>
<div>
<div>Level 2: What is priority inversion and how do mutexes help?</div>
<div>
<div>Expected Answer:</div>
<div>
<strong>Priority inversion:</strong> High-priority thread H waits for mutex held by low-priority thread L. Medium-priority thread M preempts L. Now H is effectively blocked by M, inverting priorities. Famous case: Mars Pathfinder mission (1997) had system resets due to this.<br/><br/>
<strong>Solution: Priority Inheritance Protocol.</strong> When H blocks on mutex held by L, L temporarily inherits H's priority. L can't be preempted by M, finishes quickly, releases mutex, H proceeds. Requires ownership tracking (hence mutex, not semaphore).<br/><br/>
<strong>Alternative: Priority Ceiling Protocol.</strong> Each mutex has a ceiling (highest priority of any thread that might use it). Thread holding mutex runs at ceiling priority. Prevents priority inversion and deadlock but requires knowing all potential users upfront.
</div>
</div>
<div>
<div>Level 3: Design a rate limiter using semaphores that allows N requests per second with burst capability.</div>
<div>
<div>Expected Answer:</div>
<div>
<strong>Token Bucket with Semaphore:</strong><br/>
  Initialize semaphore with burst capacity B. A refiller thread adds tokens at rate N/sec (signals semaphore) up to max B. Requests wait on semaphore.<br/><br/>
<p><strong>Implementation:</strong><br/><br />
<code>tokens = Semaphore(BURST_CAPACITY)</code><br/><br />
<code>refiller: every (1/N) seconds: if count &lt; BURST: signal(tokens)</code><br/><br />
<code>request: tokens.wait(timeout); process(); // No need to signal back</code><br/><br/></p>
<p><strong>Edge cases:</strong><br/></p>
<ol>
<li><strong>Refiller precision:</strong> OS timer resolution limits rate accuracy. Batch refills (add 10 tokens every 10/N seconds) for better precision.<br/></li>
<li><strong>Graceful degradation:</strong> Use timed wait so requests get &quot;rejected&quot; response rather than blocking forever.<br/></li>
<li><strong>Distributed version:</strong> Replace semaphore with Redis INCR + TTL or distributed rate limiter. See <a href="/topic/system-design/rate-limiting">[rate-limiting]</a>.<br/><br/></li>
</ol>
<p><strong>Why semaphore fits:</strong> Token &quot;producer&quot; (refiller) and &quot;consumer&quot; (requests) are different threads. No ownership needed. Counting semantics match token counting.</p>
</div>
</div>
</div>
</div>
</div>
<hr />
<h2 id="deadlock-prevention">Deadlock Prevention</h2>
<h3 id="the-four-coffman-conditions">The Four Coffman Conditions</h3>
<p>Deadlock requires ALL four conditions to hold simultaneously. Preventing any one condition prevents deadlock.</p>
<div>
<div>The Four Coffman Conditions for Deadlock</div>
<div>
<div>
<div>1. Mutual Exclusion</div>
<div>At least one resource must be held in non-shareable mode.</div>
<div>
<div>Prevention:</div>
<div>Make resources shareable where possible (readers-writers, copy-on-write).</div>
</div>
</div>
<div>
<div>2. Hold and Wait</div>
<div>Thread holds resources while waiting for additional resources.</div>
<div>
<div>Prevention:</div>
<div>Acquire all resources atomically upfront, or release held resources before requesting new ones.</div>
</div>
</div>
<div>
<div>3. No Preemption</div>
<div>Resources cannot be forcibly taken from threads holding them.</div>
<div>
<div>Prevention:</div>
<div>Allow preemption: if request fails, release all held resources and retry. Requires rollback capability.</div>
</div>
</div>
<div>
<div>4. Circular Wait</div>
<div>Circular chain of threads, each waiting for resource held by next.</div>
<div>
<div>Prevention:</div>
<div>Impose total ordering on resource acquisition. Always acquire in same order (e.g., by address, by ID).</div>
</div>
</div>
</div>
</div>
<h3 id="lock-ordering-the-most-common-prevention-strategy">Lock Ordering: The Most Common Prevention Strategy</h3>
<div>
<div>Establishing Lock Hierarchy</div>
<div>
<div>
<div>Deadlock Prone</div>
<div>
<span>// Thread 1</span><br/>
  lock(account_A)<br/>
lock(account_B)&nbsp;&nbsp;<span>// Waits if Thread 2 holds B</span><br/>
  transfer(A, B)<br/>
  unlock(B); unlock(A)<br/><br/>
<span>// Thread 2</span><br/>
  lock(account_B)<br/>
lock(account_A)&nbsp;&nbsp;<span>// Waits if Thread 1 holds A</span><br/>
  transfer(B, A)<br/>
  unlock(A); unlock(B)
</div>
</div>
<div>
<div>Deadlock Free (Ordered)</div>
<div>
<span>// Both threads use same order</span><br/>
<span>first = min(A.id, B.id)</span><br/>
<span>second = max(A.id, B.id)</span><br/><br/>
  lock(first)<br/>
  lock(second)<br/>
  transfer(from, to)<br/>
  unlock(second)<br/>
  unlock(first)
</div>
</div>
</div>
<div>
<div>Implementation Strategies:</div>
<div>
<strong>By ID:</strong> Sort resources by unique identifier. Works for any resource with comparable ID.<br/>
<strong>By Address:</strong> Use memory address as ordering key. Language-dependent (stable in C/C++, not Java).<br/>
<strong>By Hierarchy Level:</strong> Assign levels to lock types. Only acquire locks at higher levels than currently held.
</div>
</div>
</div>
<h3 id="timeout-based-deadlock-avoidance">Timeout-Based Deadlock Avoidance</h3>
<div>
<div>Try-Lock with Backoff</div>
<div>
<span>// Acquire multiple locks without deadlock risk</span><br/>
    def acquire_locks(lock_a, lock_b, max_retries=10):<br/>
    &nbsp;&nbsp;for attempt in range(max_retries):<br/>
    &nbsp;&nbsp;&nbsp;&nbsp;if lock_a.try_lock(timeout=50ms):<br/>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if lock_b.try_lock(timeout=50ms):<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return True&nbsp;&nbsp;<span>// Success!</span><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock_a.unlock()&nbsp;&nbsp;<span>// Release and retry</span><br/>
    &nbsp;&nbsp;&nbsp;&nbsp;<br/>
&nbsp;&nbsp;&nbsp;&nbsp;<span>// Exponential backoff with jitter</span><br/>
    &nbsp;&nbsp;&nbsp;&nbsp;sleep(random(0, 2^attempt * 10ms))<br/>
    &nbsp;&nbsp;<br/>
&nbsp;&nbsp;return False&nbsp;&nbsp;<span>// Failed after retries</span>
</div>
<div>
<div>Trade-offs:</div>
<div>
<strong>Pros:</strong> No global coordination needed. Works when lock order can't be predetermined.<br/>
<strong>Cons:</strong> Livelock risk (threads keep releasing and retrying forever). Wasted work if transaction rolled back. Timeout tuning is workload-dependent.
</div>
</div>
</div>
<h3 id="deadlock-detection-and-recovery">Deadlock Detection and Recovery</h3>
<div>
<div>Wait-For Graph (Runtime Detection)</div>
<div>
    Maintain a directed graph: edge from T1 to T2 if T1 waits for resource held by T2. Cycle in graph = deadlock.
</div>
<div>
<div>
<div>Detection Algorithm</div>
<div>
  Periodically run cycle detection (DFS). Frequency trade-off: too often wastes CPU, too rare delays recovery. Typical: every few seconds or on wait timeout.
</div>
</div>
<div>
<div>Recovery Options</div>
<div>
1. <strong>Victim selection:</strong> Abort one thread in cycle (choose by age, priority, work done).<br/>
2. <strong>Resource preemption:</strong> Forcibly take resource, rollback holder's transaction.<br/>
3. <strong>Process termination:</strong> Kill deadlocked processes (last resort).
</div>
</div>
</div>
</div>
<h3 id="deadlock-prevention-interview-questions-3-level-deep">Deadlock Prevention Interview Questions (3-Level Deep)</h3>
<div>
<div>Level 1: What is a deadlock and how do you prevent it?</div>
<div>
<div>Expected Answer:</div>
<div>Deadlock is when two or more threads are permanently blocked, each waiting for a resource held by another. Classic example: Thread A holds Lock1 and waits for Lock2; Thread B holds Lock2 and waits for Lock1. Prevention: (1) Always acquire locks in a consistent global order. (2) Use timeouts with try-lock and backoff. (3) Acquire all needed locks atomically upfront. (4) Design to minimize lock requirements.</div>
</div>
<div>
<div>Level 2: You've implemented lock ordering but still see occasional hangs. What could be wrong?</div>
<div>
<div>Expected Answer:</div>
<div>
<strong>Possible causes:</strong><br/>
1. <strong>Incomplete ordering:</strong> Some code path acquires locks in different order. Use static analysis tools (e.g., ThreadSanitizer, Java's jstack) to detect.<br/>
2. <strong>Hidden locks:</strong> Library code or callbacks acquire locks you don't control. Calling unknown code while holding lock is dangerous.<br/>
3. <strong>Lock-order inversion via callbacks:</strong> Thread A: lock(X) -> callback -> lock(Y). Thread B: lock(Y) -> calls into A's code -> lock(X). Callback creates hidden dependency.<br/>
4. <strong>Database deadlocks:</strong> Application-level ordering doesn't help if database rows are locked in different orders.<br/>
5. <strong>Distributed deadlock:</strong> Lock ordering works within one process, but distributed locks across services need global coordination.<br/><br/>
<strong>Debugging:</strong> Get thread dumps during hang. Analyze wait-for relationships. Add lock acquisition logging with timestamps.
</div>
</div>
<div>
<div>Level 3: Design a distributed transaction system that prevents deadlocks across multiple database shards.</div>
<div>
<div>Expected Answer:</div>
<div>
<strong>Option 1: Global Lock Ordering</strong><br/>
  Assign global IDs to all lockable resources across shards. Sort and acquire in ID order. Challenge: requires knowing all resources upfront, adds latency for sorted acquisition.<br/><br/>
<p><strong>Option 2: Wound-Wait or Wait-Die (Timestamp-Based)</strong><br/><br />
Assign timestamps to transactions at start. When T1 wants lock held by T2:<br/></p>
<ul>
<li><strong>Wound-Wait:</strong> If T1 older, wound (abort) T2. If T1 younger, T1 waits.<br/></li>
<li><strong>Wait-Die:</strong> If T1 older, T1 waits. If T1 younger, T1 dies (aborts).<br/><br />
No cycles possible since younger transactions never wait for older ones. Used in Google Spanner.<br/><br/></li>
</ul>
<p><strong>Option 3: Optimistic Concurrency Control</strong><br/><br />
Don't acquire locks during transaction. At commit time, validate no conflicts. If conflict, abort and retry. No locks = no deadlocks. Works well for low-contention workloads. See <a href="/topic/system-design/distributed-transactions">[distributed-transactions]</a>.<br/><br/></p>
<p><strong>Option 4: Two-Phase Locking with Timeout</strong><br/><br />
Standard 2PL but with lock timeout. On timeout, abort transaction and retry with exponential backoff. Simple but may cause high abort rates under contention.<br/><br/></p>
<p><strong>Trade-offs:</strong> Global ordering adds coordination latency. Timestamp methods may abort viable transactions. OCC has high abort rates under contention. Timeout wastes work on aborted transactions. Choice depends on workload: read-heavy favors OCC, write-heavy favors timestamp ordering.</p>
</div>
</div>
</div>
</div>
</div>
<hr />
<h2 id="code-implementation">Code Implementation</h2>
<h3 id="production-ready-thread-pool">Production-Ready Thread Pool</h3>
<pre><code class="language-python">import threading
import queue
import time
import logging
from dataclasses import dataclass, field
from typing import Callable, Any, Optional, List, Dict
from contextlib import contextmanager
from enum import Enum
import random

logger = logging.getLogger(__name__)


class RejectionPolicy(Enum):
    &quot;&quot;&quot;Policy when queue is full and max threads reached.&quot;&quot;&quot;
    ABORT = &quot;abort&quot;           # Raise exception
    CALLER_RUNS = &quot;caller&quot;    # Execute in calling thread (backpressure)
    DISCARD = &quot;discard&quot;       # Silently drop
    DISCARD_OLDEST = &quot;oldest&quot; # Drop oldest in queue


@dataclass
class Task:
    &quot;&quot;&quot;Represents a unit of work with metadata for observability.&quot;&quot;&quot;
    func: Callable
    args: tuple = ()
    kwargs: dict = field(default_factory=dict)
    callback: Optional[Callable[[Any], None]] = None
    error_callback: Optional[Callable[[Exception], None]] = None
    priority: int = 0
    created_at: float = field(default_factory=time.time)
    task_id: str = field(default_factory=lambda: f&quot;task-{random.randint(0, 999999):06d}&quot;)

    def __lt__(self, other: 'Task') -&gt; bool:
        # Lower priority number = higher priority
        # Tie-break by creation time (FIFO within same priority)
        if self.priority != other.priority:
            return self.priority &lt; other.priority
        return self.created_at &lt; other.created_at


class ThreadPoolMetrics:
    &quot;&quot;&quot;Thread-safe metrics collection for the pool.&quot;&quot;&quot;

    def __init__(self):
        self._lock = threading.Lock()
        self.tasks_submitted = 0
        self.tasks_completed = 0
        self.tasks_failed = 0
        self.tasks_rejected = 0
        self.total_wait_time_ms = 0.0
        self.total_execution_time_ms = 0.0

    def record_submission(self):
        with self._lock:
            self.tasks_submitted += 1

    def record_completion(self, wait_ms: float, exec_ms: float):
        with self._lock:
            self.tasks_completed += 1
            self.total_wait_time_ms += wait_ms
            self.total_execution_time_ms += exec_ms

    def record_failure(self):
        with self._lock:
            self.tasks_failed += 1

    def record_rejection(self):
        with self._lock:
            self.tasks_rejected += 1

    def snapshot(self) -&gt; Dict[str, Any]:
        with self._lock:
            avg_wait = (self.total_wait_time_ms / self.tasks_completed
                       if self.tasks_completed &gt; 0 else 0)
            avg_exec = (self.total_execution_time_ms / self.tasks_completed
                       if self.tasks_completed &gt; 0 else 0)
            return {
                &quot;tasks_submitted&quot;: self.tasks_submitted,
                &quot;tasks_completed&quot;: self.tasks_completed,
                &quot;tasks_failed&quot;: self.tasks_failed,
                &quot;tasks_rejected&quot;: self.tasks_rejected,
                &quot;avg_wait_time_ms&quot;: round(avg_wait, 2),
                &quot;avg_execution_time_ms&quot;: round(avg_exec, 2),
            }


class ThreadPool:
    &quot;&quot;&quot;
    Production-ready thread pool with:
    - Priority queue with configurable capacity
    - Graceful shutdown with timeout
    - Comprehensive metrics
    - Configurable rejection policies
    - Auto-scaling between min and max workers
    &quot;&quot;&quot;

    def __init__(
        self,
        name: str = &quot;pool&quot;,
        min_workers: int = 2,
        max_workers: int = 10,
        queue_size: int = 1000,
        rejection_policy: RejectionPolicy = RejectionPolicy.ABORT,
        idle_timeout_seconds: float = 60.0
    ):
        if min_workers &lt; 1:
            raise ValueError(&quot;min_workers must be &gt;= 1&quot;)
        if max_workers &lt; min_workers:
            raise ValueError(&quot;max_workers must be &gt;= min_workers&quot;)

        self.name = name
        self.min_workers = min_workers
        self.max_workers = max_workers
        self.rejection_policy = rejection_policy
        self.idle_timeout = idle_timeout_seconds

        # Use PriorityQueue for task ordering
        self._task_queue: queue.PriorityQueue = queue.PriorityQueue(maxsize=queue_size)
        self._workers: List[threading.Thread] = []
        self._active_count = 0
        self._active_lock = threading.Lock()

        self._shutdown_event = threading.Event()
        self._pool_lock = threading.Lock()

        self.metrics = ThreadPoolMetrics()

        # Start minimum workers
        for i in range(min_workers):
            self._spawn_worker(core=True)

        logger.info(f&quot;ThreadPool '{name}' started with {min_workers} workers&quot;)

    def _spawn_worker(self, core: bool = False):
        &quot;&quot;&quot;Spawn a new worker thread.&quot;&quot;&quot;
        worker_id = len(self._workers)
        worker = threading.Thread(
            target=self._worker_loop,
            args=(worker_id, core),
            daemon=True,
            name=f&quot;{self.name}-worker-{worker_id}&quot;
        )
        self._workers.append(worker)
        worker.start()

    def _worker_loop(self, worker_id: int, is_core: bool):
        &quot;&quot;&quot;
        Main worker loop. Core workers never terminate;
        non-core workers terminate after idle timeout.
        &quot;&quot;&quot;
        idle_since: Optional[float] = None

        while not self._shutdown_event.is_set():
            try:
                # Shorter timeout for non-core workers to check idle status
                timeout = 1.0 if is_core else min(1.0, self.idle_timeout / 10)
                priority, task = self._task_queue.get(timeout=timeout)
                idle_since = None  # Reset idle timer

            except queue.Empty:
                # Check if non-core worker should terminate
                if not is_core:
                    if idle_since is None:
                        idle_since = time.time()
                    elif time.time() - idle_since &gt; self.idle_timeout:
                        logger.debug(f&quot;Worker {worker_id} terminating due to idle timeout&quot;)
                        return
                continue

            # Track timing
            wait_time_ms = (time.time() - task.created_at) * 1000
            exec_start = time.time()

            with self._active_lock:
                self._active_count += 1

            try:
                result = task.func(*task.args, **task.kwargs)
                exec_time_ms = (time.time() - exec_start) * 1000
                self.metrics.record_completion(wait_time_ms, exec_time_ms)

                if task.callback:
                    try:
                        task.callback(result)
                    except Exception as cb_err:
                        logger.error(f&quot;Callback error for {task.task_id}: {cb_err}&quot;)

            except Exception as e:
                self.metrics.record_failure()
                logger.error(f&quot;Task {task.task_id} failed: {e}&quot;, exc_info=True)

                if task.error_callback:
                    try:
                        task.error_callback(e)
                    except Exception as cb_err:
                        logger.error(f&quot;Error callback failed: {cb_err}&quot;)
            finally:
                with self._active_lock:
                    self._active_count -= 1
                self._task_queue.task_done()

    def submit(
        self,
        func: Callable,
        *args,
        priority: int = 0,
        callback: Optional[Callable[[Any], None]] = None,
        error_callback: Optional[Callable[[Exception], None]] = None,
        **kwargs
    ) -&gt; Optional[str]:
        &quot;&quot;&quot;
        Submit a task to the pool.
        Returns task_id if queued, None if rejected.
        Raises RuntimeError if pool is shutdown.
        &quot;&quot;&quot;
        if self._shutdown_event.is_set():
            raise RuntimeError(f&quot;ThreadPool '{self.name}' is shut down&quot;)

        task = Task(
            func=func,
            args=args,
            kwargs=kwargs,
            callback=callback,
            error_callback=error_callback,
            priority=priority
        )

        # Try to queue the task
        try:
            self._task_queue.put_nowait((priority, task))
            self.metrics.record_submission()

            # Consider scaling up
            self._maybe_scale_up()

            return task.task_id

        except queue.Full:
            return self._handle_rejection(task)

    def _maybe_scale_up(self):
        &quot;&quot;&quot;Spawn additional worker if needed and allowed.&quot;&quot;&quot;
        with self._pool_lock:
            queue_depth = self._task_queue.qsize()
            worker_count = len([w for w in self._workers if w.is_alive()])

            # Scale up if queue is building and we haven't hit max
            if queue_depth &gt; worker_count and worker_count &lt; self.max_workers:
                self._spawn_worker(core=False)
                logger.debug(f&quot;Scaled up to {worker_count + 1} workers&quot;)

    def _handle_rejection(self, task: Task) -&gt; Optional[str]:
        &quot;&quot;&quot;Handle task when queue is full.&quot;&quot;&quot;
        self.metrics.record_rejection()

        if self.rejection_policy == RejectionPolicy.ABORT:
            raise queue.Full(f&quot;Task queue full, task {task.task_id} rejected&quot;)

        elif self.rejection_policy == RejectionPolicy.CALLER_RUNS:
            # Execute in calling thread - provides natural backpressure
            logger.warning(f&quot;Queue full, executing {task.task_id} in caller thread&quot;)
            try:
                result = task.func(*task.args, **task.kwargs)
                if task.callback:
                    task.callback(result)
            except Exception as e:
                if task.error_callback:
                    task.error_callback(e)
            return task.task_id

        elif self.rejection_policy == RejectionPolicy.DISCARD:
            logger.warning(f&quot;Discarding task {task.task_id} due to full queue&quot;)
            return None

        elif self.rejection_policy == RejectionPolicy.DISCARD_OLDEST:
            try:
                _, oldest = self._task_queue.get_nowait()
                logger.warning(f&quot;Discarding oldest task {oldest.task_id}&quot;)
                self._task_queue.put_nowait((task.priority, task))
                return task.task_id
            except queue.Empty:
                return None

        return None

    def get_stats(self) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Get current pool statistics.&quot;&quot;&quot;
        with self._active_lock:
            active = self._active_count

        alive_workers = len([w for w in self._workers if w.is_alive()])

        return {
            &quot;name&quot;: self.name,
            &quot;workers_alive&quot;: alive_workers,
            &quot;workers_active&quot;: active,
            &quot;workers_idle&quot;: alive_workers - active,
            &quot;queue_depth&quot;: self._task_queue.qsize(),
            &quot;shutdown&quot;: self._shutdown_event.is_set(),
            **self.metrics.snapshot()
        }

    def shutdown(self, wait: bool = True, timeout: float = 30.0):
        &quot;&quot;&quot;
        Initiate graceful shutdown.
        If wait=True, blocks until workers finish or timeout expires.
        &quot;&quot;&quot;
        logger.info(f&quot;Shutting down ThreadPool '{self.name}'...&quot;)
        self._shutdown_event.set()

        if wait:
            deadline = time.time() + timeout
            for worker in self._workers:
                remaining = max(0, deadline - time.time())
                worker.join(timeout=remaining)
                if worker.is_alive():
                    logger.warning(f&quot;Worker {worker.name} did not terminate in time&quot;)

        logger.info(f&quot;ThreadPool '{self.name}' shutdown complete&quot;)


class ReadWriteLock:
    &quot;&quot;&quot;
    Readers-writers lock with writer preference.
    Multiple readers can hold the lock simultaneously.
    Writers get exclusive access and priority over waiting readers.
    &quot;&quot;&quot;

    def __init__(self):
        self._lock = threading.Lock()
        self._readers_ok = threading.Condition(self._lock)
        self._writers_ok = threading.Condition(self._lock)

        self._readers = 0
        self._writers_waiting = 0
        self._writer_active = False

    @contextmanager
    def read_lock(self):
        &quot;&quot;&quot;Acquire read lock (shared access).&quot;&quot;&quot;
        with self._lock:
            # Wait if writer is active or writers are waiting (writer preference)
            while self._writer_active or self._writers_waiting &gt; 0:
                self._readers_ok.wait()
            self._readers += 1

        try:
            yield
        finally:
            with self._lock:
                self._readers -= 1
                # If last reader and writers waiting, signal one writer
                if self._readers == 0 and self._writers_waiting &gt; 0:
                    self._writers_ok.notify()

    @contextmanager
    def write_lock(self):
        &quot;&quot;&quot;Acquire write lock (exclusive access).&quot;&quot;&quot;
        with self._lock:
            self._writers_waiting += 1
            try:
                # Wait until no readers and no active writer
                while self._readers &gt; 0 or self._writer_active:
                    self._writers_ok.wait()
                self._writer_active = True
            finally:
                self._writers_waiting -= 1

        try:
            yield
        finally:
            with self._lock:
                self._writer_active = False
                # Prefer writers; if none waiting, wake all readers
                if self._writers_waiting &gt; 0:
                    self._writers_ok.notify()
                else:
                    self._readers_ok.notify_all()


class BoundedSemaphore:
    &quot;&quot;&quot;
    Counting semaphore with upper bound.
    Useful for resource pools where you want to detect over-release bugs.
    &quot;&quot;&quot;

    def __init__(self, value: int):
        if value &lt; 0:
            raise ValueError(&quot;Semaphore value must be &gt;= 0&quot;)
        self._value = value
        self._max_value = value
        self._lock = threading.Lock()
        self._not_zero = threading.Condition(self._lock)

    def acquire(self, blocking: bool = True, timeout: Optional[float] = None) -&gt; bool:
        &quot;&quot;&quot;
        Acquire the semaphore (decrement counter).
        Returns True if acquired, False if timeout expired.
        &quot;&quot;&quot;
        with self._not_zero:
            if not blocking:
                if self._value &lt;= 0:
                    return False
                self._value -= 1
                return True

            deadline = time.time() + timeout if timeout else None

            while self._value &lt;= 0:
                remaining = None
                if deadline:
                    remaining = deadline - time.time()
                    if remaining &lt;= 0:
                        return False

                if not self._not_zero.wait(timeout=remaining):
                    return False  # Timeout

            self._value -= 1
            return True

    def release(self):
        &quot;&quot;&quot;
        Release the semaphore (increment counter).
        Raises ValueError if releasing would exceed initial value.
        &quot;&quot;&quot;
        with self._not_zero:
            if self._value &gt;= self._max_value:
                raise ValueError(&quot;Semaphore released too many times&quot;)
            self._value += 1
            self._not_zero.notify()

    @property
    def available(self) -&gt; int:
        with self._lock:
            return self._value

    def __enter__(self):
        self.acquire()
        return self

    def __exit__(self, *args):
        self.release()


def acquire_locks_safely(*locks, timeout: float = 5.0, max_retries: int = 10) -&gt; bool:
    &quot;&quot;&quot;
    Acquire multiple locks without deadlock using timeout and backoff.

    Args:
        *locks: Locks to acquire (must support acquire(timeout=...))
        timeout: Timeout for each lock acquisition attempt
        max_retries: Maximum retry attempts

    Returns:
        True if all locks acquired, False otherwise.
        Caller must release locks if True is returned.
    &quot;&quot;&quot;
    acquired = []

    for attempt in range(max_retries):
        acquired.clear()
        success = True

        for lock in locks:
            if hasattr(lock, 'acquire'):
                if lock.acquire(timeout=timeout / len(locks)):
                    acquired.append(lock)
                else:
                    success = False
                    break
            else:
                # Assume it's a context manager style lock
                raise TypeError(f&quot;Lock {lock} doesn't support timeout-based acquire&quot;)

        if success:
            return True

        # Release any acquired locks
        for lock in reversed(acquired):
            lock.release()

        # Exponential backoff with jitter
        backoff = min(1.0, (2 ** attempt) * 0.01)
        jitter = random.uniform(0, backoff)
        time.sleep(backoff + jitter)

    return False


# === Usage Examples ===

if __name__ == &quot;__main__&quot;:
    logging.basicConfig(level=logging.INFO)

    # Thread pool example
    pool = ThreadPool(
        name=&quot;example&quot;,
        min_workers=2,
        max_workers=8,
        queue_size=100,
        rejection_policy=RejectionPolicy.CALLER_RUNS
    )

    def work(x):
        time.sleep(0.1)
        return x * 2

    results = []

    for i in range(20):
        pool.submit(
            work, i,
            callback=lambda r: results.append(r),
            priority=i % 3  # Mix of priorities
        )

    time.sleep(3)
    print(f&quot;Results: {sorted(results)}&quot;)
    print(f&quot;Stats: {pool.get_stats()}&quot;)
    pool.shutdown()

    # ReadWriteLock example
    rwlock = ReadWriteLock()
    shared_data = {&quot;value&quot;: 0}

    def reader(reader_id):
        with rwlock.read_lock():
            print(f&quot;Reader {reader_id} sees: {shared_data['value']}&quot;)
            time.sleep(0.1)

    def writer(writer_id, new_value):
        with rwlock.write_lock():
            shared_data['value'] = new_value
            print(f&quot;Writer {writer_id} set: {new_value}&quot;)
            time.sleep(0.2)

    threads = []
    for i in range(5):
        threads.append(threading.Thread(target=reader, args=(i,)))
    threads.append(threading.Thread(target=writer, args=(0, 42)))
    for i in range(5, 8):
        threads.append(threading.Thread(target=reader, args=(i,)))

    for t in threads:
        t.start()
    for t in threads:
        t.join()
</code></pre>
<hr />
<h2 id="quick-reference-card">Quick Reference Card</h2>
<div>
<div>Concurrency Patterns Cheat Sheet</div>
<div>
<div>
<div>Thread Pool Sizing</div>
<div>
<strong>CPU-bound:</strong> N = cores + 1<br/>
<strong>I/O-bound:</strong> N = cores x (1 + W/C)<br/>
<strong>Mixed:</strong> Separate pools or managed blocking<br/>
<strong>Start:</strong> 2x cores, then benchmark
</div>
</div>
<div>
<div>Deadlock Prevention</div>
<div>
  1. Lock ordering (by ID/address)<br/>
  2. Timeout + exponential backoff<br/>
  3. Lock hierarchy levels<br/>
  4. Acquire all upfront or none
</div>
</div>
<div>
<div>When to Use What</div>
<div>
<strong>Mutex:</strong> Protecting shared state<br/>
<strong>Semaphore:</strong> Resource counting, signaling<br/>
<strong>RWLock:</strong> Read-heavy, long reads<br/>
<strong>Condition:</strong> Wait for state change
</div>
</div>
<div>
<div>Common Pitfalls</div>
<div>
        - if instead of while (spurious wakeup)<br/>
        - Unbounded queue = OOM risk<br/>
        - Calling unknown code holding lock<br/>
        - Binary semaphore != mutex
</div>
</div>
</div>
</div>
<div>
<div>Coffman Conditions Summary</div>
<table>
<tr>
<th>Condition</th>
<th>Description</th>
<th>Prevention</th>
</tr>
<tr>
<td>Mutual Exclusion</td>
<td>Resource held exclusively</td>
<td>Make shareable (RWLock, COW)</td>
</tr>
<tr>
<td>Hold and Wait</td>
<td>Hold while waiting for more</td>
<td>Acquire all or none upfront</td>
</tr>
<tr>
<td>No Preemption</td>
<td>Can't forcibly take resource</td>
<td>Release on failure, retry</td>
</tr>
<tr>
<td>Circular Wait</td>
<td>Cycle in wait graph</td>
<td>Total ordering on acquisition</td>
</tr>
</table>
</div>
<hr />
<h2 id="related-topics">Related Topics</h2>
<ul>
<li><a href="/topic/system-design/distributed-locking">[distributed-locking]</a> - Coordination across processes and machines</li>
<li><a href="/topic/system-design/message-queues">[message-queues]</a> - Asynchronous producer-consumer at scale</li>
<li><a href="/topic/system-design/rate-limiting">[rate-limiting]</a> - Controlling throughput with token buckets and semaphores</li>
<li><a href="/topic/system-design/connection-pooling">[connection-pooling]</a> - Resource pool management</li>
<li><a href="/topic/system-design/bulkhead-pattern">[bulkhead-pattern]</a> - Isolation to prevent cascade failures</li>
<li><a href="/topic/system-design/distributed-transactions">[distributed-transactions]</a> - Coordinating transactions across services</li>
</ul>
