<h1 id="consensus-algorithms">Consensus Algorithms</h1>
<h2 id="overview">Overview</h2>
<p><strong>Simple Explanation</strong>: <span>Consensus algorithms</span> help multiple computers agree on a single value or decision, even when some computers fail or the network has issues. Think of it like getting a group of friends to agree on where to eat dinner - even if some friends can't hear well or leave early, the group still needs to reach a decision everyone follows.</p>
<p>In distributed systems, consensus is the foundation for building reliable databases, <a href="/topic/system-design/distributed-locking">[leader election]</a>, and configuration management. Without consensus, you can't guarantee that all nodes in a cluster see the same data in the same order. It's tightly coupled with <a href="/topic/system-design/cap-theorem">[CAP theorem]</a> trade-offs and <a href="/topic/system-design/database-replication">[database replication]</a> strategies.</p>
<div>
<div>THE CONSENSUS PROBLEM</div>
<div>Multiple nodes must agree on a single value:</div>
<div>
<div>
<div>Node A</div>
<div>proposes X</div>
</div>
<div>
<div>Node B</div>
<div>proposes Y</div>
</div>
<div>
<div>Node C</div>
<div>proposes X</div>
</div>
</div>
<div>consensus protocol</div>
<div>
<div>
<div>Node A</div>
<div>decides: X</div>
</div>
<div>
<div>Node B</div>
<div>decides: X</div>
</div>
<div>
<div>Node C</div>
<div>decides: X</div>
</div>
</div>
<div>
<span>All nodes agree on X (could have been Y, but MUST be same value)</span>
</div>
</div>
<h2 id="why-it-matters-real-company-examples">Why It Matters: Real Company Examples</h2>
<div>
<p><strong>Google Spanner</strong> uses <span>Paxos</span> to achieve global consistency across data centers. Every write goes through Paxos consensus, enabling Google to offer externally-consistent reads worldwide - something previously thought impossible at scale.</p>
<p><strong>CockroachDB and TiDB</strong> use <span>Raft</span> for distributed SQL transactions. When you run a distributed transaction, Raft ensures all replicas agree on the commit order, preventing data inconsistencies.</p>
<p><strong>Apache Kafka</strong> uses Zab (similar to Raft) in ZooKeeper for controller election and configuration management. When a Kafka broker fails, consensus determines which broker becomes the new partition leader. See <a href="/topic/system-design/message-queues">[message queues]</a>.</p>
<p><strong>Ethereum 2.0</strong> switched from Proof-of-Work to a <span>BFT-style consensus</span> (Casper) to secure $400B+ in assets while reducing energy consumption by 99.95%.</p>
<p><strong>etcd (Kubernetes)</strong> uses Raft for cluster coordination. Every Kubernetes control plane operation depends on etcd's consensus to maintain cluster state consistency.</p>
</div>
<h2 id="properties-of-consensus">Properties of Consensus</h2>
<div>
<ol>
<li><strong><span>Agreement</span></strong>: All correct nodes decide on the same value</li>
<li><strong><span>Validity</span></strong>: The decided value was proposed by some node</li>
<li><strong><span>Termination</span></strong>: All correct nodes eventually decide</li>
<li><strong><span>Integrity</span></strong>: Each node decides at most once</li>
</ol>
</div>
<div>
<p><strong>The FLP Impossibility Theorem (1985)</strong>: In an asynchronous system with even one faulty node, no deterministic consensus algorithm can guarantee all three properties simultaneously. Practical algorithms use <strong>timeouts</strong> to work around this limitation - they sacrifice pure liveness for practical termination.</p>
</div>
<hr />
<h2 id="quorum-the-foundation-of-consensus">Quorum: The Foundation of Consensus</h2>
<p><span>Quorum</span> is the minimum number of nodes that must participate in an operation for it to be valid. Understanding quorum is essential for all consensus algorithms.</p>
<div>
<div>QUORUM MATHEMATICS</div>
<div>
<div>
<div>Crash Fault Tolerance</div>
<div>
<div>n = 2f + 1</div>
<div>f = max failures tolerated</div>
</div>
<div>
<div>5 nodes: tolerates 2 failures</div>
<div>3 nodes: tolerates 1 failure</div>
<div>Quorum = majority = (n/2)+1</div>
</div>
</div>
<div>
<div>Byzantine Fault Tolerance</div>
<div>
<div>n = 3f + 1</div>
<div>f = max Byzantine failures</div>
</div>
<div>
<div>7 nodes: tolerates 2 Byzantine</div>
<div>4 nodes: tolerates 1 Byzantine</div>
<div>Quorum = 2f+1 (super-majority)</div>
</div>
</div>
</div>
<div>
<div>Why These Numbers?</div>
<div>
<div><strong>Crash (2f+1):</strong> Need overlapping majorities - any two majorities share at least one node</div>
<div><strong>Byzantine (3f+1):</strong> Need 2f+1 honest responses out of 3f+1 total to outvote f liars</div>
</div>
</div>
</div>
<h3 id="quorum-intersection-property">Quorum Intersection Property</h3>
<div>
<div>QUORUM INTERSECTION ENSURES CONSISTENCY</div>
<div>
<div>
<div>
<span>Write<br/>Quorum</span>
</div>
<div>
<span>Read<br/>Quorum</span>
</div>
<div>
<span>Shared<br/>Node</span>
</div>
</div>
<div>
<div>The Key Insight</div>
<div>
  If W + R> N, any read quorum overlaps with any write quorum. The shared node(s) have the latest write, ensuring reads see fresh data.
</div>
</div>
</div>
<div>
<div>
<div>W=3, R=3, N=5</div>
<div>Strong consistency</div>
</div>
<div>
<div>W=1, R=5, N=5</div>
<div>Fast writes, slow reads</div>
</div>
<div>
<div>W=5, R=1, N=5</div>
<div>Slow writes, fast reads</div>
</div>
</div>
</div>
<hr />
<h2 id="algorithm-comparison">Algorithm Comparison</h2>
<div>
<div>CONSENSUS ALGORITHM COMPARISON</div>
<div>
<table>
  <thead>
<tr>
<th>Algorithm</th>
<th>Fault Model</th>
<th>Nodes for f faults</th>
<th>Message Complexity</th>
<th>Used In</th>
</tr>
  </thead>
  <tbody>
<tr>
<td>Paxos</td>
<td><span>Crash</span></td>
<td>2f + 1</td>
<td>O(n)</td>
<td>Chubby, Spanner, Megastore</td>
</tr>
<tr>
<td>Multi-Paxos</td>
<td><span>Crash</span></td>
<td>2f + 1</td>
<td>O(n) amortized</td>
<td>Spanner, Chubby</td>
</tr>
<tr>
<td>Raft</td>
<td><span>Crash</span></td>
<td>2f + 1</td>
<td>O(n)</td>
<td>etcd, CockroachDB, Consul, TiKV</td>
</tr>
<tr>
<td>PBFT</td>
<td><span>Byzantine</span></td>
<td>3f + 1</td>
<td>O(n^2)</td>
<td>Hyperledger Fabric</td>
</tr>
<tr>
<td>Zab</td>
<td><span>Crash</span></td>
<td>2f + 1</td>
<td>O(n)</td>
<td>ZooKeeper, Kafka</td>
</tr>
<tr>
<td>Viewstamped Replication</td>
<td><span>Crash</span></td>
<td>2f + 1</td>
<td>O(n)</td>
<td>PBFT basis</td>
</tr>
  </tbody>
</table>
</div>
<div>
<div>
<span>Crash fault:</span>
<span> Node stops responding (fail-stop model)</span>
</div>
<div>
<span>Byzantine:</span>
<span> Node may behave maliciously or arbitrarily</span>
</div>
</div>
</div>
<hr />
<h2 id="paxos-the-foundation">Paxos: The Foundation</h2>
<p><span>Paxos</span> was introduced by Leslie Lamport in 1989 and is the theoretical foundation for most consensus algorithms. Understanding Paxos is crucial for interviews, even though Raft is more commonly implemented.</p>
<h3 id="paxos-roles">Paxos Roles</h3>
<div>
<div>PAXOS ROLES</div>
<div>
<div>
<div>Proposer</div>
<div>
<div>Proposes values to be agreed upon</div>
<div>
<strong>Actions:</strong> Prepare, Propose
</div>
</div>
</div>
<div>
<div>Acceptor</div>
<div>
<div>Votes on proposals (the "memory")</div>
<div>
<strong>Actions:</strong> Promise, Accept
</div>
</div>
</div>
<div>
<div>Learner</div>
<div>
<div>Learns the decided value</div>
<div>
<strong>Actions:</strong> Learn accepted value
</div>
</div>
</div>
</div>
<div>
<div>
<strong>Note:</strong> In practice, a single node often plays all three roles simultaneously. The separation is conceptual.
</div>
</div>
</div>
<h3 id="basic-paxos-two-phases">Basic Paxos: Two Phases</h3>
<div>
<div>BASIC PAXOS PROTOCOL</div>
<div>
<div>
<div>Phase 1: Prepare</div>
<div>
<div>
<div>Proposer</div>
<div>n = 1</div>
</div>
<div>
<div>Prepare(n=1)</div>
<div>---------------></div>
</div>
<div>
<div>
<div>A1</div>
</div>
<div>
<div>A2</div>
</div>
<div>
<div>A3</div>
</div>
</div>
</div>
<div>
<strong>Acceptor response:</strong> Promise to not accept proposals &lt; n. Return any previously accepted (n', v').
</div>
</div>
<div>
<div>Phase 2: Accept</div>
<div>
<div>
<div>Proposer</div>
<div>v = "X"</div>
</div>
<div>
<div>Accept(n=1, v="X")</div>
<div>---------------></div>
</div>
<div>
<div>
<div>A1: X</div>
</div>
<div>
<div>A2: X</div>
</div>
<div>
<div>A3: X</div>
</div>
</div>
</div>
<div>
<strong>Acceptor response:</strong> Accept if no promise to higher proposal. Value "X" is now <span>chosen</span> (majority accepted).
</div>
</div>
</div>
<div>
<div>Critical Rule: Value Selection</div>
<div>
If any acceptor returns a previously accepted value in Phase 1, the proposer <strong>MUST</strong> propose that value (with highest proposal number) in Phase 2. This ensures safety even with concurrent proposers.
</div>
</div>
</div>
<h3 id="multi-paxos-optimization">Multi-Paxos Optimization</h3>
<p>Basic Paxos requires two round trips per value. <span>Multi-Paxos</span> optimizes this by establishing a stable leader.</p>
<div>
<div>MULTI-PAXOS: AMORTIZED CONSENSUS</div>
<div>
<div>
<div>Basic Paxos (per value)</div>
<div>
<div>
<div>1. Prepare -> Promises</div>
<div>2. Accept -> Accepted</div>
<div>4 message delays per value</div>
</div>
</div>
</div>
<div>
<div>Multi-Paxos (amortized)</div>
<div>
<div>
<div>1. Prepare (once per leader)</div>
<div>2. Accept -> Accepted (per value)</div>
<div>2 message delays per value</div>
</div>
</div>
</div>
</div>
<div>
<div>
<strong>Leader lease:</strong> The leader "owns" a range of proposal numbers. Until it fails or a higher proposal appears, it skips Phase 1 for subsequent values.
</div>
</div>
</div>
<hr />
<h2 id="raft-the-understandable-consensus">Raft: The Understandable Consensus</h2>
<p><span>Raft</span> was designed by Diego Ongaro and John Ousterhout specifically to be easier to understand than Paxos while providing equivalent guarantees.</p>
<h3 id="rafts-three-sub-problems">Raft's Three Sub-Problems</h3>
<div>
<div>RAFT DECOMPOSITION</div>
<div>
<div>
<div>1</div>
<div>Leader Election</div>
<div>Choose one leader among nodes</div>
</div>
<div>
<div>2</div>
<div>Log Replication</div>
<div>Leader replicates log to followers</div>
</div>
<div>
<div>3</div>
<div>Safety</div>
<div>Guarantees for correctness</div>
</div>
</div>
</div>
<h3 id="leader-election-deep-dive">Leader Election Deep Dive</h3>
<div>
<div>RAFT LEADER ELECTION STATE MACHINE</div>
<div>
<div>
<div>FOLLOWER</div>
<div>default state</div>
</div>
<div>
<div>timeout</div>
<div>---></div>
</div>
<div>
<div>CANDIDATE</div>
<div>seeking votes</div>
</div>
<div>
<div>majority</div>
<div>---></div>
</div>
<div>
<div>LEADER</div>
<div>sends heartbeats</div>
</div>
</div>
<div>
<div>
<div>Candidate -> Follower</div>
<div>Higher term discovered</div>
</div>
<div>
<div>Leader -> Follower</div>
<div>Higher term discovered</div>
</div>
<div>
<div>Candidate -> Candidate</div>
<div>Split vote timeout</div>
</div>
</div>
<div>
<div>Election Timeout Randomization</div>
<div>
Timeouts are randomized (e.g., 150-300ms) to prevent <span>split votes</span>. If all nodes had identical timeouts, they'd all become candidates simultaneously.
</div>
</div>
</div>
<h3 id="raft-terms-and-log-structure">Raft Terms and Log Structure</h3>
<div>
<div>RAFT TERM AND LOG CONCEPTS</div>
<div>
<div>Terms: Logical Clock</div>
<div>
<div>
<div>Term 1</div>
<div>Leader A</div>
</div>
<div>
<div>Term 2</div>
<div>Election</div>
</div>
<div>
<div>Term 3</div>
<div>Leader B</div>
</div>
<div>
<div>Term 3</div>
<div>...</div>
</div>
<div>
<div>Term 4</div>
<div>Leader C</div>
</div>
</div>
<div>Each term has at most one leader. Terms act as a logical clock to detect stale information.</div>
</div>
<div>
<div>Log Structure</div>
<div>
<div>
<div>idx 1</div>
<div>t1: x=1</div>
</div>
<div>
<div>idx 2</div>
<div>t1: y=2</div>
</div>
<div>
<div>idx 3</div>
<div>t3: x=3</div>
</div>
<div>
<div>idx 4</div>
<div>t3: z=5</div>
</div>
<div>
<div>idx 5</div>
<div>t4: y=7</div>
</div>
</div>
<div>^ commitIndex = 4</div>
</div>
<div>
<div>Log Matching Property</div>
<div>
  If two logs contain an entry with the same index and term, then:
  <br/>1. They store the same command
  <br/>2. All preceding entries are identical
</div>
</div>
</div>
<h3 id="log-replication-flow">Log Replication Flow</h3>
<div>
<div>RAFT LOG REPLICATION</div>
<div>
<div>
<div>
<div>Client</div>
</div>
<div>--- SET x=5 ---></div>
<div>
<div>Leader</div>
<div>appends to log</div>
</div>
</div>
<div>
<div>
<div>
<div>AppendEntries</div>
<div>----></div>
<div>
<div>Follower 1</div>
<div>ACK</div>
</div>
</div>
<div>
<div>AppendEntries</div>
<div>----></div>
<div>
<div>Follower 2</div>
<div>ACK</div>
</div>
</div>
<div>
<div>AppendEntries</div>
<div>----></div>
<div>
<div>Follower 3</div>
<div>TIMEOUT</div>
</div>
</div>
</div>
</div>
<div>
<div>
<div>Leader</div>
<div>2/3 ACKs = majority</div>
</div>
<div>COMMIT!</div>
<div><--- OK ---</div>
<div>
<div>Client</div>
</div>
</div>
</div>
<div>
<div>Commit vs Applied</div>
<div>
<strong>Committed:</strong> Entry replicated on majority - guaranteed durable<br/>
<strong>Applied:</strong> Entry executed by state machine - produces side effects
</div>
</div>
</div>
<hr />
<h2 id="split-brain-problem">Split-Brain Problem</h2>
<p><span>Split-brain</span> occurs when a network partition causes nodes to form multiple independent clusters, each believing it's the primary. This can lead to data inconsistency and corruption.</p>
<div>
<div>SPLIT-BRAIN SCENARIO</div>
<div>
<div>
<div>Partition A</div>
<div>
<div>
<div>Leader</div>
</div>
<div>
<div>Node 2</div>
</div>
</div>
<div>2 nodes (minority)</div>
</div>
<div>
<div>NETWORK</div>
<div>PARTITION</div>
<div>X</div>
</div>
<div>
<div>Partition B</div>
<div>
<div>
<div>Node 3</div>
</div>
<div>
<div>Node 4</div>
</div>
<div>
<div>Node 5</div>
</div>
</div>
<div>3 nodes (majority)</div>
</div>
</div>
<div>
<div>
<div>Partition A (Old Leader)</div>
<div>
  Cannot commit new entries (no quorum)<br/>
Existing leader becomes <strong>read-only</strong><br/>
  Eventually steps down when term increases
</div>
</div>
<div>
<div>Partition B (New Leader)</div>
<div>
  Election timeout triggers new election<br/>
  New leader elected with higher term<br/>
  Continues accepting writes normally
</div>
</div>
</div>
<div>
<div>How Consensus Prevents Split-Brain</div>
<div>
<strong>Quorum requirement:</strong> Leader needs majority to commit. With 5 nodes, both partitions cannot have 3+ nodes.<br/>
<strong>Term numbers:</strong> Old leader's stale term is rejected when partition heals.
</div>
</div>
</div>
<h3 id="split-brain-prevention-strategies">Split-Brain Prevention Strategies</h3>
<div>
<ol>
<li><strong><span>Quorum-based writes</span></strong>: Require majority acknowledgment before committing</li>
<li><strong><span>Fencing tokens</span></strong>: Monotonically increasing tokens to detect stale leaders</li>
<li><strong><span>Leader leases</span></strong>: Time-bounded leadership with clock synchronization (see <a href="/topic/system-design/distributed-locking">[distributed locking]</a>)</li>
<li><strong><span>STONITH</span></strong>: &quot;Shoot The Other Node In The Head&quot; - forcibly kill uncertain nodes</li>
<li><strong><span>Witness nodes</span></strong>: Odd-numbered quorum helpers that don't store data</li>
</ol>
</div>
<hr />
<h2 id="byzantine-fault-tolerance-bft">Byzantine Fault Tolerance (BFT)</h2>
<p><span>Byzantine faults</span> are the most general class of failures where nodes can behave arbitrarily - including lying, sending conflicting information, or colluding maliciously.</p>
<div>
<div>BYZANTINE GENERALS PROBLEM</div>
<div>
<div>
<div>General A</div>
<div>ATTACK</div>
<div>Loyal</div>
</div>
<div>
<div>General B</div>
<div>RETREAT?</div>
<div>Traitor</div>
</div>
<div>
<div>General C</div>
<div>ATTACK</div>
<div>Loyal</div>
</div>
<div>
<div>General D</div>
<div>ATTACK</div>
<div>Loyal</div>
</div>
</div>
<div>
<div>The Problem</div>
<div>
  Traitor B tells A "I'll attack" but tells C "I'll retreat". How can loyal generals agree?
</div>
</div>
<div>
<div>BFT Solution (3f+1 nodes)</div>
<div>
  With 4 generals and 1 traitor: A, C, D share messages. Even if B lies differently to each, the 3 loyal generals see majority "ATTACK" and agree.
</div>
</div>
</div>
<h3 id="practical-bft-pbft">Practical BFT (PBFT)</h3>
<div>
<div>PBFT THREE-PHASE PROTOCOL</div>
<div>
<div>
<div>Phase 1: Pre-prepare</div>
<div>
  Primary (leader) broadcasts request to all replicas with sequence number
</div>
<div>
  Primary -> All: PRE-PREPARE(view, seq, request)
</div>
</div>
<div>
<div>Phase 2: Prepare</div>
<div>
  Each replica broadcasts PREPARE to all others. Wait for 2f+1 matching prepares.
</div>
<div>
  Replica i -> All: PREPARE(view, seq, digest, i)
</div>
</div>
<div>
<div>Phase 3: Commit</div>
<div>
  Each replica broadcasts COMMIT. Wait for 2f+1 matching commits, then execute.
</div>
<div>
  Replica i -> All: COMMIT(view, seq, i)
</div>
</div>
</div>
<div>
<div>
<div>Cost</div>
<div>
  O(n^2) messages per consensus<br/>
  Limited to ~20-100 nodes<br/>
  Higher latency than Raft/Paxos
</div>
</div>
<div>
<div>Benefit</div>
<div>
  Tolerates malicious nodes<br/>
  Essential for blockchains<br/>
  Cryptographic guarantees
</div>
</div>
</div>
</div>
<h3 id="when-to-use-bft-vs-cft">When to Use BFT vs CFT</h3>
<div>
<div>BFT vs CFT DECISION MATRIX</div>
<div>
<table>
  <thead>
<tr>
<th>Scenario</th>
<th>Recommendation</th>
<th>Why</th>
</tr>
  </thead>
  <tbody>
<tr>
<td>Internal microservices</td>
<td><span>CFT (Raft)</span></td>
<td>Trust boundary within org</td>
</tr>
<tr>
<td>Public blockchain</td>
<td><span>BFT</span></td>
<td>Untrusted participants</td>
</tr>
<tr>
<td>Consortium blockchain</td>
<td><span>BFT</span></td>
<td>Partial trust between orgs</td>
</tr>
<tr>
<td>Database replication</td>
<td><span>CFT (Raft)</span></td>
<td>Performance critical, trusted</td>
</tr>
<tr>
<td>Financial settlement</td>
<td><span>BFT or CFT+Audit</span></td>
<td>Depends on trust model</td>
</tr>
  </tbody>
</table>
</div>
</div>
<hr />
<h2 id="code-examples">Code Examples</h2>
<h3 id="python---simplified-raft-node">Python - Simplified Raft Node</h3>
<pre><code class="language-python">from enum import Enum
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any
import random
import threading
import time

class NodeState(Enum):
    FOLLOWER = &quot;follower&quot;
    CANDIDATE = &quot;candidate&quot;
    LEADER = &quot;leader&quot;

@dataclass
class LogEntry:
    term: int
    command: Any
    index: int

@dataclass
class RaftNode:
    node_id: str
    peers: List[str]

    # Persistent state (survives restarts)
    current_term: int = 0
    voted_for: Optional[str] = None
    log: List[LogEntry] = field(default_factory=list)

    # Volatile state
    state: NodeState = NodeState.FOLLOWER
    commit_index: int = 0
    last_applied: int = 0

    # Leader state (reinitialized after election)
    next_index: Dict[str, int] = field(default_factory=dict)
    match_index: Dict[str, int] = field(default_factory=dict)

    # Timing
    election_timeout: float = 0
    last_heartbeat: float = 0

    def __post_init__(self):
        self.reset_election_timeout()
        self.lock = threading.Lock()

    def reset_election_timeout(self):
        # Random timeout between 150-300ms prevents split votes
        self.election_timeout = random.uniform(150, 300) / 1000
        self.last_heartbeat = time.time()

    def should_start_election(self) -&gt; bool:
        return time.time() - self.last_heartbeat &gt; self.election_timeout

    def start_election(self):
        &quot;&quot;&quot;Start leader election when timeout expires.&quot;&quot;&quot;
        with self.lock:
            self.state = NodeState.CANDIDATE
            self.current_term += 1
            self.voted_for = self.node_id  # Vote for self
            self.reset_election_timeout()

        votes = 1  # Already voted for self

        last_log_index = len(self.log)
        last_log_term = self.log[-1].term if self.log else 0

        # Request votes from all peers in parallel
        for peer in self.peers:
            vote_granted = self.request_vote(
                peer,
                self.current_term,
                last_log_index,
                last_log_term
            )
            if vote_granted:
                votes += 1

        # Need majority to win
        quorum = (len(self.peers) + 1) // 2 + 1
        if votes &gt;= quorum:
            self.become_leader()

    def become_leader(self):
        &quot;&quot;&quot;Transition to leader state.&quot;&quot;&quot;
        with self.lock:
            self.state = NodeState.LEADER

            # Initialize leader state for each follower
            next_idx = len(self.log) + 1
            for peer in self.peers:
                self.next_index[peer] = next_idx
                self.match_index[peer] = 0

        # Immediately send heartbeats to establish authority
        self.send_heartbeats()

    def receive_vote_request(self, candidate_id: str, term: int,
                             last_log_index: int, last_log_term: int) -&gt; bool:
        &quot;&quot;&quot;Handle RequestVote RPC from candidate.&quot;&quot;&quot;
        with self.lock:
            # Reject if candidate's term is old
            if term &lt; self.current_term:
                return False

            # Step down if we see a newer term
            if term &gt; self.current_term:
                self.current_term = term
                self.state = NodeState.FOLLOWER
                self.voted_for = None

            # Check if candidate's log is at least as up-to-date
            my_last_term = self.log[-1].term if self.log else 0
            my_last_index = len(self.log)

            log_ok = (last_log_term &gt; my_last_term or
                     (last_log_term == my_last_term and
                      last_log_index &gt;= my_last_index))

            # Grant vote if we haven't voted yet and log is ok
            if (self.voted_for is None or
                self.voted_for == candidate_id) and log_ok:
                self.voted_for = candidate_id
                self.reset_election_timeout()
                return True

            return False

    def append_entries(self, term: int, leader_id: str,
                       prev_log_index: int, prev_log_term: int,
                       entries: List[LogEntry], leader_commit: int) -&gt; bool:
        &quot;&quot;&quot;Handle AppendEntries RPC (heartbeat or log replication).&quot;&quot;&quot;
        with self.lock:
            if term &lt; self.current_term:
                return False

            self.reset_election_timeout()  # Leader is alive

            if term &gt; self.current_term:
                self.current_term = term
                self.state = NodeState.FOLLOWER

            # Log consistency check
            if prev_log_index &gt; 0:
                if len(self.log) &lt; prev_log_index:
                    return False  # Log too short
                if self.log[prev_log_index - 1].term != prev_log_term:
                    return False  # Term mismatch

            # Append new entries (overwriting conflicts)
            for i, entry in enumerate(entries):
                idx = prev_log_index + i
                if idx &lt; len(self.log):
                    if self.log[idx].term != entry.term:
                        self.log = self.log[:idx]  # Remove conflicting entries
                        self.log.append(entry)
                else:
                    self.log.append(entry)

            # Update commit index
            if leader_commit &gt; self.commit_index:
                self.commit_index = min(leader_commit, len(self.log))

            return True

    def request_vote(self, peer: str, term: int,
                     last_log_index: int, last_log_term: int) -&gt; bool:
        &quot;&quot;&quot;Send RequestVote RPC to peer (stub for actual network call).&quot;&quot;&quot;
        # In real implementation, this would be an RPC call
        return True

    def send_heartbeats(self):
        &quot;&quot;&quot;Send empty AppendEntries to all followers.&quot;&quot;&quot;
        # In real implementation, this would send RPCs
        pass
</code></pre>
<h3 id="go---raft-leader-election">Go - Raft Leader Election</h3>
<pre><code class="language-go">package raft

import (
    &quot;math/rand&quot;
    &quot;sync&quot;
    &quot;time&quot;
)

type NodeState int

const (
    Follower NodeState = iota
    Candidate
    Leader
)

type LogEntry struct {
    Term    int
    Command interface{}
}

type RaftNode struct {
    mu sync.Mutex

    id    int
    peers []int

    // Persistent state
    currentTerm int
    votedFor    int
    log         []LogEntry

    // Volatile state
    state       NodeState
    commitIndex int
    lastApplied int

    // Leader state
    nextIndex  map[int]int
    matchIndex map[int]int

    // Channels for communication
    heartbeatCh chan struct{}
    voteCh      chan bool
}

func NewRaftNode(id int, peers []int) *RaftNode {
    node := &amp;RaftNode{
        id:          id,
        peers:       peers,
        currentTerm: 0,
        votedFor:    -1,
        log:         make([]LogEntry, 0),
        state:       Follower,
        commitIndex: 0,
        lastApplied: 0,
        nextIndex:   make(map[int]int),
        matchIndex:  make(map[int]int),
        heartbeatCh: make(chan struct{}, 100),
        voteCh:      make(chan bool, 100),
    }

    go node.run()
    return node
}

func (n *RaftNode) run() {
    for {
        switch n.getState() {
        case Follower:
            n.runFollower()
        case Candidate:
            n.runCandidate()
        case Leader:
            n.runLeader()
        }
    }
}

func (n *RaftNode) runFollower() {
    timeout := time.Duration(150+rand.Intn(150)) * time.Millisecond
    timer := time.NewTimer(timeout)
    defer timer.Stop()

    select {
    case &lt;-n.heartbeatCh:
        // Reset timer on heartbeat
        return
    case &lt;-timer.C:
        // Election timeout - become candidate
        n.mu.Lock()
        n.state = Candidate
        n.mu.Unlock()
    }
}

func (n *RaftNode) runCandidate() {
    n.mu.Lock()
    n.currentTerm++
    n.votedFor = n.id
    term := n.currentTerm
    n.mu.Unlock()

    votes := 1 // Vote for self

    // Request votes from peers
    var wg sync.WaitGroup
    var voteMu sync.Mutex

    for _, peer := range n.peers {
        wg.Add(1)
        go func(peerId int) {
            defer wg.Done()
            if n.requestVote(peerId, term) {
                voteMu.Lock()
                votes++
                voteMu.Unlock()
            }
        }(peer)
    }

    wg.Wait()

    quorum := (len(n.peers)+1)/2 + 1
    n.mu.Lock()
    defer n.mu.Unlock()

    if n.state != Candidate || n.currentTerm != term {
        return // State changed during election
    }

    if votes &gt;= quorum {
        n.state = Leader
        // Initialize leader state
        for _, peer := range n.peers {
            n.nextIndex[peer] = len(n.log) + 1
            n.matchIndex[peer] = 0
        }
    }
}

func (n *RaftNode) runLeader() {
    // Send heartbeats every 50ms
    ticker := time.NewTicker(50 * time.Millisecond)
    defer ticker.Stop()

    for range ticker.C {
        if n.getState() != Leader {
            return
        }
        n.sendHeartbeats()
    }
}

func (n *RaftNode) getState() NodeState {
    n.mu.Lock()
    defer n.mu.Unlock()
    return n.state
}

func (n *RaftNode) requestVote(peer int, term int) bool {
    // In real implementation, this would be an RPC call
    return true
}

func (n *RaftNode) sendHeartbeats() {
    // In real implementation, send AppendEntries RPCs to all peers
}
</code></pre>
<hr />
<h2 id="common-pitfalls">Common Pitfalls</h2>
<div>
<h3 id="1-split-brain-during-network-partitions">1. Split Brain During Network Partitions</h3>
<p><strong>Problem</strong>: Two leaders elected in different partitions.<br />
<strong>Solution</strong>: Require majority <span>quorum</span> for all operations. Minority partition becomes read-only.</p>
<h3 id="2-forgetting-to-persist-state">2. Forgetting to Persist State</h3>
<p><strong>Problem</strong>: Node restarts and loses voted_for, causing double-voting.<br />
<strong>Solution</strong>: Always fsync currentTerm and votedFor before responding to RPCs.</p>
<h3 id="3-election-timeout-too-short">3. Election Timeout Too Short</h3>
<p><strong>Problem</strong>: Constant elections during high latency, no progress.<br />
<strong>Solution</strong>: Set timeout to at least 10x your p99 network latency.</p>
<h3 id="4-not-handling-stale-leaders">4. Not Handling Stale Leaders</h3>
<p><strong>Problem</strong>: Old leader continues accepting writes after partition heals.<br />
<strong>Solution</strong>: Check term in every operation; step down if stale.</p>
<h3 id="5-log-divergence">5. Log Divergence</h3>
<p><strong>Problem</strong>: Followers have conflicting log entries after leader failure.<br />
<strong>Solution</strong>: Always overwrite conflicting entries (Raft's Log Matching property).</p>
<h3 id="6-committing-entries-from-previous-terms">6. Committing Entries from Previous Terms</h3>
<p><strong>Problem</strong>: Leader commits old-term entries that might be overwritten.<br />
<strong>Solution</strong>: Only commit entries from current term; previous entries commit indirectly.</p>
<h3 id="7-ignoring-pre-vote-optimization">7. Ignoring Pre-Vote Optimization</h3>
<p><strong>Problem</strong>: Isolated nodes rejoin and disrupt cluster with stale elections.<br />
<strong>Solution</strong>: Implement pre-vote: candidates check if they could win before incrementing term.</p>
</div>
<hr />
<h2 id="interview-questions---3-level-deep-dive">Interview Questions - 3-Level Deep Dive</h2>
<div>
<h3 id="q1-why-is-raft-preferred-over-paxos-in-practice">Q1: Why is Raft preferred over Paxos in practice?</h3>
<p><strong>Answer</strong>: Raft is preferred because it was explicitly designed for understandability. It decomposes consensus into three independent sub-problems (<span>leader election</span>, <span>log replication</span>, <span>safety</span>), uses a strong leader model, and has clear state transitions. Paxos, while theoretically elegant, is notoriously difficult to implement correctly.</p>
<div>
<pre><code>**Follow-up 1.1: What are the practical differences in implementation complexity?**

Paxos requires handling concurrent proposers with proposal number conflicts, implementing the &quot;adopt highest-numbered accepted value&quot; rule correctly, and managing the conceptual separation of proposers, acceptors, and learners. Raft simplifies this by having a single leader that handles all client requests, eliminating proposal conflicts. The leader election in Raft uses simple term numbers and majority voting, while Paxos leader election (Multi-Paxos) requires running a full Paxos round.
</code></pre>
<div>
<p><strong>Follow-up 1.1.1: Can Paxos outperform Raft in any scenario?</strong></p>
<p>Yes, in scenarios with multiple concurrent proposers or geo-distributed systems. Paxos doesn't require a stable leader, so it can make progress during leader churn. Multi-Paxos can also pipeline proposals more aggressively. Google Spanner uses Paxos partially because their geo-distributed setup benefits from its flexibility in leader placement and the ability to handle multiple data centers as first-class participants.</p>
</div>
</div>
<div>
<pre><code>**Follow-up 1.2: How does Raft's strong leader model affect availability?**

The strong leader is a single point of failure for write availability. When the leader fails, the cluster cannot accept writes until a new leader is elected (typically 150-300ms). This is acceptable for most applications but problematic for systems requiring continuous write availability. Multi-leader approaches sacrifice some consistency guarantees for better write availability.
</code></pre>
<div>
<p><strong>Follow-up 1.2.1: How do production systems mitigate leader failure impact?</strong></p>
<p>Production systems use several techniques: (1) <span>Pre-vote optimization</span> to prevent disruptive elections, (2) <span>Leader stickiness</span> with lease-based leadership to reduce unnecessary elections, (3) <span>Client-side retries</span> with exponential backoff during elections, (4) <span>Multi-Raft</span> where data is sharded across multiple Raft groups (used by CockroachDB, TiKV) so leader failures affect only one shard.</p>
</div>
</div>
</div>
<div>
<h3 id="q2-how-does-raft-handle-network-partitions">Q2: How does Raft handle network partitions?</h3>
<p><strong>Answer</strong>: During a partition, the minority side cannot elect a leader (lacks quorum) and becomes read-only. The majority side continues operating with a new leader if needed. When the partition heals, nodes in the minority catch up from the leader's log, and any stale leader steps down upon seeing a higher term.</p>
<div>
<pre><code>**Follow-up 2.1: What happens to in-flight client requests during a partition?**

Requests to the old leader in the minority partition will timeout (leader can't get quorum for commits). Requests to the new leader succeed normally. After partition heals, clients connected to the old leader discover it's no longer leader (via error response or redirection) and must retry with the new leader. This is why clients need proper retry logic and leader discovery mechanisms.
</code></pre>
<div>
<p><strong>Follow-up 2.1.1: How do you prevent data loss for uncommitted entries?</strong></p>
<p>Uncommitted entries on the old leader may be lost. This is by design - Raft only guarantees durability for <span>committed</span> entries (replicated to majority). Clients should not consider a write successful until receiving confirmation. For critical operations, use application-level acknowledgment (e.g., read-your-writes by reading back). Some systems implement &quot;sticky sessions&quot; to ensure clients always talk to the same replica until explicitly redirected.</p>
</div>
</div>
<div>
<pre><code>**Follow-up 2.2: Can you have linearizable reads during a partition?**

The minority partition cannot provide linearizable reads because it might be stale. The majority partition can provide linearizable reads through the leader (with a ReadIndex optimization or lease-based reads). Followers in the majority can serve linearizable reads if they confirm the leader's lease is still valid or wait for a heartbeat round.
</code></pre>
<div>
<p><strong>Follow-up 2.2.1: Explain the ReadIndex optimization in detail.</strong></p>
<p>ReadIndex allows linearizable reads without writing to the log: (1) Leader records current commitIndex as readIndex, (2) Leader sends heartbeat to confirm it's still leader, (3) Leader waits until appliedIndex &gt;= readIndex, (4) Leader executes read. This avoids log writes for reads while maintaining linearizability. For follower reads, the follower asks the leader for the current commitIndex, then waits locally until it has applied that index. See <a href="/topic/system-design/database-replication">[database replication]</a> for more on read consistency models.</p>
</div>
</div>
</div>
<div>
<h3 id="q3-whats-the-difference-between-crash-and-byzantine-failures">Q3: What's the difference between crash and Byzantine failures?</h3>
<p><strong>Answer</strong>: <span>Crash failures</span> assume nodes fail by stopping (fail-stop model) - they either work correctly or don't respond. <span>Byzantine failures</span> assume nodes can behave arbitrarily - lying, sending conflicting messages, or colluding. Crash-fault tolerant systems need 2f+1 nodes for f failures; Byzantine-fault tolerant systems need 3f+1.</p>
<div>
<pre><code>**Follow-up 3.1: Why does BFT require 3f+1 nodes instead of 2f+1?**

With Byzantine faults, you need 2f+1 honest nodes to outvote f Byzantine nodes, but you also need f extra nodes because Byzantine nodes might not respond (mimicking crash). So you need n - f &gt;= 2f+1, which gives n &gt;= 3f+1. Additionally, in BFT, you can't trust any single response - you need a quorum of 2f+1 matching responses to be sure at least f+1 are from honest nodes.
</code></pre>
<div>
<p><strong>Follow-up 3.1.1: How does PBFT achieve consensus with malicious nodes?</strong></p>
<p>PBFT uses three phases with all-to-all communication: (1) <span>Pre-prepare</span>: leader broadcasts request, (2) <span>Prepare</span>: each node broadcasts to all others, waits for 2f+1 matching prepares, (3) <span>Commit</span>: each node broadcasts commit, waits for 2f+1 matching commits. The redundant communication (O(n^2) messages) ensures that even if Byzantine nodes send conflicting messages, honest nodes see consistent quorums. Cryptographic signatures prevent message forgery.</p>
</div>
</div>
<div>
<pre><code>**Follow-up 3.2: When would you use BFT in a non-blockchain context?**

BFT is useful when you can't trust all participants: (1) Multi-organization systems where participants might cheat, (2) Systems with untrusted hardware (protecting against compromised servers), (3) Highly regulated environments requiring tamper-evident audit logs, (4) Supply chain tracking across competing companies. The overhead is significant, so it's only justified when the trust assumption is genuinely required.
</code></pre>
<div>
<p><strong>Follow-up 3.2.1: What are modern BFT optimizations for better performance?</strong></p>
<p>Modern BFT systems use several optimizations: (1) <span>Speculative execution</span>: execute before full consensus, rollback if needed (Zyzzyva), (2) <span>Threshold signatures</span>: aggregate signatures to reduce message size, (3) <span>Trusted execution environments (TEE)</span>: use hardware enclaves to reduce Byzantine assumptions (CCF), (4) <span>HotStuff</span>: linear message complexity through pipelining and leader rotation, used by Facebook's Diem/Libra.</p>
</div>
</div>
</div>
<div>
<h3 id="q4-how-many-failures-can-a-5-node-raft-cluster-tolerate">Q4: How many failures can a 5-node Raft cluster tolerate?</h3>
<p><strong>Answer</strong>: A 5-node cluster can tolerate 2 failures. With 5 nodes, the quorum (majority) is 3. As long as 3 nodes are alive, the cluster can elect a leader and commit entries. The formula is: with n nodes, tolerates (n-1)/2 failures.</p>
<div>
<pre><code>**Follow-up 4.1: Why use 5 nodes instead of 4 or 6?**

5 nodes is often optimal because: (1) 4 and 5 nodes both tolerate 2 failures (quorum is 3 for both), so 5th node costs money without improving fault tolerance, (2) 6 nodes tolerates 2 failures (quorum is 4), same as 5, (3) Odd numbers prevent ties in voting. Going from 5 to 7 nodes increases fault tolerance to 3 failures. Choose based on your failure domain analysis and cost constraints.
</code></pre>
<div>
<p><strong>Follow-up 4.1.1: How do you handle multi-datacenter deployments with Raft?</strong></p>
<p>Multi-DC deployments face a tradeoff: (1) <span>Quorum within single DC</span>: fast commits but vulnerable to DC failure, (2) <span>Quorum across DCs</span>: DC-failure tolerant but high latency (cross-DC round trips). Solutions include: (a) Witness nodes in third DC (just vote, don't store data), (b) Flexible Paxos with asymmetric quorums, (c) Multi-Raft with strategic shard placement. CockroachDB lets you configure &quot;localities&quot; to prefer same-region replicas. See <a href="/topic/system-design/availability">[availability]</a> for more on multi-DC patterns.</p>
</div>
</div>
<div>
<pre><code>**Follow-up 4.2: What happens when exactly half the nodes fail (2 of 4)?**

With 2 of 4 nodes remaining, you cannot reach quorum (need 3). The cluster becomes unavailable for writes and linearizable reads. This is why even-numbered clusters are discouraged - you pay for an extra node but get no additional fault tolerance. With 4 nodes, you might as well use 3 and save costs, or use 5 and gain one more failure tolerance.
</code></pre>
<div>
<p><strong>Follow-up 4.2.1: Can you recover from loss of quorum without data loss?</strong></p>
<p>If logs are intact on disk, yes - you can manually force a new configuration with the surviving nodes. This is an operational procedure, not automatic. etcd provides <code>--force-new-cluster</code> for this. CockroachDB has similar recovery procedures. The key insight is that Raft's safety guarantees are for the algorithm's operation - manual intervention with operational tools can bypass normal quorum requirements when necessary, but you must ensure no concurrent operations.</p>
</div>
</div>
</div>
<div>
<h3 id="q5-explain-the-difference-between-committed-and-applied-in-raft">Q5: Explain the difference between committed and applied in Raft.</h3>
<p><strong>Answer</strong>: <span>Committed</span> means an entry is replicated on a majority of nodes and is guaranteed durable - it will never be lost or overwritten. <span>Applied</span> means the entry has been executed by the state machine, producing side effects (e.g., updating a database). Committed entries must be applied in order, but there's often a lag between commit and apply.</p>
<div>
<pre><code>**Follow-up 5.1: Why is this distinction important for client responses?**

A client should receive a success response only after the entry is committed (durable), not just when received by the leader. However, the client doesn't need to wait for apply - the commit guarantees the operation will eventually be applied. Some systems return immediately after commit; others wait for apply if the response depends on execution (e.g., a read-after-write). Understanding this helps design proper [[API design]](/topic/system-design/api-design) semantics.
</code></pre>
<div>
<p><strong>Follow-up 5.1.1: How do you handle slow state machine apply?</strong></p>
<p>If the state machine apply is slow (e.g., complex database operations), committed entries can queue up. Solutions: (1) <span>Async apply</span>: respond to client after commit, apply asynchronously (but track what's applied for reads), (2) <span>Batch apply</span>: group multiple entries into single state machine operation, (3) <span>Separate commit and apply threads</span>: parallelize the two operations, (4) <span>Snapshot-based recovery</span>: if too far behind, restore from snapshot instead of replaying log.</p>
</div>
</div>
<div>
<pre><code>**Follow-up 5.2: What's lastApplied used for in Raft?**

lastApplied tracks the highest log index applied to the state machine. It's used for: (1) Ensuring entries are applied in order (only apply when commitIndex &gt; lastApplied), (2) Read consistency checks (can serve read when lastApplied &gt;= readIndex), (3) Snapshot creation (snapshot represents state at lastApplied), (4) Recovery after restart (don't re-apply already-applied entries).
</code></pre>
<div>
<p><strong>Follow-up 5.2.1: How do snapshots interact with log compaction?</strong></p>
<p>Snapshots capture state machine state at a point in time (lastApplied). Once snapshotted, log entries before that index can be discarded. When a follower is very behind, the leader sends the snapshot instead of replaying the entire log. Key considerations: (1) Snapshot must be consistent (atomic point-in-time capture), (2) Include Raft metadata (term, index) in snapshot, (3) Handle partial snapshot transfers gracefully, (4) Don't discard entries that uncommitted followers still need.</p>
</div>
</div>
</div>
<div>
<h3 id="q6-how-does-raft-prevent-split-brain-with-an-even-number-of-nodes">Q6: How does Raft prevent split-brain with an even number of nodes?</h3>
<p><strong>Answer</strong>: Majority requires (n/2)+1 nodes. With 4 nodes, you need 3 to agree. Even in a perfect 2-2 split, neither side has 3 nodes, so neither can elect a leader. The cluster becomes unavailable rather than risking inconsistency. This is the <a href="/topic/system-design/cap-theorem">[CAP theorem]</a> in action - Raft chooses consistency over availability during partitions.</p>
<div>
<pre><code>**Follow-up 6.1: What are witness nodes and when should you use them?**
</code></pre>
<p><span>Witness nodes</span> (or arbiters) participate in voting but don't store data. They're useful for: (1) Breaking ties in even-numbered clusters across 2 DCs (put witness in 3rd location), (2) Reducing storage costs (don't replicate data to witness), (3) Cross-DC quorum with lower latency (witness is lightweight). CockroachDB, MongoDB, and etcd all support witness-like configurations.</p>
<div>
<p><strong>Follow-up 6.1.1: How do witnesses affect read scalability and recovery?</strong></p>
<p>Witnesses can vote for leader election but cannot serve reads (no data). They can't become leader in most implementations. For recovery after majority loss, witnesses don't help - you need actual data nodes. In split-brain prevention, witnesses are valuable, but for data durability, you still need sufficient data-bearing replicas. Design your witness placement based on failure domain analysis, not just node count.</p>
</div>
</div>
<div>
<pre><code>**Follow-up 6.2: Can flexible quorums help with the even-node problem?**
</code></pre>
<p><span>Flexible Paxos</span> allows asymmetric quorums: write quorum + read quorum &gt; n (instead of both being majority). With 4 nodes, you could use write quorum of 3 and read quorum of 2. This still prevents split-brain (3+2 &gt; 4 ensures overlap) but allows more flexibility. However, Raft's standard implementation uses symmetric majority quorums.</p>
<div>
<p><strong>Follow-up 6.2.1: What's the practical use case for flexible quorums?</strong></p>
<p>Flexible quorums shine in geo-distributed systems. Example: 5 nodes across 3 DCs (2+2+1). With standard quorum (3), every write requires cross-DC round trip. With flexible quorum: write quorum of 3 (can be satisfied within single DC if you have 3 there), read quorum of 3 (ensures seeing latest write). You optimize for the common case (writes within DC) while maintaining safety. This is how Google Spanner achieves low latency despite global distribution.</p>
</div>
</div>
</div>
<div>
<h3 id="q7-what-happens-if-a-leader-commits-an-entry-but-crashes-before-notifying-followers">Q7: What happens if a leader commits an entry but crashes before notifying followers?</h3>
<p><strong>Answer</strong>: The entry is safe because it was replicated to a majority before commit. The new leader will necessarily have that entry (leader election requires the most up-to-date log). The new leader includes it in subsequent AppendEntries, and followers will commit it when they see the leader's higher commitIndex.</p>
<div>
<pre><code>**Follow-up 7.1: How does the &quot;up-to-date log&quot; requirement work in leader election?**

In Raft, voters reject candidates with logs less up-to-date than their own. &quot;Up-to-date&quot; means: (1) Higher last log term wins, (2) If terms equal, longer log wins. This ensures the new leader has all committed entries. Since committed entries are on majority, and candidate needs majority votes, at least one voter has the committed entry and will reject candidates without it.
</code></pre>
<div>
<p><strong>Follow-up 7.1.1: What about uncommitted entries from the old leader's current term?</strong></p>
<p>Uncommitted entries from the old leader's term might be lost if they weren't on a majority. The new leader might have different entries at those indices. This is safe because the client never received confirmation. However, there's a subtle issue: a new leader cannot immediately commit entries from previous terms by counting replicas. It must first commit an entry from its own term, which then indirectly commits all prior entries. This prevents the &quot;figure 8&quot; scenario in the Raft paper.</p>
</div>
</div>
<div>
<pre><code>**Follow-up 7.2: How do clients know their request succeeded if the leader crashed?**
</code></pre>
<p>Clients should use <span>idempotent requests</span> with unique IDs. After timeout, client retries with same ID. If the original request was committed, the new leader's state machine returns cached result. If not committed, it's safe to re-execute. Systems like etcd store request IDs and results in the state machine. This also requires client session management for lease-based duplicate detection.</p>
<div>
<p><strong>Follow-up 7.2.1: How long should you keep client request IDs to prevent duplicates?</strong></p>
<p>This depends on your client timeout and retry policy. Typically: (1) Use client sessions with TTL (e.g., 30 seconds), (2) Store request ID -&gt; response mapping in state machine, (3) Clean up entries when session expires or client explicitly ends session. For exactly-once semantics, you need persistent storage of request IDs. Some systems (like etcd) bound this by limiting concurrent client requests and using sequence numbers per session.</p>
</div>
</div>
</div>
</div>
<hr />
<h2 id="best-practices">Best Practices</h2>
<div>
<ol>
<li>
<p><strong>Use odd number of nodes</strong> - Maximizes fault tolerance per dollar spent</p>
</li>
<li>
<p><strong>Monitor leader elections</strong> - Frequent elections indicate network or configuration issues. Track election count, term progression, and leader tenure.</p>
</li>
<li>
<p><strong>Tune timeouts carefully</strong> - Election timeout should be &gt;&gt; heartbeat interval (typically 10x). Consider your p99 network latency.</p>
</li>
<li>
<p><strong>Implement proper persistence</strong> - Log and voted state must survive restarts. Use fsync before responding to RPCs.</p>
</li>
<li>
<p><strong>Test failure scenarios</strong> - Use chaos engineering to validate behavior. Simulate network partitions, slow disks, and clock skew.</p>
</li>
<li>
<p><strong>Consider read scalability</strong> - Linearizable reads require leader involvement. Use <a href="/topic/system-design/caching">[caching]</a> or relaxed consistency for read-heavy workloads.</p>
</li>
<li>
<p><strong>Pre-vote optimization</strong> - Prevents disruption from isolated nodes rejoining with stale elections.</p>
</li>
<li>
<p><strong>Implement log compaction</strong> - Without snapshots, logs grow unbounded. Schedule regular compaction.</p>
</li>
<li>
<p><strong>Use Multi-Raft for large datasets</strong> - Single Raft group limits throughput. Shard data across multiple groups.</p>
</li>
<li>
<p><strong>Plan for operational scenarios</strong> - Membership changes, node replacement, and disaster recovery need documented procedures.</p>
</li>
</ol>
</div>
<hr />
<h2 id="related-topics">Related Topics</h2>
<ul>
<li><a href="/topic/system-design/cap-theorem">[CAP Theorem]</a> - Fundamental trade-offs in distributed systems</li>
<li><a href="/topic/system-design/database-replication">[Database Replication]</a> - Replication strategies and consistency models</li>
<li><a href="/topic/system-design/distributed-locking">[Distributed Locking]</a> - Leader election and coordination</li>
<li><a href="/topic/system-design/availability">[Availability]</a> - High availability patterns</li>
<li><a href="/topic/system-design/message-queues">[Message Queues]</a> - Kafka and ZooKeeper use cases</li>
<li><a href="/topic/system-design/event-sourcing">[Event Sourcing]</a> - Log-based architectures</li>
</ul>
