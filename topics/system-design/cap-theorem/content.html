<h1 id="cap-theorem">CAP Theorem</h1>
<div class="tldr-box">
    <div class="tldr-header">TL;DR</div>
    <ul class="tldr-list">
        <li>CAP Theorem: pick 2 of 3 - Consistency, Availability, or Partition tolerance</li>
        <li>In practice, network partitions happen, so you choose CP (consistent) or AP (available)</li>
        <li>CP systems (MongoDB, HBase): sacrifice availability for consistency during partitions</li>
        <li>AP systems (Cassandra, DynamoDB): sacrifice consistency for availability during partitions</li>
        <li>Modern systems use tunable consistency (eventual consistency spectrum)</li>
    </ul>
</div>
<div class="concept-section type-definition">
<h2 id="overview">Overview</h2>
<p>The <span>CAP Theorem</span> is one of the most fundamental concepts in distributed systems design. Proposed by computer scientist Eric Brewer in 2000 and later proven by Seth Gilbert and Nancy Lynch, it states that a distributed data store can only provide two out of three guarantees simultaneously: <span>Consistency</span>, <span>Availability</span>, and <span>Partition Tolerance</span>.</p>
<div>
<h3>THE CAP THEOREM TRIANGLE</h3>
<div>
<div>
<div>
<div>Consistency (C)</div>
<div>All nodes see same data</div>
<div>at the same time</div>
</div>
<div>
<div>Availability (A)</div>
<div>Every request receives</div>
<div>a response</div>
</div>
<div>
<div>Partition Tolerance (P)</div>
<div>System works despite</div>
<div>network failures</div>
</div>
</div>
<div>
<span>Key Insight: In distributed systems, you can only guarantee TWO!</span>
</div>
</div>
</div>
<p><strong>The Simple Explanation</strong>: Imagine you have data stored on multiple servers across the world. When a network cable gets cut between two data centers (a <span>partition</span>), you face a choice: either refuse some requests to keep data consistent, or accept requests but risk serving stale data. You cannot have both perfect consistency AND perfect availability when networks fail.</p>
</div>
<hr />
<div class="concept-section type-important">
<h2 id="why-it-matters-real-company-examples">Why It Matters: Real Company Examples</h2>
<p>Understanding CAP Theorem is crucial because it drives fundamental architecture decisions at every major tech company:</p>
<div>
<h3>REAL-WORLD CAP DECISIONS</h3>
<div>
<div>
<div>Amazon DynamoDB (AP)</div>
<div>Shopping carts prioritize availability - a customer should always be able to add items, even if it means briefly seeing stale inventory counts.</div>
</div>
<div>
<div>Google Spanner (CP)</div>
<div>Bank transactions require strong consistency - you can never show incorrect balance, even if it means briefly rejecting requests.</div>
</div>
<div>
<div>Netflix (AP)</div>
<div>Video recommendations can be stale for a few seconds - availability matters more than showing the absolutely latest suggestions.</div>
</div>
<div>
<div>Stripe (CP)</div>
<div>Payment processing demands consistency - double-charging a customer is unacceptable, so reject requests if uncertain.</div>
</div>
</div>
</div>
<p><strong>Interview Insight</strong>: Companies often ask &quot;how would you design X?&quot; The CAP theorem is usually the first design decision you need to articulate. Should your system prioritize never losing data (CP) or always being responsive (AP)?</p>
</div>
<hr />
<div class="concept-section type-definition">
<h2 id="how-it-works-the-three-guarantees">How It Works: The Three Guarantees</h2>
<h3 id="consistency-c">Consistency (C)</h3>
<p><span>Linearizable consistency</span> means all nodes see the same data at the same time. After a successful write, every subsequent read from any node returns that value. This is also called <span>strong consistency</span> or <span>atomic consistency</span>.</p>
<div>
<h3>STRONG CONSISTENCY IN ACTION</h3>
<div>
<div>
<div>
<div>Client</div>
<div>writes balance = $100</div>
</div>
<div>--></div>
<div>
<div>Primary</div>
<div>$100</div>
</div>
</div>
<div>
<div>
<div>sync</div>
<div>|</div>
<div>v</div>
</div>
<div>
<div>Node A</div>
<div>$100</div>
</div>
<div>
<div>Node B</div>
<div>$100</div>
</div>
<div>
<div>Node C</div>
<div>$100</div>
</div>
</div>
<div>
<span>Result: Any node you read from shows $100 immediately</span>
</div>
</div>
</div>
<h3 id="availability-a">Availability (A)</h3>
<p>Every request to a non-failing node receives a response (success or failure), without guarantee that it contains the most recent write. The system is <span>always responsive</span>, even during network issues.</p>
<div>
<h3>HIGH AVAILABILITY PATTERN</h3>
<div>
<div>
<div>Client 1</div>
<div>--></div>
<div>Node A</div>
<div>--></div>
<div>Response OK</div>
</div>
<div>
<div>Client 2</div>
<div>--></div>
<div>Node B</div>
<div>--></div>
<div>Response OK</div>
</div>
<div>
<div>Client 3</div>
<div>--></div>
<div>Node C</div>
<div>--></div>
<div>Response OK</div>
</div>
</div>
<div>
<span>Result: Every request gets a response (may be stale data)</span>
</div>
</div>
<h3 id="partition-tolerance-p">Partition Tolerance (P)</h3>
<p>The system continues to operate despite arbitrary message loss or failure of part of the network. <span>Network partitions are inevitable</span> in distributed systems - cables get cut, routers fail, packets get lost.</p>
<div>
<h3>NETWORK PARTITION SCENARIO</h3>
<div>
<div>
<div>Data Center A</div>
<div>Nodes 1, 2, 3</div>
<div>
<div>3 healthy nodes</div>
</div>
</div>
<div>
<div>NETWORK SPLIT</div>
<div>Cannot communicate</div>
<div>X</div>
</div>
<div>
<div>Data Center B</div>
<div>Nodes 4, 5, 6</div>
<div>
<div>3 healthy nodes</div>
</div>
</div>
</div>
<div>
<span>Question: Should both sides keep serving requests?</span>
</div>
</div>
<hr />
<h2 id="the-trade-off-why-you-must-choose">The Trade-off: Why You Must Choose</h2>
<p>In a distributed system, network partitions WILL happen. When they do, you face an impossible choice:</p>
<div>
<h3>THE PARTITION DILEMMA</h3>
<div>
<div>
<div>OPTION 1: Choose Consistency (CP)</div>
<div>
  "I cannot confirm the other data center received this update..."<br><br>
<span>"REJECT the request to avoid inconsistency!"</span>
</div>
<div>
<span>+ Data always consistent across nodes</span>
<span>+ No stale reads or conflicts</span>
<span>- Some requests will fail during partition</span>
<span>- Higher latency for coordination</span>
</div>
</div>
<div>
<div>OPTION 2: Choose Availability (AP)</div>
<div>
  "I'll accept the write locally and sync later..."<br><br>
<span>"ACCEPT the request, deal with conflicts later!"</span>
</div>
<div>
<span>+ All requests succeed (always responsive)</span>
<span>+ Lower latency, no coordination wait</span>
<span>- May return stale data</span>
<span>- Requires conflict resolution strategy</span>
</div>
</div>
</div>
</div>
<hr />
<h2 id="cp-vs-ap-systems-deep-comparison">CP vs AP Systems: Deep Comparison</h2>
<h3 id="cp-systems-consistency--partition-tolerance">CP Systems (Consistency + Partition Tolerance)</h3>
<p><span>CP systems</span> sacrifice availability during partitions. They use techniques like <a href="/topic/system-design/consensus-algorithms">[quorum consensus]</a> to ensure data consistency, meaning some requests will be rejected when nodes cannot communicate.</p>
<div>
<h3>CP SYSTEM: QUORUM CONSENSUS</h3>
<div>5 nodes, quorum = 3 (majority required for any operation)</div>
<div>
<div>
<div>Normal Operation</div>
<div>
  Write to Node A: ACK<br>
  Write to Node B: ACK<br>
  Write to Node C: ACK<br>
<span>3/5 = Quorum reached = SUCCESS</span>
</div>
</div>
<div>
<div>During Partition</div>
<div>
  Write to Node A: ACK<br>
  Write to Node B: UNREACHABLE<br>
  Write to Node C: UNREACHABLE<br>
<span>1/5 &lt; Quorum = REQUEST REJECTED</span>
</div>
</div>
</div>
</div>
<p><strong>CP System Examples:</strong></p>
<table>
<thead>
<tr>
<th>Database</th>
<th>Consistency Mechanism</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MongoDB</strong> (default)</td>
<td>Majority write concern</td>
<td>Document storage, CMS</td>
</tr>
<tr>
<td><strong>HBase</strong></td>
<td>Strong consistency via ZooKeeper</td>
<td>Analytics, time-series</td>
</tr>
<tr>
<td><strong>Redis Cluster</strong></td>
<td>WAIT command for sync replication</td>
<td>Caching with consistency</td>
</tr>
<tr>
<td><strong>Zookeeper</strong></td>
<td>Zab consensus protocol</td>
<td>Coordination, leader election</td>
</tr>
<tr>
<td><strong>etcd</strong></td>
<td>Raft consensus</td>
<td>Kubernetes config, service discovery</td>
</tr>
<tr>
<td><strong>CockroachDB</strong></td>
<td>Serializable transactions</td>
<td>Financial applications</td>
</tr>
<tr>
<td><strong>Google Spanner</strong></td>
<td>TrueTime + Paxos</td>
<td>Global transactions</td>
</tr>
</tbody>
</table>
<h3 id="ap-systems-availability--partition-tolerance">AP Systems (Availability + Partition Tolerance)</h3>
<p><span>AP systems</span> sacrifice consistency during partitions. They accept writes on any available node and reconcile later using <a href="/topic/system-design/event-sourcing">[eventual consistency]</a> strategies.</p>
<div>
<h3>AP SYSTEM: EVENTUAL CONSISTENCY</h3>
<div>
<div>
<div>During Partition (Divergent State):</div>
<div>
<div>
<div>
<div>Node A</div>
<div>X = 10</div>
</div>
<div>Write accepted!</div>
</div>
<div>
<span>PARTITION</span>
</div>
<div>
<div>
<div>Node B</div>
<div>X = 5 (stale)</div>
</div>
<div>Returns old value</div>
</div>
</div>
</div>
<div>
<div>After Partition Heals (Convergent State):</div>
<div>
<div>
<div>Node A</div>
<div>X = 10</div>
</div>
<div>
<div>anti-entropy</div>
<div>sync</div>
</div>
<div>
<div>Node B</div>
<div>X = 10</div>
</div>
</div>
</div>
</div>
</div>
<p><strong>AP System Examples:</strong></p>
<table>
<thead>
<tr>
<th>Database</th>
<th>Consistency Model</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Cassandra</strong></td>
<td>Tunable consistency, eventual by default</td>
<td>Time-series, messaging</td>
</tr>
<tr>
<td><strong>DynamoDB</strong></td>
<td>Eventual consistency default</td>
<td>Shopping carts, sessions</td>
</tr>
<tr>
<td><strong>CouchDB</strong></td>
<td>MVCC, eventual consistency</td>
<td>Offline-first apps</td>
</tr>
<tr>
<td><strong>Riak</strong></td>
<td>Vector clocks, siblings</td>
<td>High availability storage</td>
</tr>
<tr>
<td><strong>Amazon S3</strong></td>
<td>Eventual consistency (now strong for PUTs)</td>
<td>Object storage</td>
</tr>
<tr>
<td><strong>Voldemort</strong></td>
<td>Vector clocks</td>
<td>LinkedIn's key-value store</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="pacelc-theorem-beyond-cap">PACELC Theorem: Beyond CAP</h2>
<p>The <span>PACELC theorem</span> extends CAP by addressing what happens when there is NO partition. Proposed by Daniel Abadi in 2010, it states:</p>
<blockquote>
<p><strong>P</strong>artition? <strong>A</strong>vailability vs <strong>C</strong>onsistency, <strong>E</strong>lse <strong>L</strong>atency vs <strong>C</strong>onsistency</p>
</blockquote>
<div>
<h3>PACELC THEOREM</h3>
<div>
<div>
<div>IF PARTITION (P)</div>
<div>
<div>
<div>A</div>
<div>Availability</div>
</div>
<div>vs</div>
<div>
<div>C</div>
<div>Consistency</div>
</div>
</div>
<div>Same as CAP: choose A or C</div>
</div>
<div>
<div>ELSE (Normal Operation)</div>
<div>
<div>
<div>L</div>
<div>Latency</div>
</div>
<div>vs</div>
<div>
<div>C</div>
<div>Consistency</div>
</div>
</div>
<div>New insight: coordination adds latency</div>
</div>
</div>
</div>
<h3 id="pacelc-classifications">PACELC Classifications</h3>
<div>
<table>
<thead>
<tr>
<th>System</th>
<th>During Partition (PAC)</th>
<th>Normal Operation (ELC)</th>
<th>Classification</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>DynamoDB</strong></td>
<td>A (Availability)</td>
<td>L (Latency)</td>
<td>PA/EL</td>
</tr>
<tr>
<td><strong>Cassandra</strong></td>
<td>A (Availability)</td>
<td>L (Latency)</td>
<td>PA/EL</td>
</tr>
<tr>
<td><strong>Riak</strong></td>
<td>A (Availability)</td>
<td>L (Latency)</td>
<td>PA/EL</td>
</tr>
<tr>
<td><strong>MongoDB</strong></td>
<td>C (Consistency)</td>
<td>L (Latency)</td>
<td>PC/EL</td>
</tr>
<tr>
<td><strong>PNUTS</strong></td>
<td>C (Consistency)</td>
<td>L (Latency)</td>
<td>PC/EL</td>
</tr>
<tr>
<td><strong>Spanner</strong></td>
<td>C (Consistency)</td>
<td>C (Consistency)</td>
<td>PC/EC</td>
</tr>
<tr>
<td><strong>VoltDB</strong></td>
<td>C (Consistency)</td>
<td>C (Consistency)</td>
<td>PC/EC</td>
</tr>
<tr>
<td><strong>CockroachDB</strong></td>
<td>C (Consistency)</td>
<td>C (Consistency)</td>
<td>PC/EC</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Key Insight</strong>: Even when the network is healthy, choosing strong consistency means waiting for coordination between nodes, adding latency. Systems like Spanner (PC/EC) prioritize consistency always, accepting higher latency as the cost.</p>
<hr />
<h2 id="partition-handling-strategies">Partition Handling Strategies</h2>
<p>When a network partition occurs, systems use various strategies to handle it:</p>
<div>
<h3>PARTITION HANDLING STRATEGIES</h3>
<div>
<div>
<div>1. Quorum-Based Decisions</div>
<div>
  Majority side continues operating. Minority side rejects writes.
<div>
<span>Used by: Raft, Paxos, MongoDB</span>
</div>
</div>
</div>
<div>
<div>2. Last-Write-Wins (LWW)</div>
<div>
  Accept all writes, resolve conflicts using timestamps. Most recent timestamp wins.
<div>
<span>Used by: Cassandra, DynamoDB</span>
</div>
</div>
</div>
<div>
<div>3. Vector Clocks</div>
<div>
  Track causality to detect concurrent writes. Keep multiple versions (siblings) for manual resolution.
<div>
<span>Used by: Riak, Voldemort</span>
</div>
</div>
</div>
<div>
<div>4. CRDTs</div>
<div>
  Conflict-free Replicated Data Types. Data structures that automatically merge without conflicts.
<div>
<span>Used by: Redis (CRDB), Riak</span>
</div>
</div>
</div>
<div>
<div>5. Sloppy Quorum + Hinted Handoff</div>
<div>
  Write to any available nodes (even non-replica). Hand off to correct nodes when they recover.
<div>
<span>Used by: DynamoDB, Cassandra</span>
</div>
</div>
</div>
<div>
<div>6. Read Repair + Anti-Entropy</div>
<div>
  On read, detect inconsistencies and repair them. Background process syncs all replicas.
<div>
<span>Used by: Cassandra, DynamoDB</span>
</div>
</div>
</div>
</div>
</div>
<h3 id="sloppy-quorum-and-hinted-handoff-explained">Sloppy Quorum and Hinted Handoff Explained</h3>
<div>
<h3>SLOPPY QUORUM + HINTED HANDOFF</h3>
<div>
<div>
<div>Step 1: Partition occurs, preferred nodes unreachable</div>
<div>
<div>
<div>Client</div>
<div>write key=K</div>
</div>
<div>--></div>
<div>
<div>Node A</div>
<div>UNREACHABLE</div>
</div>
<div>
<div>Node B</div>
<div>UNREACHABLE</div>
</div>
</div>
</div>
<div>
<div>Step 2: Write to available nodes with hint</div>
<div>
<div>
<div>Node X</div>
<div>hint: "for Node A"</div>
</div>
<div>
<div>Node Y</div>
<div>hint: "for Node B"</div>
</div>
</div>
</div>
<div>
<div>Step 3: Partition heals, hints handed off</div>
<div>
<div>
<div>Node X</div>
<div>sends to A</div>
</div>
<div>--></div>
<div>
<div>Node A</div>
<div>data restored</div>
</div>
</div>
</div>
</div>
</div>
<hr />
<h2 id="database-cappacelc-classifications">Database CAP/PACELC Classifications</h2>
<div>
<table>
<thead>
<tr>
<th>Database</th>
<th>CAP</th>
<th>PACELC</th>
<th>Tunable</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>PostgreSQL</strong> (single)</td>
<td>CA*</td>
<td>N/A</td>
<td>No</td>
<td>*Not distributed</td>
</tr>
<tr>
<td><strong>MongoDB</strong></td>
<td>CP</td>
<td>PC/EC</td>
<td>Yes</td>
<td>Write concern adjustable</td>
</tr>
<tr>
<td><strong>Cassandra</strong></td>
<td>AP</td>
<td>PA/EL</td>
<td>Yes</td>
<td>Consistency level per query</td>
</tr>
<tr>
<td><strong>DynamoDB</strong></td>
<td>AP</td>
<td>PA/EL</td>
<td>Yes</td>
<td>Eventually consistent default</td>
</tr>
<tr>
<td><strong>Redis Cluster</strong></td>
<td>CP</td>
<td>PC/EL</td>
<td>Limited</td>
<td>WAIT for sync</td>
</tr>
<tr>
<td><strong>CockroachDB</strong></td>
<td>CP</td>
<td>PC/EC</td>
<td>No</td>
<td>Serializable always</td>
</tr>
<tr>
<td><strong>Google Spanner</strong></td>
<td>CP</td>
<td>PC/EC</td>
<td>No</td>
<td>TrueTime enables global consistency</td>
</tr>
<tr>
<td><strong>Riak</strong></td>
<td>AP</td>
<td>PA/EL</td>
<td>Yes</td>
<td>Tunable N/R/W values</td>
</tr>
<tr>
<td><strong>etcd</strong></td>
<td>CP</td>
<td>PC/EC</td>
<td>No</td>
<td>Raft consensus</td>
</tr>
<tr>
<td><strong>Consul</strong></td>
<td>CP</td>
<td>PC/EC</td>
<td>No</td>
<td>Raft consensus</td>
</tr>
<tr>
<td><strong>CouchDB</strong></td>
<td>AP</td>
<td>PA/EL</td>
<td>No</td>
<td>Multi-master replication</td>
</tr>
<tr>
<td><strong>ScyllaDB</strong></td>
<td>AP</td>
<td>PA/EL</td>
<td>Yes</td>
<td>Cassandra-compatible</td>
</tr>
<tr>
<td><strong>TiDB</strong></td>
<td>CP</td>
<td>PC/EL</td>
<td>Yes</td>
<td>Raft + async replication</td>
</tr>
<tr>
<td><strong>YugabyteDB</strong></td>
<td>CP</td>
<td>PC/EC</td>
<td>Yes</td>
<td>Raft per tablet</td>
</tr>
</tbody>
</table>
</div>
<hr />
<h2 id="code-examples">Code Examples</h2>
<h3 id="python---cp-system-with-quorum">Python - CP System with Quorum</h3>
<pre><code>                ```python
                from typing import List, Optional, Any
                from dataclasses import dataclass
                import time
                import threading

                @dataclass
                class VersionedValue:
                value: Any
                version: int
                timestamp: float

                class NetworkError(Exception):
                &quot;&quot;&quot;Raised when a node is unreachable.&quot;&quot;&quot;
                pass

                class UnavailableError(Exception):
                &quot;&quot;&quot;Raised when quorum cannot be reached.&quot;&quot;&quot;
                pass

                class CPDatabase:
                &quot;&quot;&quot;
                Consistency + Partition Tolerance database simulation.

                Sacrifices availability during partitions - rejects requests
                when quorum cannot be reached.
                &quot;&quot;&quot;

                def __init__(self, nodes: List['Node'], quorum_size: int = None):
                self.nodes = nodes
                # Default to majority quorum
                self.quorum = quorum_size or (len(nodes) // 2 + 1)
                self.lock = threading.Lock()

                def write(self, key: str, value: Any) -&gt; bool:
                &quot;&quot;&quot;
                Write requires quorum acknowledgment.

                Blocks until quorum is reached or raises UnavailableError.
                &quot;&quot;&quot;
                version = int(time.time() * 1000)
                acks = 0
                errors = []

                for node in self.nodes:
                try:
                if node.write(key, value, version):
                acks += 1
                except NetworkError as e:
                errors.append(str(e))
                continue  # Node unreachable due to partition

                if acks &lt; self.quorum:
                raise UnavailableError(
                f&quot;Cannot reach quorum: {acks}/{self.quorum} nodes responded. &quot;
                f&quot;Errors: {errors}&quot;
                )
                return True

                def read(self, key: str) -&gt; Any:
                &quot;&quot;&quot;
                Read from quorum and return most recent value.

                Uses read-repair to fix stale replicas.
                &quot;&quot;&quot;
                responses = []

                for node in self.nodes:
                try:
                response = node.read(key)
                if response:
                responses.append((node, response))
                except NetworkError:
                continue

                if len(responses) &lt; self.quorum:
                raise UnavailableError(&quot;Cannot reach read quorum&quot;)

                # Find the most recent value (highest version)
                latest = max(responses, key=lambda r: r[1].version)

                # Read repair: update stale nodes
                for node, response in responses:
                if response.version &lt; latest[1].version:
                try:
                node.write(key, latest[1].value, latest[1].version)
                except NetworkError:
                pass  # Best effort repair

                return latest[1].value
                ```
</code></pre>
<h3 id="python---ap-system-with-vector-clocks">Python - AP System with Vector Clocks</h3>
<pre><code>                ```python
                from dataclasses import dataclass, field
                from typing import Dict, List, Any, Optional
                import time
                import copy

                @dataclass
                class VectorClock:
                &quot;&quot;&quot;
                Track causality for conflict resolution.

                Vector clocks allow detecting concurrent writes that
                need application-level conflict resolution.
                &quot;&quot;&quot;
                clocks: Dict[str, int] = field(default_factory=dict)

                def increment(self, node_id: str):
                &quot;&quot;&quot;Increment this node's clock component.&quot;&quot;&quot;
                self.clocks[node_id] = self.clocks.get(node_id, 0) + 1

                def merge(self, other: 'VectorClock'):
                &quot;&quot;&quot;Merge with another vector clock (take max of each component).&quot;&quot;&quot;
                for node_id, count in other.clocks.items():
                self.clocks[node_id] = max(
                self.clocks.get(node_id, 0), count
                )

                def dominates(self, other: 'VectorClock') -&gt; bool:
                &quot;&quot;&quot;Check if this clock happened-after the other.&quot;&quot;&quot;
                dominated = False
                for node_id in set(self.clocks.keys()) | set(other.clocks.keys()):
                my_count = self.clocks.get(node_id, 0)
                other_count = other.clocks.get(node_id, 0)
                if my_count &lt; other_count:
                return False
                if my_count &gt; other_count:
                dominated = True
                return dominated

                def concurrent_with(self, other: 'VectorClock') -&gt; bool:
                &quot;&quot;&quot;Check if two events are concurrent (neither dominates).&quot;&quot;&quot;
                return not self.dominates(other) and not other.dominates(self)

                class APDatabase:
                &quot;&quot;&quot;
                Availability + Partition Tolerance database simulation.

                Always accepts writes locally. Uses vector clocks to
                detect conflicts during sync.
                &quot;&quot;&quot;

                def __init__(self, node_id: str):
                self.node_id = node_id
                self.data: Dict[str, List[tuple]] = {}  # key -&gt; [(value, vector_clock)]
                self.clock = VectorClock()
                self.sync_queue: List[tuple] = []

                def write(self, key: str, value: Any) -&gt; bool:
                &quot;&quot;&quot;
                Always accept writes locally - never fails!

                Returns True immediately. Conflict resolution happens
                during reads or background sync.
                &quot;&quot;&quot;
                self.clock.increment(self.node_id)
                clock_copy = VectorClock(dict(self.clock.clocks))

                # Store as new version (may create siblings)
                if key not in self.data:
                self.data[key] = []

                # Remove dominated versions, keep concurrent ones
                new_versions = []
                for existing_value, existing_clock in self.data[key]:
                if not clock_copy.dominates(existing_clock):
                new_versions.append((existing_value, existing_clock))

                new_versions.append((value, clock_copy))
                self.data[key] = new_versions

                # Queue for async replication
                self.sync_queue.append((key, value, clock_copy))
                return True  # Always succeeds!

                def read(self, key: str) -&gt; Any:
                &quot;&quot;&quot;
                Always return local value - never fails!

                May return multiple values (siblings) if there are
                unresolved conflicts.
                &quot;&quot;&quot;
                if key not in self.data:
                return None

                versions = self.data[key]
                if len(versions) == 1:
                return versions[0][0]  # Single value, no conflict

                # Multiple concurrent versions - return all for resolution
                return [v[0] for v in versions]

                def resolve_conflicts(self, key: str, resolved_value: Any):
                &quot;&quot;&quot;Application-level conflict resolution.&quot;&quot;&quot;
                self.clock.increment(self.node_id)
                clock_copy = VectorClock(dict(self.clock.clocks))

                # Merge all existing clocks
                for _, existing_clock in self.data.get(key, []):
                clock_copy.merge(existing_clock)

                # Replace all versions with resolved value
                self.data[key] = [(resolved_value, clock_copy)]

                def receive_sync(self, key: str, value: Any, remote_clock: VectorClock):
                &quot;&quot;&quot;
                Receive update from another node after partition heals.

                Merges with local state, potentially creating siblings.
                &quot;&quot;&quot;
                self.clock.merge(remote_clock)

                if key not in self.data:
                self.data[key] = [(value, remote_clock)]
                return

                # Check relationship with existing versions
                new_versions = []
                is_dominated = False

                for existing_value, existing_clock in self.data[key]:
                if remote_clock.dominates(existing_clock):
                # Remote is newer, discard local
                continue
                elif existing_clock.dominates(remote_clock):
                # Local is newer, ignore remote
                is_dominated = True
                new_versions.append((existing_value, existing_clock))
                else:
                # Concurrent - keep both as siblings
                new_versions.append((existing_value, existing_clock))

                if not is_dominated:
                new_versions.append((value, remote_clock))

                self.data[key] = new_versions
                ```
</code></pre>
<h3 id="go---crdt-counter-for-ap-systems">Go - CRDT Counter for AP Systems</h3>
<pre><code>                ```go
                package cap

                import (
                &quot;sync&quot;
                )

                // GCounter implements a grow-only counter CRDT.
                // Multiple nodes can increment concurrently, and
                // all counters automatically converge on merge.
                type GCounter struct {
                nodeID string
                counts map[string]uint64
                mu     sync.RWMutex
                }

                func NewGCounter(nodeID string) *GCounter {
                return &amp;GCounter{
                nodeID: nodeID,
                counts: make(map[string]uint64),
                }
                }

                // Increment adds to this node's counter.
                // Can be called during partition - always succeeds.
                func (gc *GCounter) Increment(delta uint64) {
                gc.mu.Lock()
                defer gc.mu.Unlock()
                gc.counts[gc.nodeID] += delta
                }

                // Value returns the total count across all nodes.
                func (gc *GCounter) Value() uint64 {
                gc.mu.RLock()
                defer gc.mu.RUnlock()

                var total uint64
                for _, count := range gc.counts {
                total += count
                }
                return total
                }

                // Merge combines with another counter.
                // Called when partition heals - automatically resolves conflicts.
                func (gc *GCounter) Merge(other *GCounter) {
                gc.mu.Lock()
                defer gc.mu.Unlock()
                other.mu.RLock()
                defer other.mu.RUnlock()

                for nodeID, count := range other.counts {
                if existing, ok := gc.counts[nodeID]; !ok || count &gt; existing {
                gc.counts[nodeID] = count
                }
                }
                }

                // PNCounter implements a positive-negative counter CRDT.
                // Supports both increment and decrement operations.
                type PNCounter struct {
                nodeID string
                pos    *GCounter
                neg    *GCounter
                }

                func NewPNCounter(nodeID string) *PNCounter {
                return &amp;PNCounter{
                nodeID: nodeID,
                pos:    NewGCounter(nodeID),
                neg:    NewGCounter(nodeID),
                }
                }

                func (pn *PNCounter) Increment(delta uint64) {
                pn.pos.Increment(delta)
                }

                func (pn *PNCounter) Decrement(delta uint64) {
                pn.neg.Increment(delta)
                }

                func (pn *PNCounter) Value() int64 {
                return int64(pn.pos.Value()) - int64(pn.neg.Value())
                }

                func (pn *PNCounter) Merge(other *PNCounter) {
                pn.pos.Merge(other.pos)
                pn.neg.Merge(other.neg)
                }
                ```
</code></pre>
<hr />
<h2 id="common-pitfalls">Common Pitfalls</h2>
<div>
<div>1. Thinking CA is Possible in Distributed Systems</div>
<div>Network partitions WILL happen. Any truly distributed system must be partition-tolerant. "CA systems" like single-node PostgreSQL are not distributed - they have no partitions because there is no network between nodes.</div>
</div>
<div>
<div>2. Confusing CAP Consistency with ACID Consistency</div>
<div>CAP consistency means all nodes see the same data (linearizability). ACID consistency means data satisfies integrity constraints (foreign keys, check constraints). A CP system can still have ACID violations if transactions are not properly implemented.</div>
</div>
<div>
<div>3. Treating CAP as Binary</div>
<div>Real systems exist on a spectrum. Many databases offer tunable consistency - you can choose different consistency levels per operation. DynamoDB, Cassandra, and MongoDB all offer this flexibility. You might use strong consistency for payments but eventual consistency for analytics.</div>
</div>
<div>
<div>4. Ignoring Latency (PACELC)</div>
<div>CAP only describes behavior during partitions. The PACELC theorem extends this: even when no partition exists, you must choose between latency and consistency. Strong consistency requires coordination, which adds latency. A geo-distributed CP system might have 200ms write latency.</div>
</div>
<div>
<div>5. Not Understanding What "Availability" Means in CAP</div>
<div>CAP availability is absolute: every non-failing node must respond. This is different from "five nines" availability (99.999% uptime). A CP system can have excellent uptime but sacrifice CAP availability during rare partitions.</div>
</div>
<hr />
<h2 id="interview-questions-3-level-deep-dive">Interview Questions: 3-Level Deep Dive</h2>
<h3 id="level-1-conceptual-understanding">Level 1: Conceptual Understanding</h3>
<div>
<p><strong>Q1: Why can't we have all three CAP guarantees?</strong></p>
<p>A: During a <span>network partition</span>, nodes cannot communicate. If a write comes in, you must either:<br />
- Accept it on available nodes (sacrificing consistency - other nodes have stale data)<br />
- Reject it until partition heals (sacrificing availability)</p>
<p>You cannot both accept the write AND guarantee all nodes are consistent when they can't communicate.</p>
<div>
<strong>Follow-up Q1.1: But what if partitions never happen?</strong>
<p>A: This is the key insight of <span>PACELC theorem</span>. Even without partitions, you still face a tradeoff: Latency vs Consistency. Achieving strong consistency requires coordination between nodes (waiting for acknowledgments), which adds latency. You can't have both zero-latency AND strong consistency.</p>
<div>
<strong>Follow-up Q1.1.1: How does Google Spanner achieve strong consistency with low latency?</strong>
<p>A: Spanner uses <span>TrueTime</span> - GPS-synchronized atomic clocks in every data center that bound clock uncertainty to ~7ms. This allows Spanner to assign globally-ordered timestamps without cross-datacenter round trips for reads. But writes still require Paxos consensus, so write latency is still bounded by network latency between datacenters (typically 50-100ms for global deployments).</p>
</div>
</div>
</div>
<div>
<p><strong>Q2: Is MongoDB CP or AP?</strong></p>
<p>A: MongoDB is <span>CP by default</span> (requires majority acknowledgment for writes), but can be configured for AP behavior with <code>{w: 1, j: false}</code> settings. This is a great example of how modern databases offer <span>tunable consistency</span>.</p>
<div>
<strong>Follow-up Q2.1: What happens to writes during a MongoDB replica set election?</strong>
<p>A: Writes are rejected during election (typically 10-12 seconds). This is the CP trade-off - availability is sacrificed to maintain consistency. Clients receive a &quot;not primary&quot; error and must retry. The election timeout is configurable but lowering it risks split-brain scenarios.</p>
<div>
<strong>Follow-up Q2.1.1: How can you minimize impact of elections on application availability?</strong>
<p>A: Several strategies:</p>
<ol>
<li><span>Retryable writes</span>: MongoDB 3.6+ automatically retries failed writes on new primary</li>
<li><span>Connection pooling with retry logic</span>: Application retries with exponential backoff</li>
<li><span>Read preference configuration</span>: Route reads to secondaries during election</li>
<li><span>Priority configuration</span>: Ensure preferred primary to reduce unexpected elections</li>
</ol>
</div>
</div>
</div>
<div>
<p><strong>Q3: How do AP systems handle conflicts?</strong></p>
<p>A: Common strategies include:</p>
<ul>
<li><span>Last-Write-Wins (LWW)</span>: Highest timestamp wins (simple but can lose data)</li>
<li><span>Vector Clocks</span>: Track causality, detect conflicts, keep siblings</li>
<li><span>CRDTs</span>: Data structures that automatically merge without conflicts</li>
<li><span>Application-level resolution</span>: Let users/application resolve conflicts</li>
</ul>
<div>
<strong>Follow-up Q3.1: What's wrong with Last-Write-Wins?</strong>
<p>A: LWW can silently <span>lose data</span>. Example: User A adds item X to cart at T1, User B adds item Y at T2 (from different node). LWW would keep only item Y. The &quot;lost update&quot; problem. Clock skew between nodes makes this worse - a &quot;later&quot; write might have an earlier timestamp.</p>
<div>
<strong>Follow-up Q3.1.1: How do CRDTs solve this without coordination?</strong>
<p>A: CRDTs (Conflict-free Replicated Data Types) are mathematically designed to merge without conflicts. For a shopping cart:</p>
<ul>
<li>
<p>Use a <span>G-Set (Grow-only Set)</span>: Items can only be added, never removed. Merge = union.</p>
</li>
<li>
<p>Use a <span>OR-Set (Observed-Remove Set)</span>: Track add/remove operations with unique tags. An item is present if any add tag has no corresponding remove.</p>
<p>Key insight: CRDTs trade expressiveness for automatic convergence. Not all operations can be expressed as CRDTs.</p>
</li>
</ul>
</div>
</div>
</div>
<h3 id="level-2-design-questions">Level 2: Design Questions</h3>
<div>
<p><strong>Q4: &quot;Design a shopping cart system - would you choose CP or AP?&quot;</strong></p>
<p>A: <span>AP is typically better</span> for shopping carts because:<br />
- Availability matters more - customers should always be able to add items<br />
- Temporary inconsistency is acceptable - showing slightly stale cart is fine<br />
- Conflicts are rare and can be merged (union of items added)<br />
- Lost sales from unavailability &gt; cost of occasional duplicate items</p>
<p>Amazon's DynamoDB was literally designed for this use case (Dynamo paper).</p>
<div>
<strong>Follow-up Q4.1: How would you handle the inventory check - can you oversell?</strong>
<p>A: This is where it gets nuanced. For the cart itself: AP. But for <span>inventory reservation at checkout</span>: you need CP. Strategy:</p>
<ol>
<li>Cart operations (add/remove items): AP, eventual consistency</li>
<li>Checkout/payment: CP, require inventory lock</li>
<li>Compensating transaction: If inventory unavailable at checkout, notify user</li>
</ol>
<p>This is the <span>&quot;Saga pattern&quot;</span> - long-running transactions with compensation.</p>
<div>
<strong>Follow-up Q4.1.1: What if two users try to buy the last item simultaneously?</strong>
<p>A: Several approaches:</p>
<ol>
<li><span>Optimistic locking</span>: Both try to decrement inventory. Second one fails, cart shows &quot;item unavailable&quot;.</li>
<li><span>Soft reservation</span>: Reserve inventory for X minutes during checkout. Expire if not completed.</li>
<li><span>Accept oversell, handle operationally</span>: For high-volume items, brief oversell is acceptable. Notify customer, offer alternatives.</li>
</ol>
<p>The key is matching business requirements to technical constraints. A flash sale needs stricter controls than regular inventory.</p>
</div>
</div>
</div>
<div>
<p><strong>Q5: &quot;Design a banking system - CP or AP?&quot;</strong></p>
<p>A: <span>CP is required</span> because:<br />
- Incorrect balances are unacceptable (regulatory, trust)<br />
- Double-spending must be prevented<br />
- Regulatory compliance requires accurate records<br />
- Users accept occasional &quot;try again later&quot; errors</p>
<p>Banks prefer rejecting transactions over showing wrong balances.</p>
<div>
<strong>Follow-up Q5.1: But ATMs work offline - isn't that AP?</strong>
<p>A: Great observation! ATMs use a hybrid approach:</p>
<ol>
<li><span>Offline limits</span>: ATMs have maximum offline withdrawal limits</li>
<li><span>Risk-based decisions</span>: Known customer, small amount = allow offline</li>
<li><span>Reconciliation</span>: Transactions queued and settled when online</li>
<li><span>Fraud detection</span>: Patterns analyzed to detect abuse</li>
</ol>
<p>This is AP with <span>bounded inconsistency</span> - accepting risk for availability.</p>
<div>
<strong>Follow-up Q5.1.1: How do global banks handle cross-datacenter transactions?</strong>
<p>A: Modern approaches:</p>
<ol>
<li><span>Google Spanner</span>: TrueTime + Paxos for globally consistent transactions (50-100ms latency)</li>
<li><span>CockroachDB</span>: Raft consensus per range, serializable isolation</li>
<li><span>Region affinity</span>: Most transactions stay in one region, cross-region only for global operations</li>
<li><span>Async replication for reads</span>: Strong writes to primary region, eventual reads from local region</li>
</ol>
<p>Key insight: Global strong consistency is expensive (latency). Design to minimize cross-region transactions.</p>
</div>
</div>
</div>
<h3 id="level-3-advancededge-cases">Level 3: Advanced/Edge Cases</h3>
<div>
<p><strong>Q6: &quot;What happens if a network partition lasts for days?&quot;</strong></p>
<p>A: This reveals important system design considerations:</p>
<ol>
<li><span>CP systems</span>: Minority partition remains unavailable. May need manual intervention to prevent data divergence.</li>
<li><span>AP systems</span>: Both sides accumulate writes. When partition heals, massive reconciliation needed.</li>
</ol>
<div>
<strong>Follow-up Q6.1: How do you handle the reconciliation when an AP system heals after days?</strong>
<p>A: Several challenges and solutions:</p>
<ol>
<li><span>Merkle trees</span>: Hash-based comparison to efficiently find divergent data (used by Cassandra, Dynamo)</li>
<li><span>Incremental sync</span>: Stream changes since last known sync point</li>
<li><span>Conflict queue</span>: Surface unresolvable conflicts for human review</li>
<li><span>Rate limiting</span>: Throttle sync to prevent overwhelming the system</li>
</ol>
<div>
<strong>Follow-up Q6.1.1: What if there are millions of conflicts?</strong>
<p>A: This is where <span>semantic conflict resolution</span> becomes critical:</p>
<ol>
<li><span>Domain-specific rules</span>: &quot;For user profiles, prefer newer update. For counters, sum both sides.&quot;</li>
<li><span>Automated resolution</span>: 99% of conflicts can be auto-resolved with good heuristics</li>
<li><span>Human escalation</span>: Flag truly ambiguous cases for review</li>
<li><span>Audit log</span>: Keep full history for compliance and debugging</li>
</ol>
<p>Real example: Git merge conflicts are &quot;human escalation&quot; - the system detects conflict, human resolves.</p>
</div>
</div>
</div>
<div>
<p><strong>Q7: &quot;How would you implement tunable consistency in a distributed database?&quot;</strong></p>
<p>A: Key mechanisms:</p>
<ol>
<li><span>Write concern</span>: How many nodes must acknowledge a write (W)</li>
<li><span>Read concern</span>: How many nodes must agree on read (R)</li>
<li><span>Quorum formula</span>: W + R &gt; N ensures overlap, guaranteeing read sees latest write</li>
</ol>
<div>
<strong>Follow-up Q7.1: What are the tradeoffs of different W/R configurations?</strong>
<p>A: For N=3 nodes:</p>
<table>
<thead>
<tr>
<th>W</th>
<th>R</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr>
<td>3</td>
<td>1</td>
<td>Strong write, fast read. Write latency = slowest node.</td>
</tr>
<tr>
<td>1</td>
<td>3</td>
<td>Fast write, strong read. Read latency = slowest node.</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>Balanced. Tolerates 1 failure for both reads and writes.</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>Fastest but no consistency guarantee. May read stale.</td>
</tr>
</tbody>
</table>
<div>
<strong>Follow-up Q7.1.1: How does Cassandra's LOCAL_QUORUM work in multi-DC setups?</strong>
<p>A: LOCAL_QUORUM provides quorum within the <span>local datacenter only</span>:</p>
<ol>
<li>Write to local DC quorum (fast, low latency)</li>
<li>Async replicate to remote DCs (background)</li>
<li>Read from local DC quorum (fast)</li>
</ol>
<p>Tradeoff: If local DC fails, you lose recent writes not yet replicated. EACH_QUORUM requires quorum in every DC (higher latency, better durability).</p>
<p>This is <span>PACELC in action</span>: During partition between DCs, you have local availability. During normal operation, you choose low latency over global consistency.</p>
</div>
</div>
</div>
<div>
<p><strong>Q8: &quot;Explain split-brain in distributed systems and how to prevent it.&quot;</strong></p>
<p>A: <span>Split-brain</span> occurs when a partition causes both sides to believe they are the primary/leader, accepting conflicting writes.</p>
<p>Prevention strategies:</p>
<ol>
<li><span>Quorum-based election</span>: Require majority to elect leader</li>
<li><span>Fencing tokens</span>: Monotonically increasing tokens invalidate stale leaders</li>
<li><span>STONITH</span>: &quot;Shoot The Other Node In The Head&quot; - forcibly power off suspected failed node</li>
</ol>
<div>
<strong>Follow-up Q8.1: How do fencing tokens work in practice?</strong>
<p>A: Fencing tokens prevent stale leaders from corrupting data:</p>
<ol>
<li>Lock service issues monotonically increasing <span>fencing token</span> with each lock grant</li>
<li>Client includes token in all storage requests</li>
<li>Storage rejects requests with tokens lower than previously seen</li>
<li>If old leader wakes up with stale token, its writes are rejected</li>
</ol>
<div>
<strong>Follow-up Q8.1.1: What if the storage system doesn't support fencing tokens?</strong>
<p>A: You have a dangerous gap. Mitigations:</p>
<ol>
<li><span>Lease with margin</span>: Set lock TTL &gt; max clock skew + max GC pause + max network delay</li>
<li><span>Double-check pattern</span>: Re-verify lock ownership before critical operations</li>
<li><span>Idempotent operations</span>: Design operations so replay is safe</li>
<li><span>Use storage with native fencing</span>: ZooKeeper's ephemeral nodes, etcd's leases</li>
</ol>
<p>Key insight from Martin Kleppmann: <span>Distributed locks alone don't provide safety</span> - you need fencing for correctness.</p>
</div>
</div>
</div>
<hr />
<h2 id="best-practices">Best Practices</h2>
<div>
<ol>
<li><strong>Start with requirements, not technology</strong> - Understand your consistency and availability needs before choosing a database</li>
<li><strong>Use tunable consistency wisely</strong> - Strong consistency for critical paths (payments), eventual for others (analytics)</li>
<li><strong>Plan for partition recovery</strong> - How will you reconcile divergent data? Test this!</li>
<li><strong>Monitor replication lag</strong> - Even &quot;consistent&quot; systems have replication delays. Alert on high lag.</li>
<li><strong>Implement retry logic</strong> - CP systems will reject requests during partitions. Clients must retry.</li>
<li><strong>Use idempotency keys</strong> - Retries should be safe. Include unique request IDs.</li>
<li><strong>Document your consistency model</strong> - Developers need to understand what guarantees they have</li>
<li><strong>Test failure scenarios</strong> - Use chaos engineering to validate behavior during partitions</li>
</ol>
</div>
<hr />
<h2 id="related-topics">Related Topics</h2>
<pre><code>                - [[Database Replication]](/topic/system-design/database-replication) - How data is copied between nodes
                - [[Consensus Algorithms]](/topic/system-design/consensus-algorithms) - Raft, Paxos for CP systems
                - [[Distributed Locking]](/topic/system-design/distributed-locking) - Coordinating access across nodes
                - [[Database Sharding]](/topic/system-design/database-sharding) - Horizontal partitioning strategies
                - [[Event Sourcing]](/topic/system-design/event-sourcing) - Append-only logs for eventual consistency
                - [[Rate Limiting]](/topic/system-design/rate-limiting) - Protecting systems during overload
</code></pre>
