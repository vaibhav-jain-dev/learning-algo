<h1 id="microservices-architecture">Microservices Architecture</h1>
<h2 id="the-restaurant-kitchen-analogy">The Restaurant Kitchen Analogy</h2>
<p>Imagine a small restaurant where one chef does everything - takes orders, preps ingredients, cooks, plates, and cleans. This works great with few customers. But when the restaurant gets popular, everything bottlenecks on that one person. If they get sick, the entire restaurant closes.</p>
<p>Now imagine a large restaurant kitchen: one station for appetizers, another for entrees, a dedicated pastry chef, and prep cooks. Each station operates independently, specializes in their craft, and can scale (add more cooks) based on demand. If the dessert station breaks down, customers can still get their main courses.</p>
<p><strong>That's the difference between monolithic and microservices architecture.</strong></p>
<hr />
<h2 id="real-world-context-how-netflix-transformed">Real-World Context: How Netflix Transformed</h2>
<p>In 2008, Netflix had a massive database corruption that took down their entire service for three days. Their monolithic architecture meant one failure cascaded everywhere. This near-death experience triggered their famous migration to microservices.</p>
<p>Today, Netflix runs over <strong>1,000 microservices</strong> handling 2 billion API requests daily. Each service - recommendations, user profiles, streaming, billing - operates independently. When their recommendation engine has issues, you can still watch videos.</p>
<div>
<h4>MONOLITH vs MICROSERVICES</h4>
<div>
<div>
<div>
<div></div>
<span>Monolith</span>
</div>
<div>
<div>
<div>Users</div>
<div>Orders</div>
<div>Inventory</div>
</div>
<div>
<span>Single Database</span>
</div>
</div>
<div>
  One codebase, one deployment, one failure point
</div>
</div>
<div>
<div>
<div></div>
<span>Microservices</span>
</div>
<div>
<div>
<div>
  Users<br/>Service
</div>
<div>
  User DB
</div>
</div>
<div>
<div>
  Orders<br/>Service
</div>
<div>
  Order DB
</div>
</div>
<div>
<div>
  Inventory<br/>Service
</div>
<div>
  Inv DB
</div>
</div>
</div>
<div>
  Independent deployment, isolated failures
</div>
</div>
</div>
</div>
<hr />
<h2 id="core-principles">Core Principles</h2>
<h3 id="1-single-responsibility">1. Single Responsibility</h3>
<p>Each service does ONE thing well - like a specialist doctor vs a general practitioner.</p>
<p><strong>Amazon Example</strong>: Amazon has separate services for cart, checkout, recommendations, reviews, and inventory. The team that owns &quot;Add to Cart&quot; doesn't touch payment processing.</p>
<h3 id="2-database-per-service">2. Database Per Service</h3>
<p>Each service owns its data completely - no shared databases.</p>
<p><strong>Why this matters</strong>: When Uber's ride-matching service needs to change its database schema, it shouldn't require coordinating with the payment team.</p>
<h3 id="3-smart-endpoints-dumb-pipes">3. Smart Endpoints, Dumb Pipes</h3>
<p>Services communicate through simple protocols (HTTP/gRPC). The intelligence lives in the services, not the message broker.</p>
<h3 id="4-design-for-failure">4. Design for Failure</h3>
<p>Assume everything will fail. Netflix's famous Chaos Monkey randomly kills production services to ensure teams build resilient systems.</p>
<hr />
<h2 id="service-boundaries-domain-driven-design">Service Boundaries: Domain-Driven Design</h2>
<div>
<h4>BOUNDED CONTEXTS & SERVICE BOUNDARIES</h4>
<div>
<div>
<div>Order Context</div>
<div>
<div>Entities:</div>
<div>Order, OrderItem, OrderStatus</div>
</div>
<div>
<div>Operations:</div>
<div>Create, Cancel, Update</div>
</div>
</div>
<div>
<div>Payment Context</div>
<div>
<div>Entities:</div>
<div>Payment, Refund, Invoice</div>
</div>
<div>
<div>Operations:</div>
<div>Charge, Refund, Verify</div>
</div>
</div>
<div>
<div>Shipping Context</div>
<div>
<div>Entities:</div>
<div>Shipment, Address, Carrier</div>
</div>
<div>
<div>Operations:</div>
<div>Ship, Track, Return</div>
</div>
</div>
</div>
<div>
<div>
<div><span>Anti-Corruption Layer</span> translates between contexts</div>
</div>
</div>
</div>
<p><span>Bounded contexts</span> from <a href="/topic/system-design/ddd">[Domain-Driven Design]</a> define natural service boundaries. Each context has its own <span>ubiquitous language</span> - &quot;Customer&quot; in Sales means something different than in Shipping.</p>
<h3 id="identifying-service-boundaries">Identifying Service Boundaries</h3>
<p><strong>Key Questions to Ask:</strong></p>
<ol>
<li>Can this capability be deployed independently?</li>
<li>Does it have a clear owner (single team)?</li>
<li>Does it align with a business capability?</li>
<li>Can it scale independently based on its own demand?</li>
</ol>
<p><strong>Warning Signs of Wrong Boundaries:</strong></p>
<ul>
<li>Services that always deploy together</li>
<li>Circular dependencies between services</li>
<li>Distributed monolith - changes require coordinating multiple teams</li>
<li>Data duplication without clear ownership</li>
</ul>
<hr />
<h2 id="service-communication-patterns">Service Communication Patterns</h2>
<div>
<h4>COMMUNICATION PATTERNS</h4>
<div>
<div>
<div>Synchronous (Request-Response)</div>
<div>Service A waits for Service B's response</div>
<div>
  Order Service --> User Service<br/>
  "Get user #123"<br/>
  <-- Returns user data
</div>
<div>
<span>Best for:</span> <span>Real-time queries, simple flows</span>
</div>
</div>
<div>
<div>Asynchronous (Event-Driven)</div>
<div>Service A publishes event, doesn't wait</div>
<div>
  Order Service --> Event Bus<br/>
  "OrderCreated event"<br/>
  Inventory, Email, Analytics listen
</div>
<div>
<span>Best for:</span> <span>Decoupling, high throughput</span>
</div>
</div>
</div>
</div>
<h3 id="inter-service-communication-deep-dive">Inter-Service Communication Deep Dive</h3>
<div>
<h4>COMMUNICATION PROTOCOLS COMPARISON</h4>
<div>
<table>
  <thead>
<tr>
<th>Protocol</th>
<th>Latency</th>
<th>Use Case</th>
<th>Trade-offs</th>
</tr>
  </thead>
  <tbody>
<tr>
<td><strong>REST/HTTP</strong></td>
<td>~10-100ms</td>
<td>CRUD operations, external APIs</td>
<td>Simple but verbose</td>
</tr>
<tr>
<td><strong>gRPC</strong></td>
<td>~1-10ms</td>
<td>Internal service-to-service</td>
<td>Fast but needs proto files</td>
</tr>
<tr>
<td><strong>GraphQL</strong></td>
<td>~10-50ms</td>
<td>API aggregation, BFF pattern</td>
<td>Flexible but complex caching</td>
</tr>
<tr>
<td><strong>Message Queue</strong></td>
<td>~1-1000ms</td>
<td>Async workflows, event sourcing</td>
<td>Decoupled but eventual consistency</td>
</tr>
  </tbody>
</table>
</div>
</div>
<p>See <a href="/topic/system-design/api-gateway">[API Gateway]</a> for routing patterns and <a href="/topic/system-design/message-queues">[Message Queues]</a> for async communication details.</p>
<h3 id="python-implementation-event-driven-communication">Python Implementation: Event-Driven Communication</h3>
<pre><code class="language-python">import json
import redis
from dataclasses import dataclass, asdict
from typing import Callable, Dict, List
from datetime import datetime

@dataclass
class Event:
    &quot;&quot;&quot;Base event class for all domain events&quot;&quot;&quot;
    event_type: str
    timestamp: str
    data: dict

    def to_json(self) -&gt; str:
        return json.dumps(asdict(self))

    @classmethod
    def from_json(cls, json_str: str) -&gt; 'Event':
        return cls(**json.loads(json_str))

class EventBus:
    &quot;&quot;&quot;
    Simple event bus using Redis pub/sub
    Used by: Uber for ride events, Shopify for order events
    &quot;&quot;&quot;
    def __init__(self, redis_url: str = &quot;redis://localhost:6379&quot;):
        self.redis = redis.from_url(redis_url)
        self.pubsub = self.redis.pubsub()
        self.handlers: Dict[str, List[Callable]] = {}

    def publish(self, event: Event) -&gt; None:
        &quot;&quot;&quot;Publish event to all subscribers&quot;&quot;&quot;
        channel = event.event_type
        self.redis.publish(channel, event.to_json())
        print(f&quot;Published: {event.event_type}&quot;)

    def subscribe(self, event_type: str, handler: Callable) -&gt; None:
        &quot;&quot;&quot;Register handler for event type&quot;&quot;&quot;
        if event_type not in self.handlers:
            self.handlers[event_type] = []
            self.pubsub.subscribe(event_type)
        self.handlers[event_type].append(handler)

    def start_listening(self) -&gt; None:
        &quot;&quot;&quot;Start processing events (run in separate thread)&quot;&quot;&quot;
        for message in self.pubsub.listen():
            if message['type'] == 'message':
                event = Event.from_json(message['data'])
                for handler in self.handlers.get(event.event_type, []):
                    try:
                        handler(event)
                    except Exception as e:
                        print(f&quot;Handler error: {e}&quot;)

# Order Service - publishes events
class OrderService:
    def __init__(self, event_bus: EventBus):
        self.event_bus = event_bus
        self.orders = {}

    def create_order(self, user_id: str, items: list, total: float) -&gt; dict:
        order_id = f&quot;order_{len(self.orders) + 1}&quot;
        order = {
            &quot;id&quot;: order_id,
            &quot;user_id&quot;: user_id,
            &quot;items&quot;: items,
            &quot;total&quot;: total,
            &quot;status&quot;: &quot;created&quot;
        }
        self.orders[order_id] = order

        # Publish event - other services react
        self.event_bus.publish(Event(
            event_type=&quot;order.created&quot;,
            timestamp=datetime.utcnow().isoformat(),
            data=order
        ))

        return order

# Inventory Service - subscribes to order events
class InventoryService:
    def __init__(self, event_bus: EventBus):
        self.inventory = {&quot;item_1&quot;: 100, &quot;item_2&quot;: 50}
        event_bus.subscribe(&quot;order.created&quot;, self.handle_order_created)

    def handle_order_created(self, event: Event) -&gt; None:
        &quot;&quot;&quot;Reserve inventory when order is created&quot;&quot;&quot;
        for item in event.data[&quot;items&quot;]:
            item_id = item[&quot;product_id&quot;]
            quantity = item[&quot;quantity&quot;]
            if self.inventory.get(item_id, 0) &gt;= quantity:
                self.inventory[item_id] -= quantity
                print(f&quot;Reserved {quantity} of {item_id}&quot;)
            else:
                print(f&quot;Insufficient inventory for {item_id}&quot;)

# Notification Service - also subscribes
class NotificationService:
    def __init__(self, event_bus: EventBus):
        event_bus.subscribe(&quot;order.created&quot;, self.handle_order_created)

    def handle_order_created(self, event: Event) -&gt; None:
        &quot;&quot;&quot;Send confirmation email when order is created&quot;&quot;&quot;
        user_id = event.data[&quot;user_id&quot;]
        order_id = event.data[&quot;id&quot;]
        print(f&quot;Sending email to {user_id} for order {order_id}&quot;)

# Usage
event_bus = EventBus()
order_service = OrderService(event_bus)
inventory_service = InventoryService(event_bus)
notification_service = NotificationService(event_bus)

# When order is created, both services react automatically
order = order_service.create_order(
    user_id=&quot;user_123&quot;,
    items=[{&quot;product_id&quot;: &quot;item_1&quot;, &quot;quantity&quot;: 2}],
    total=99.99
)
</code></pre>
<hr />
<h2 id="the-circuit-breaker-pattern">The Circuit Breaker Pattern</h2>
<p><strong>The Analogy</strong>: Electrical circuit breakers trip when there's too much current, preventing house fires. Software circuit breakers &quot;trip&quot; when a service is failing, preventing cascade failures.</p>
<div>
<h4>CIRCUIT BREAKER STATES</h4>
<div>
<div>
<div>
<span>CLOSED</span>
</div>
<div>Normal flow</div>
</div>
<div>--></div>
<div>
<div>
<span>OPEN</span>
</div>
<div>Fail fast</div>
</div>
<div>--></div>
<div>
<div>
<span>HALF-OPEN</span>
</div>
<div>Test recovery</div>
</div>
</div>
</div>
<h3 id="python-implementation-production-circuit-breaker">Python Implementation: Production Circuit Breaker</h3>
<pre><code class="language-python">import time
from enum import Enum
from threading import Lock
from typing import Callable, Any, Optional
from functools import wraps

class CircuitState(Enum):
    CLOSED = &quot;closed&quot;      # Normal operation
    OPEN = &quot;open&quot;          # Failing fast
    HALF_OPEN = &quot;half_open&quot;  # Testing recovery

class CircuitBreaker:
    &quot;&quot;&quot;
    Production-grade circuit breaker
    Similar to Netflix Hystrix, Resilience4j
    &quot;&quot;&quot;
    def __init__(
        self,
        failure_threshold: int = 5,
        recovery_timeout: int = 30,
        half_open_max_calls: int = 3
    ):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.half_open_max_calls = half_open_max_calls

        self.state = CircuitState.CLOSED
        self.failures = 0
        self.successes = 0
        self.last_failure_time: Optional[float] = None
        self.half_open_calls = 0
        self.lock = Lock()

    def call(self, func: Callable, *args, **kwargs) -&gt; Any:
        &quot;&quot;&quot;Execute function with circuit breaker protection&quot;&quot;&quot;
        with self.lock:
            if self.state == CircuitState.OPEN:
                if self._should_attempt_reset():
                    self.state = CircuitState.HALF_OPEN
                    self.half_open_calls = 0
                else:
                    raise CircuitOpenError(
                        f&quot;Circuit is OPEN. Retry after {self._time_until_retry():.1f}s&quot;
                    )

            if self.state == CircuitState.HALF_OPEN:
                if self.half_open_calls &gt;= self.half_open_max_calls:
                    raise CircuitOpenError(&quot;Circuit HALF_OPEN: max test calls reached&quot;)
                self.half_open_calls += 1

        try:
            result = func(*args, **kwargs)
            self._on_success()
            return result
        except Exception as e:
            self._on_failure()
            raise

    def _should_attempt_reset(self) -&gt; bool:
        return (
            self.last_failure_time is not None and
            time.time() - self.last_failure_time &gt;= self.recovery_timeout
        )

    def _time_until_retry(self) -&gt; float:
        if self.last_failure_time is None:
            return 0
        elapsed = time.time() - self.last_failure_time
        return max(0, self.recovery_timeout - elapsed)

    def _on_success(self) -&gt; None:
        with self.lock:
            if self.state == CircuitState.HALF_OPEN:
                self.successes += 1
                if self.successes &gt;= self.half_open_max_calls:
                    self._reset()
            elif self.state == CircuitState.CLOSED:
                self.failures = 0

    def _on_failure(self) -&gt; None:
        with self.lock:
            self.failures += 1
            self.last_failure_time = time.time()

            if self.state == CircuitState.HALF_OPEN:
                self._trip()
            elif self.failures &gt;= self.failure_threshold:
                self._trip()

    def _trip(self) -&gt; None:
        self.state = CircuitState.OPEN
        print(f&quot;Circuit TRIPPED - now OPEN&quot;)

    def _reset(self) -&gt; None:
        self.state = CircuitState.CLOSED
        self.failures = 0
        self.successes = 0
        print(f&quot;Circuit RESET - now CLOSED&quot;)

class CircuitOpenError(Exception):
    pass

# Decorator for easy use
def circuit_breaker(breaker: CircuitBreaker):
    def decorator(func: Callable) -&gt; Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            return breaker.call(func, *args, **kwargs)
        return wrapper
    return decorator

# Usage example
user_service_breaker = CircuitBreaker(failure_threshold=3, recovery_timeout=10)

@circuit_breaker(user_service_breaker)
def get_user_from_service(user_id: str) -&gt; dict:
    &quot;&quot;&quot;Call external user service&quot;&quot;&quot;
    import requests
    response = requests.get(f&quot;http://user-service/users/{user_id}&quot;, timeout=5)
    response.raise_for_status()
    return response.json()

# With fallback
def get_user_with_fallback(user_id: str) -&gt; dict:
    try:
        return get_user_from_service(user_id)
    except CircuitOpenError:
        # Return cached data when circuit is open
        return get_cached_user(user_id)
    except Exception:
        return {&quot;id&quot;: user_id, &quot;name&quot;: &quot;Unknown&quot;, &quot;cached&quot;: True}
</code></pre>
<hr />
<h2 id="data-consistency-patterns">Data Consistency Patterns</h2>
<div>
<h4>DATA CONSISTENCY SPECTRUM</h4>
<div>
<div>Strong</div>
<div>Eventual</div>
<div>Weak</div>
</div>
<div>
<div>
<div>2PC / Saga</div>
<div>Guaranteed consistency across services</div>
</div>
<div>
<div>Outbox Pattern</div>
<div>Reliable event publishing</div>
</div>
<div>
<div>Fire & Forget</div>
<div>Best effort, may lose events</div>
</div>
</div>
</div>
<h3 id="the-outbox-pattern">The Outbox Pattern</h3>
<p><span>The Outbox Pattern</span> solves the dual-write problem: how do you reliably update your database AND publish an event atomically?</p>
<div>
<h4>OUTBOX PATTERN FLOW</h4>
<div>
<div>
<div>
<div>Step 1: Single Transaction</div>
<div>Write business data + outbox event in same DB transaction</div>
</div>
</div>
<div>|</div>
<div>
<div>
<div>Step 2: Outbox Reader</div>
<div>Background process polls outbox table, publishes to message broker</div>
</div>
</div>
<div>|</div>
<div>
<div>
<div>Step 3: Mark Processed</div>
<div>Delete or mark outbox record after successful publish</div>
</div>
</div>
</div>
</div>
<pre><code class="language-python">from datetime import datetime
from typing import Optional
import json

class OutboxPattern:
    &quot;&quot;&quot;
    Transactional Outbox Pattern implementation
    Ensures atomic updates between business data and events
    &quot;&quot;&quot;

    def __init__(self, db_session):
        self.db = db_session

    def create_order_with_outbox(self, order_data: dict) -&gt; dict:
        &quot;&quot;&quot;
        Creates order and outbox entry in single transaction
        &quot;&quot;&quot;
        with self.db.begin_transaction() as tx:
            # 1. Insert business data
            order = tx.execute(
                &quot;INSERT INTO orders (user_id, total, status) VALUES (?, ?, ?) RETURNING id&quot;,
                (order_data['user_id'], order_data['total'], 'created')
            )
            order_id = order.fetchone()[0]

            # 2. Insert outbox event in SAME transaction
            event_payload = {
                'event_type': 'order.created',
                'order_id': order_id,
                'user_id': order_data['user_id'],
                'total': order_data['total'],
                'timestamp': datetime.utcnow().isoformat()
            }

            tx.execute(
                &quot;&quot;&quot;INSERT INTO outbox
                   (aggregate_type, aggregate_id, event_type, payload, created_at)
                   VALUES (?, ?, ?, ?, ?)&quot;&quot;&quot;,
                ('order', order_id, 'order.created',
                 json.dumps(event_payload), datetime.utcnow())
            )

            tx.commit()  # Both succeed or both fail

        return {'id': order_id, **order_data}


class OutboxPublisher:
    &quot;&quot;&quot;
    Background process that reads outbox and publishes events
    Run as separate worker/cron job
    &quot;&quot;&quot;

    def __init__(self, db_session, message_broker):
        self.db = db_session
        self.broker = message_broker

    def process_outbox(self, batch_size: int = 100):
        &quot;&quot;&quot;
        Poll outbox table and publish pending events
        &quot;&quot;&quot;
        # Get unpublished events
        events = self.db.execute(
            &quot;&quot;&quot;SELECT id, aggregate_type, aggregate_id, event_type, payload
               FROM outbox
               WHERE published_at IS NULL
               ORDER BY created_at
               LIMIT ?&quot;&quot;&quot;,
            (batch_size,)
        ).fetchall()

        for event in events:
            try:
                # Publish to message broker
                self.broker.publish(
                    topic=event['event_type'],
                    message=event['payload']
                )

                # Mark as published
                self.db.execute(
                    &quot;UPDATE outbox SET published_at = ? WHERE id = ?&quot;,
                    (datetime.utcnow(), event['id'])
                )
                self.db.commit()

            except Exception as e:
                # Log and continue - will retry on next poll
                print(f&quot;Failed to publish event {event['id']}: {e}&quot;)
                self.db.rollback()
</code></pre>
<p>See <a href="/topic/system-design/event-sourcing">[Event Sourcing]</a> for event-based data storage and <a href="/topic/system-design/cqrs">[CQRS]</a> for read/write separation.</p>
<hr />
<h2 id="the-saga-pattern-distributed-transactions">The Saga Pattern: Distributed Transactions</h2>
<p><strong>The Problem</strong>: In a monolith, you wrap multiple operations in a database transaction. With microservices, each service has its own database - traditional transactions don't work across service boundaries.</p>
<p><strong>The Solution</strong>: Saga pattern - a sequence of local transactions where each step has a compensating action for rollback.</p>
<div>
<h4>SAGA PATTERN: ORDER PROCESSING</h4>
<div>
<div>
<div>1. Create Order</div>
<span>--></span>
<div>Compensate: Cancel Order</div>
</div>
<div>
<div>2. Reserve Stock</div>
<span>--></span>
<div>Compensate: Release Stock</div>
</div>
<div>
<div>3. Charge Payment</div>
<span>--></span>
<div>Compensate: Refund Payment</div>
</div>
<div>
<div>4. Ship Order</div>
<span>--></span>
<div>Final step - no compensation</div>
</div>
</div>
</div>
<h3 id="choreography-vs-orchestration">Choreography vs Orchestration</h3>
<div>
<h4>SAGA COORDINATION STYLES</h4>
<div>
<div>
<div>Choreography</div>
<div>
<div>
<div>Order</div>
<div>Inventory</div>
<div>Payment</div>
</div>
<div>Services react to events independently</div>
</div>
<div>
<div>+ No central coordinator</div>
<div>+ Loose coupling</div>
<div>- Hard to track flow</div>
</div>
</div>
<div>
<div>Orchestration</div>
<div>
<div>
<div>Orchestrator</div>
</div>
<div>
<div>Order</div>
<div>Inventory</div>
<div>Payment</div>
</div>
</div>
<div>
<div>+ Clear flow control</div>
<div>+ Easy to understand</div>
<div>- Single point of coordination</div>
</div>
</div>
</div>
</div>
<h3 id="python-implementation-saga-orchestrator">Python Implementation: Saga Orchestrator</h3>
<pre><code class="language-python">from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from enum import Enum

class SagaStatus(Enum):
    PENDING = &quot;pending&quot;
    EXECUTING = &quot;executing&quot;
    COMPLETED = &quot;completed&quot;
    COMPENSATING = &quot;compensating&quot;
    FAILED = &quot;failed&quot;

@dataclass
class SagaStep:
    name: str
    execute: callable
    compensate: callable

class SagaOrchestrator:
    &quot;&quot;&quot;
    Orchestration-based saga pattern
    Used by: Uber (trip booking), Airbnb (reservation)
    &quot;&quot;&quot;
    def __init__(self, steps: List[SagaStep]):
        self.steps = steps
        self.status = SagaStatus.PENDING
        self.completed_steps: List[SagaStep] = []
        self.context: Dict[str, Any] = {}

    def execute(self, initial_context: Dict[str, Any]) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Execute saga steps in order, compensate on failure&quot;&quot;&quot;
        self.context = initial_context.copy()
        self.status = SagaStatus.EXECUTING

        for step in self.steps:
            try:
                print(f&quot;Executing: {step.name}&quot;)
                result = step.execute(self.context)
                self.context.update(result or {})
                self.completed_steps.append(step)
            except Exception as e:
                print(f&quot;Step '{step.name}' failed: {e}&quot;)
                self._compensate()
                self.status = SagaStatus.FAILED
                raise SagaFailedError(f&quot;Saga failed at '{step.name}': {e}&quot;)

        self.status = SagaStatus.COMPLETED
        return self.context

    def _compensate(self) -&gt; None:
        &quot;&quot;&quot;Execute compensating transactions in reverse order&quot;&quot;&quot;
        self.status = SagaStatus.COMPENSATING
        print(&quot;Starting compensation...&quot;)

        for step in reversed(self.completed_steps):
            try:
                print(f&quot;Compensating: {step.name}&quot;)
                step.compensate(self.context)
            except Exception as e:
                # Log but continue - compensation must complete
                print(f&quot;Compensation failed for '{step.name}': {e}&quot;)

class SagaFailedError(Exception):
    pass

# Real-world example: E-commerce order saga
class OrderSaga:
    def __init__(self, order_service, inventory_service, payment_service, shipping_service):
        self.saga = SagaOrchestrator([
            SagaStep(
                name=&quot;create_order&quot;,
                execute=lambda ctx: self._create_order(ctx, order_service),
                compensate=lambda ctx: self._cancel_order(ctx, order_service)
            ),
            SagaStep(
                name=&quot;reserve_inventory&quot;,
                execute=lambda ctx: self._reserve_inventory(ctx, inventory_service),
                compensate=lambda ctx: self._release_inventory(ctx, inventory_service)
            ),
            SagaStep(
                name=&quot;process_payment&quot;,
                execute=lambda ctx: self._process_payment(ctx, payment_service),
                compensate=lambda ctx: self._refund_payment(ctx, payment_service)
            ),
            SagaStep(
                name=&quot;schedule_shipping&quot;,
                execute=lambda ctx: self._schedule_shipping(ctx, shipping_service),
                compensate=lambda ctx: None  # Final step - no compensation
            )
        ])

    def _create_order(self, ctx, service) -&gt; dict:
        order = service.create(ctx['user_id'], ctx['items'])
        return {'order_id': order['id']}

    def _cancel_order(self, ctx, service) -&gt; None:
        service.cancel(ctx['order_id'])

    def _reserve_inventory(self, ctx, service) -&gt; dict:
        reservation = service.reserve(ctx['items'])
        return {'reservation_id': reservation['id']}

    def _release_inventory(self, ctx, service) -&gt; None:
        service.release(ctx['reservation_id'])

    def _process_payment(self, ctx, service) -&gt; dict:
        payment = service.charge(ctx['user_id'], ctx['total'])
        return {'payment_id': payment['id']}

    def _refund_payment(self, ctx, service) -&gt; None:
        service.refund(ctx['payment_id'])

    def _schedule_shipping(self, ctx, service) -&gt; dict:
        shipment = service.schedule(ctx['order_id'], ctx['address'])
        return {'shipment_id': shipment['id']}

    def process_order(self, user_id: str, items: list, total: float, address: str) -&gt; dict:
        return self.saga.execute({
            'user_id': user_id,
            'items': items,
            'total': total,
            'address': address
        })
</code></pre>
<hr />
<h2 id="deployment-strategies">Deployment Strategies</h2>
<div>
<h4>DEPLOYMENT STRATEGIES COMPARISON</h4>
<div>
<div>
<div>Rolling Deployment</div>
<div>
<div></div>
<div></div>
<div></div>
<div></div>
</div>
<div>Replace instances one at a time. Zero downtime, gradual rollout.</div>
</div>
<div>
<div>Blue-Green Deployment</div>
<div>
<div>Blue (Live)</div>
<div>Green (New)</div>
</div>
<div>Run two identical environments, switch traffic instantly.</div>
</div>
<div>
<div>Canary Deployment</div>
<div>
<div></div>
<div></div>
</div>
<div>Route small % of traffic to new version, monitor, then expand.</div>
</div>
<div>
<div>Feature Flags</div>
<div>
  if (feature.enabled("new_checkout"))<br/>
  &nbsp;&nbsp;showNewCheckout()
</div>
<div>Toggle features without deployment. Per-user or percentage rollout.</div>
</div>
</div>
</div>
<h3 id="deployment-pipeline-best-practices">Deployment Pipeline Best Practices</h3>
<p><span>Immutable infrastructure</span> means you never modify running servers - you replace them. Combined with <span>containerization</span> via Docker and orchestration with <a href="/topic/system-design/kubernetes">[Kubernetes]</a>, this enables reliable, repeatable deployments.</p>
<pre><code class="language-yaml"># Example: Kubernetes Rolling Update Configuration
apiVersion: apps/v1
kind: Deployment
metadata:
  name: order-service
spec:
  replicas: 4
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1      # At most 1 pod unavailable during update
      maxSurge: 1            # At most 1 extra pod during update
  template:
    spec:
      containers:
      - name: order-service
        image: order-service:v2.0.0
        readinessProbe:        # Only receive traffic when ready
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
        livenessProbe:         # Restart if unhealthy
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
</code></pre>
<p>See <a href="/topic/system-architectures/code-deployment">[Code Deployment]</a> for comprehensive CI/CD patterns.</p>
<hr />
<h2 id="monitoring--observability">Monitoring &amp; Observability</h2>
<div>
<h4>THE THREE PILLARS OF OBSERVABILITY</h4>
<div>
<div>
<div>Metrics</div>
<div>Numerical measurements over time</div>
<div>
<div>RED Method:</div>
<div>Rate - requests/sec</div>
<div>Errors - error rate</div>
<div>Duration - latency</div>
</div>
<div>Tools: Prometheus, Datadog</div>
</div>
<div>
<div>Logs</div>
<div>Timestamped event records</div>
<div>
<div>Best Practices:</div>
<div>Structured JSON format</div>
<div>Correlation IDs</div>
<div>Log levels (INFO/WARN/ERROR)</div>
</div>
<div>Tools: ELK Stack, Loki</div>
</div>
<div>
<div>Traces</div>
<div>Request flow across services</div>
<div>
<div>Contains:</div>
<div>Trace ID (full journey)</div>
<div>Span ID (single hop)</div>
<div>Timing & metadata</div>
</div>
<div>Tools: Jaeger, Zipkin</div>
</div>
</div>
</div>
<h3 id="distributed-tracing-implementation">Distributed Tracing Implementation</h3>
<p><span>Distributed tracing</span> is essential for debugging microservices. Every request gets a <span>trace ID</span> that follows it through all services.</p>
<div>
<h4>DISTRIBUTED TRACE VISUALIZATION</h4>
<div>
<div>Trace ID: <code>abc123</code></div>
<div>
<div>
<div>API Gateway</div>
<div>
<span>150ms</span>
</div>
</div>
<div>
<div>Order Service</div>
<div></div>
<div>
<span>80ms</span>
</div>
</div>
<div>
<div>User Service</div>
<div></div>
<div>
<span>25ms</span>
</div>
</div>
<div>
<div>Inventory DB</div>
<div></div>
<div>
<span>15ms</span>
</div>
</div>
</div>
</div>
</div>
<pre><code class="language-python">import uuid
from functools import wraps
from typing import Optional, Dict, Any
from datetime import datetime
import json

class Span:
    &quot;&quot;&quot;Represents a single unit of work in a trace&quot;&quot;&quot;

    def __init__(self, trace_id: str, span_id: str, operation: str,
                 parent_span_id: Optional[str] = None):
        self.trace_id = trace_id
        self.span_id = span_id
        self.parent_span_id = parent_span_id
        self.operation = operation
        self.start_time = datetime.utcnow()
        self.end_time: Optional[datetime] = None
        self.tags: Dict[str, Any] = {}
        self.logs: list = []

    def set_tag(self, key: str, value: Any) -&gt; 'Span':
        self.tags[key] = value
        return self

    def log(self, message: str) -&gt; 'Span':
        self.logs.append({
            'timestamp': datetime.utcnow().isoformat(),
            'message': message
        })
        return self

    def finish(self):
        self.end_time = datetime.utcnow()

    @property
    def duration_ms(self) -&gt; float:
        if self.end_time:
            return (self.end_time - self.start_time).total_seconds() * 1000
        return 0

    def to_dict(self) -&gt; dict:
        return {
            'trace_id': self.trace_id,
            'span_id': self.span_id,
            'parent_span_id': self.parent_span_id,
            'operation': self.operation,
            'start_time': self.start_time.isoformat(),
            'duration_ms': self.duration_ms,
            'tags': self.tags,
            'logs': self.logs
        }


class Tracer:
    &quot;&quot;&quot;
    Distributed tracing implementation
    Compatible with OpenTelemetry/Jaeger concepts
    &quot;&quot;&quot;

    _current_span: Optional[Span] = None

    def __init__(self, service_name: str, exporter=None):
        self.service_name = service_name
        self.exporter = exporter  # Send to Jaeger/Zipkin

    def start_span(self, operation: str,
                   trace_id: Optional[str] = None) -&gt; Span:
        &quot;&quot;&quot;Start a new span, optionally continuing existing trace&quot;&quot;&quot;
        if trace_id is None:
            trace_id = str(uuid.uuid4())

        span_id = str(uuid.uuid4())[:16]
        parent_span_id = self._current_span.span_id if self._current_span else None

        span = Span(trace_id, span_id, operation, parent_span_id)
        span.set_tag('service', self.service_name)

        self._current_span = span
        return span

    def finish_span(self, span: Span):
        &quot;&quot;&quot;Finish span and export&quot;&quot;&quot;
        span.finish()

        if self.exporter:
            self.exporter.export(span.to_dict())
        else:
            # Default: structured log output
            print(json.dumps(span.to_dict()))

        self._current_span = None

    def inject_headers(self) -&gt; Dict[str, str]:
        &quot;&quot;&quot;Get headers to propagate trace context&quot;&quot;&quot;
        if self._current_span:
            return {
                'X-Trace-Id': self._current_span.trace_id,
                'X-Span-Id': self._current_span.span_id
            }
        return {}

    def extract_headers(self, headers: Dict[str, str]) -&gt; Optional[str]:
        &quot;&quot;&quot;Extract trace ID from incoming request headers&quot;&quot;&quot;
        return headers.get('X-Trace-Id')


def traced(tracer: Tracer, operation: str):
    &quot;&quot;&quot;Decorator to automatically trace function calls&quot;&quot;&quot;
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            span = tracer.start_span(operation)
            try:
                result = func(*args, **kwargs)
                span.set_tag('status', 'success')
                return result
            except Exception as e:
                span.set_tag('status', 'error')
                span.set_tag('error.message', str(e))
                span.log(f&quot;Exception: {e}&quot;)
                raise
            finally:
                tracer.finish_span(span)
        return wrapper
    return decorator


# Usage Example
tracer = Tracer('order-service')

@traced(tracer, 'create_order')
def create_order(user_id: str, items: list) -&gt; dict:
    # This call is automatically traced
    user = get_user(user_id)
    inventory = check_inventory(items)
    return {'order_id': '123', 'user': user, 'inventory': inventory}

@traced(tracer, 'get_user')
def get_user(user_id: str) -&gt; dict:
    # Makes HTTP call with trace headers
    import requests
    headers = tracer.inject_headers()
    response = requests.get(f'http://user-service/users/{user_id}', headers=headers)
    return response.json()
</code></pre>
<hr />
<h2 id="failure-stories-and-lessons">Failure Stories and Lessons</h2>
<h3 id="amazons-2017-s3-outage">Amazon's 2017 S3 Outage</h3>
<p>A typo in a command took down a major portion of the internet. <strong>Lesson</strong>: Even giants fail. Design for failure, implement chaos engineering, and have runbooks.</p>
<h3 id="knight-capitals-440m-loss-2012">Knight Capital's $440M Loss (2012)</h3>
<p>A deployment issue with their trading microservices caused erratic trades. <strong>Lesson</strong>: Feature flags, canary deployments, and instant rollback capabilities are essential.</p>
<h3 id="ubers-microservices-migration-pain">Uber's Microservices Migration Pain</h3>
<p>Uber grew from monolith to 2,200+ microservices. They learned that too many services creates operational nightmare. <strong>Lesson</strong>: Don't create a &quot;distributed monolith&quot; - services should be cohesive.</p>
<hr />
<h2 id="interview-deep-dive-3-level-recursive-qa">Interview Deep-Dive: 3-Level Recursive Q&amp;A</h2>
<div>
<h4>SERVICE BOUNDARIES & DOMAIN DESIGN</h4>
<div>
<p>Q1: How do you determine the right boundaries for microservices?</p>
<div>
<p>
<strong>Answer:</strong> Use <span>Domain-Driven Design (DDD)</span> to identify bounded contexts. Each service should align with a business capability, have a single team owner, and be independently deployable. Key techniques include: (1) <span>Event Storming</span> - workshop to discover domain events and aggregates, (2) Analyze team communication patterns - Conway's Law suggests system boundaries mirror org structure, (3) Look for natural seams where data consistency requirements change.
</p>
</div>
<div>
<p>Q1.1: What if you get the boundaries wrong initially?</p>
<div>
<p>
<strong>Answer:</strong> Wrong boundaries manifest as: services always deploying together, excessive inter-service calls, or data duplication without clear ownership. Solutions: (1) <span>Merge services</span> if they're too fine-grained, (2) <span>Extract services</span> if a service does too much, (3) Use the <span>Strangler Fig pattern</span> for gradual refactoring. The cost of wrong boundaries is high - you're essentially doing a distributed monolith refactor.
</p>
</div>
<div>
<p>Q1.1.1: How does the Strangler Fig pattern work for service boundary changes?</p>
<div>
<p>
<strong>Answer:</strong> The Strangler Fig pattern incrementally replaces functionality without big-bang rewrites. For boundary changes: (1) Create new service with correct boundary, (2) Use an <span>Anti-Corruption Layer</span> to translate between old/new, (3) Gradually route traffic to new service using feature flags, (4) Once migrated, decommission old service. Netflix used this extensively during their cloud migration - they called it "building the plane while flying it."
</p>
</div>
</div>
</div>
</div>
</div>
<div>
<h4>INTER-SERVICE COMMUNICATION</h4>
<div>
<p>Q2: When should you use synchronous vs asynchronous communication?</p>
<div>
<p>
<strong>Answer:</strong> Use <span>synchronous (REST/gRPC)</span> when: the caller needs an immediate response, the operation is simple query/command, and latency is acceptable. Use <span>asynchronous (events/queues)</span> when: you need to decouple services, handle high throughput, or the operation can complete later. Rule of thumb: prefer async for anything that modifies state across services - it enables <span>eventual consistency</span> and better fault tolerance.
</p>
</div>
<div>
<p>Q2.1: How do you handle failures in async communication?</p>
<div>
<p>
<strong>Answer:</strong> Key patterns: (1) <span>Dead Letter Queues (DLQ)</span> - failed messages go to separate queue for analysis, (2) <span>Idempotency</span> - design handlers to safely process same message multiple times, (3) <span>Retry with exponential backoff</span> - retry 3 times with increasing delays before DLQ, (4) <span>Outbox pattern</span> - ensure events are published reliably. Always include correlation IDs to trace message flows.
</p>
</div>
<div>
<p>Q2.1.1: How do you implement idempotency in event handlers?</p>
<div>
<p>
<strong>Answer:</strong> Three approaches: (1) <span>Idempotency keys</span> - store processed event IDs in database, check before processing, (2) <span>Natural idempotency</span> - design operations to be naturally safe (e.g., "set status to X" vs "increment counter"), (3) <span>Deduplication window</span> - track recent events in Redis with TTL. For critical operations, use database constraints - if inserting with same idempotency key fails with unique constraint, the event was already processed.
</p>
</div>
</div>
</div>
</div>
</div>
<div>
<h4>DATA CONSISTENCY</h4>
<div>
<p>Q3: How do you handle data consistency across microservices without distributed transactions?</p>
<div>
<p>
<strong>Answer:</strong> Accept <span>eventual consistency</span> as the default. Use: (1) <span>Saga pattern</span> for distributed transactions with compensating actions, (2) <span>Outbox pattern</span> for reliable event publishing, (3) <span>CQRS</span> to separate read/write models and allow async synchronization. Design business processes to tolerate temporary inconsistency - e.g., "order pending" status while payment processes. Strong consistency across services requires distributed locking and has significant performance costs.
</p>
</div>
<div>
<p>Q3.1: What happens if a saga step fails after others have committed?</p>
<div>
<p>
<strong>Answer:</strong> Execute <span>compensating transactions</span> in reverse order. Key principles: (1) Compensations must be <span>idempotent</span> - they might run multiple times, (2) They must <span>eventually succeed</span> - keep retrying with alerts, (3) Some actions are <span>non-compensatable</span> (e.g., sent email) - design around this. Store saga state persistently so recovery can resume after crashes. If compensation truly fails, alert humans - this is a business-level exception.
</p>
</div>
<div>
<p>Q3.1.1: How do you handle the case where a compensation action itself fails?</p>
<div>
<p>
<strong>Answer:</strong> This is the "saga of sagas" problem. Solutions: (1) <span>Infinite retry with alerting</span> - compensation must eventually succeed, alert after N failures, (2) <span>Manual intervention queue</span> - failed compensations go to human review, (3) <span>Reconciliation jobs</span> - periodic batch jobs detect and fix inconsistencies. Design compensations to be simple and unlikely to fail. Example: if refund fails, store "refund_pending" and let a reconciliation job handle it. This is where <span>event sourcing</span> helps - you can always replay to fix state.
</p>
</div>
</div>
</div>
</div>
</div>
<div>
<h4>DEPLOYMENT STRATEGIES</h4>
<div>
<p>Q4: How do you safely deploy changes to production microservices?</p>
<div>
<p>
<strong>Answer:</strong> Layer multiple strategies: (1) <span>Canary deployments</span> - route 1-5% traffic to new version, monitor errors/latency, (2) <span>Feature flags</span> - deploy code dark, enable gradually per user segment, (3) <span>Blue-green deployments</span> - instant rollback capability, (4) <span>Automated rollback</span> - if error rate exceeds threshold, auto-revert. Every deployment should be reversible within minutes. Use immutable infrastructure - never patch running instances.
</p>
</div>
<div>
<p>Q4.1: How do you handle database schema changes with zero-downtime deployments?</p>
<div>
<p>
<strong>Answer:</strong> Use <span>expand-contract pattern</span> (also called parallel change): (1) <span>Expand</span> - add new column/table, deploy code that writes to both old and new, (2) <span>Migrate</span> - backfill old data to new structure, (3) <span>Contract</span> - deploy code that reads from new only, then drop old. Never rename columns directly - add new, migrate, drop old. For large tables, use online schema change tools (pt-online-schema-change, gh-ost) to avoid locking.
</p>
</div>
<div>
<p>Q4.1.1: What if a migration fails midway during the expand phase?</p>
<div>
<p>
<strong>Answer:</strong> Design migrations to be <span>resumable and idempotent</span>: (1) Track migration progress (e.g., "last processed ID"), (2) Use batched updates with checkpoints, (3) Application code should handle both old and new schema during migration window. If migration corrupts data, restore from backup to the new column only - old data in original column is untouched. This is why expand-contract works: the old path remains functional throughout. For critical tables, test migrations on production-clone database first.
</p>
</div>
</div>
</div>
</div>
</div>
<div>
<h4>MONITORING & DEBUGGING</h4>
<div>
<p>Q5: How do you debug issues that span multiple microservices?</p>
<div>
<p>
<strong>Answer:</strong> Implement the <span>three pillars of observability</span>: (1) <span>Distributed tracing</span> (Jaeger/Zipkin) - trace ID follows request across all services, shows latency per hop, (2) <span>Centralized logging</span> (ELK/Datadog) - correlation ID in all logs, structured JSON format, (3) <span>Metrics</span> (Prometheus/Grafana) - RED metrics per service (Rate, Errors, Duration). Create dashboards showing request flow. When debugging, start with trace to identify slow/failing service, then dive into that service's logs.
</p>
</div>
<div>
<p>Q5.1: How do you set up alerting that doesn't cause alert fatigue?</p>
<div>
<p>
<strong>Answer:</strong> Follow <span>SRE alerting principles</span>: (1) Alert on <span>symptoms, not causes</span> - "users can't checkout" not "CPU high", (2) Define <span>SLOs</span> (Service Level Objectives) and alert on SLO breach, (3) Every alert must be <span>actionable</span> - if no action needed, it's not an alert, (4) Use <span>error budgets</span> - if 99.9% SLO, you have 43 minutes/month of allowed downtime. Distinguish pages (wake someone up) from tickets (fix tomorrow). Regularly review and tune alerts.
</p>
</div>
<div>
<p>Q5.1.1: How do you define good SLOs for microservices?</p>
<div>
<p>
<strong>Answer:</strong> SLOs should reflect <span>user experience</span>: (1) <span>Availability</span> - "99.9% of requests succeed", (2) <span>Latency</span> - "95% of requests complete in <200ms, 99% in <1s", (3) <span>Correctness</span> - "99.99% of orders processed correctly". Start with what users actually need - don't promise 99.99% if 99.9% is sufficient (that's 10x harder). Different services need different SLOs - payment needs higher than recommendations. Track SLIs (indicators) continuously, compare against SLOs, alert when error budget burns too fast.
</p>
</div>
</div>
</div>
</div>
</div>
<hr />
<h2 id="common-interview-questions-quick-reference">Common Interview Questions: Quick Reference</h2>
<div>
<h4>RAPID-FIRE Q&A</h4>
<div>
<div>
<p>When should you NOT use microservices?</p>
<p>Small team (<10 devs), unclear domain boundaries, tight deadlines, early-stage startup, or when performance requires tight coupling. The operational overhead isn't worth it for simple applications.</p>
</div>
<div>
<p>How do you handle API versioning?</p>
<p>Use <span>semantic versioning</span> in URLs (/v1/users) or headers. Apply <span>tolerant reader pattern</span> - ignore unknown fields. Run multiple versions simultaneously during migration. Use <span>consumer-driven contracts</span> (Pact) to verify compatibility.</p>
</div>
<div>
<p>What's the difference between orchestration and choreography?</p>
<p><span>Orchestration</span>: Central coordinator directs workflow (easier to understand, single point of control). <span>Choreography</span>: Services react to events independently (more decoupled, harder to trace). Use orchestration for complex business processes, choreography for simple event reactions.</p>
</div>
<div>
<p>How do you handle service discovery?</p>
<p>Options: (1) <span>DNS-based</span> (Kubernetes Services) - simple, built-in, (2) <span>Service registry</span> (Consul, Eureka) - more features, health checks, (3) <span>Service mesh</span> (Istio, Linkerd) - transparent, advanced traffic management. Kubernetes DNS is sufficient for most cases.</p>
</div>
<div>
<p>What is a distributed monolith and how do you avoid it?</p>
<p>A distributed monolith has microservice deployment but monolithic coupling - services can't deploy independently, share databases, or have circular dependencies. Avoid by: enforcing <span>database per service</span>, using <span>async communication</span>, defining clear <span>API contracts</span>, and ensuring teams can deploy independently.</p>
</div>
</div>
</div>
<hr />
<h2 id="quick-reference-card">Quick Reference Card</h2>
<div>
<h4>MICROSERVICES QUICK REFERENCE</h4>
<div>
<div>
<h5>Communication</h5>
<ul>
<li>Sync: REST, gRPC (real-time)</li>
<li>Async: Events, Message Queues</li>
<li>Always set timeouts</li>
<li>Use circuit breakers</li>
</ul>
</div>
<div>
<h5>Data Patterns</h5>
<ul>
<li>Database per service</li>
<li>Saga for transactions</li>
<li>Event sourcing for audit</li>
<li>CQRS for read/write split</li>
</ul>
</div>
<div>
<h5>Resilience</h5>
<ul>
<li>Circuit breaker (fail fast)</li>
<li>Retry with backoff</li>
<li>Bulkhead (isolation)</li>
<li>Fallback (degraded mode)</li>
</ul>
</div>
<div>
<h5>Observability</h5>
<ul>
<li>Distributed tracing</li>
<li>Centralized logging</li>
<li>Metrics (RED: Rate, Error, Duration)</li>
<li>Health checks</li>
</ul>
</div>
</div>
</div>
<hr />
<h2 id="related-topics">Related Topics</h2>
<ul>
<li><a href="/topic/system-design/api-gateway">[API Gateway]</a> - Single entry point for microservices</li>
<li><a href="/topic/system-design/message-queues">[Message Queues]</a> - Async communication patterns</li>
<li><a href="/topic/system-design/service-discovery">[Service Discovery]</a> - Finding service instances</li>
<li><a href="/topic/system-design/event-sourcing">[Event Sourcing]</a> - Event-based data storage</li>
<li><a href="/topic/system-design/cqrs">[CQRS]</a> - Command Query Responsibility Segregation</li>
<li><a href="/topic/system-design/distributed-locking">[Distributed Locking]</a> - Coordination across services</li>
<li><a href="/topic/system-design/rate-limiting">[Rate Limiting]</a> - Protecting services from overload</li>
<li><a href="/topic/system-design/load-balancing">[Load Balancing]</a> - Distributing traffic across instances</li>
<li><a href="/topic/system-architectures/code-deployment">[Code Deployment]</a> - CI/CD and deployment strategies</li>
<li><a href="/topic/system-design/kubernetes">[Kubernetes]</a> - Container orchestration</li>
</ul>
