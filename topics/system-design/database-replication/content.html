<h1 id="database-replication">Database Replication</h1>
<h2 id="overview">Overview</h2>
<p><span style="color: #10b981; font-weight: 600">Database replication</span> is the process of maintaining multiple copies of data across different database servers to achieve high availability, fault tolerance, and improved read performance. Think of it like having backup copies of an important document stored in different locations - if one copy is lost or damaged, you can still access the others.</p>
<p>The key challenge is keeping all copies synchronized - when you update the original, how and when do the copies get updated? This decision profoundly impacts your system's consistency, availability, and performance characteristics.</p>
<div style="background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%); border-radius: 16px; padding: 28px; margin: 24px 0">
<h4 style="margin-top: 0; color: #1e40af; font-size: 18px">Core Trade-off Equation</h4>
<div style="font-family: 'Courier New', monospace; font-size: 16px; background: #eff6ff; padding: 16px; border-radius: 8px; text-align: center; color: #1e293b">
    Replication = Consistency vs Availability vs Latency
</div>
<div style="margin-top: 16px; color: #475569; font-size: 14px; text-align: center">
    Synchronous replication favors consistency; Asynchronous replication favors availability and latency
</div>
</div>
<p><strong>Critical Assumption</strong>: Replication assumes that network partitions are rare but possible. The choice between synchronous and asynchronous replication fundamentally determines how your system behaves during these partitions.</p>
<p><strong>Key Trade-off</strong>: <span style="color: #10b981; font-weight: 600">Durability vs Latency</span>. Synchronous replication guarantees no data loss but adds network round-trip latency to every write. Asynchronous replication provides fast writes but risks losing recently committed data during failover.</p>
<hr />
<h2 id="database-replication-architecture">Database Replication Architecture</h2>
<div style="background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%); border-radius: 16px; padding: 32px; margin: 20px 0">
<div style="text-align: center; color: #1e293b; font-size: 18px; font-weight: 600; margin-bottom: 24px; padding-bottom: 16px">DATABASE REPLICATION TOPOLOGY COMPARISON</div>
<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 24px">
<div style="background: linear-gradient(135deg, #eff6ff 0%, #dbeafe 100%); border-radius: 12px; padding: 20px">
<div style="color: #1e40af; font-weight: 600; font-size: 16px; margin-bottom: 16px; text-align: center">SINGLE-PRIMARY (Master-Slave)</div>
<div style="display: flex; flex-direction: column; align-items: center; gap: 12px">
<div style="background: linear-gradient(135deg, #3b82f6 0%, #1d4ed8 100%); color: white; padding: 12px 24px; border-radius: 8px; font-weight: 600">
  PRIMARY (Writes)
</div>
<div style="display: flex; gap: 8px; color: #6366f1">
<span style="font-size: 20px">|</span>
<span style="font-size: 20px">|</span>
<span style="font-size: 20px">|</span>
</div>
<div style="display: flex; gap: 12px">
<div style="background: #dcfce7; color: #166534; padding: 8px 16px; border-radius: 6px; font-size: 14px">Replica 1</div>
<div style="background: #dcfce7; color: #166534; padding: 8px 16px; border-radius: 6px; font-size: 14px">Replica 2</div>
<div style="background: #dcfce7; color: #166534; padding: 8px 16px; border-radius: 6px; font-size: 14px">Replica 3</div>
</div>
<div style="color: #64748b; font-size: 12px; text-align: center; margin-top: 8px">One writer, multiple readers</div>
</div>
</div>
<div style="background: linear-gradient(135deg, #fef3c7 0%, #fde68a 100%); border-radius: 12px; padding: 20px">
<div style="color: #92400e; font-weight: 600; font-size: 16px; margin-bottom: 16px; text-align: center">MULTI-PRIMARY (Master-Master)</div>
<div style="display: flex; flex-direction: column; align-items: center; gap: 12px">
<div style="display: flex; gap: 16px">
<div style="background: linear-gradient(135deg, #f59e0b 0%, #d97706 100%); color: white; padding: 12px 20px; border-radius: 8px; font-weight: 600; font-size: 14px">
  PRIMARY A
</div>
<div style="display: flex; flex-direction: column; align-items: center; justify-content: center; color: #d97706">
<span style="font-size: 12px">sync</span>
<span>⟷</span>
</div>
<div style="background: linear-gradient(135deg, #f59e0b 0%, #d97706 100%); color: white; padding: 12px 20px; border-radius: 8px; font-weight: 600; font-size: 14px">
  PRIMARY B
</div>
</div>
<div style="display: flex; gap: 40px; color: #92400e; font-size: 16px">
<span>|</span>
<span>|</span>
</div>
<div style="display: flex; gap: 40px">
<div style="background: #dcfce7; color: #166534; padding: 6px 12px; border-radius: 6px; font-size: 13px">Replica</div>
<div style="background: #dcfce7; color: #166534; padding: 6px 12px; border-radius: 6px; font-size: 13px">Replica</div>
</div>
<div style="color: #64748b; font-size: 12px; text-align: center; margin-top: 8px">Multiple writers, conflict resolution required</div>
</div>
</div>
</div>
</div>
<hr />
<h2 id="why-it-matters-real-company-examples">Why It Matters: Real Company Examples</h2>
<div style="background: linear-gradient(135deg, #f0fdf4 0%, #dcfce7 100%); border-radius: 12px; padding: 24px; margin: 20px 0">
<p><strong>Netflix</strong> uses replication across multiple AWS regions. When you press play, you're often reading from a replica geographically close to you, reducing latency from 200ms to 20ms.</p>
<p><strong>GitHub</strong> replicates every repository to multiple data centers. When their primary DC had issues in 2018, they failed over to replicas - but a 43-second <span style="color: #10b981; font-weight: 600">replication lag</span> caused some data inconsistency, teaching them to improve their replication monitoring.</p>
<p><strong>Shopify</strong> handles Black Friday traffic (4M+ requests/second) by routing reads to replicas while writes go to the primary. This lets them scale reads infinitely without overloading their write path.</p>
<p><strong>Instagram</strong> uses multi-region replication for their PostgreSQL clusters. User data is replicated globally so that a user in Tokyo sees their feed as fast as a user in New York.</p>
</div>
<hr />
<h2 id="section-1-master-slave-primary-replica-replication">Section 1: Master-Slave (Primary-Replica) Replication</h2>
<h3 id="deep-mechanics">Deep Mechanics</h3>
<p><span style="color: #10b981; font-weight: 600">Master-slave replication</span> (also called primary-replica or leader-follower) is the most common replication topology. One database node (the primary/master) accepts all write operations and propagates changes to read-only replicas (slaves/followers).</p>
<div style="background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%); border-radius: 16px; padding: 32px; margin: 20px 0">
<div style="text-align: center; color: #1e293b; font-size: 18px; font-weight: 600; margin-bottom: 24px; padding-bottom: 16px">MASTER-SLAVE WRITE PROPAGATION FLOW</div>
<div style="display: flex; flex-direction: column; gap: 16px">
<div style="display: flex; align-items: center; gap: 16px; flex-wrap: wrap">
<div style="background: linear-gradient(135deg, #a855f7 0%, #7c3aed 100%); border-radius: 8px; padding: 12px 20px; color: white">
<span style="font-weight: 600">Application</span>
</div>
<div style="display: flex; flex-direction: column; align-items: center; color: #6366f1">
<span style="font-size: 12px">INSERT/UPDATE</span>
<span style="font-size: 18px">→</span>
</div>
<div style="background: linear-gradient(135deg, #3b82f6 0%, #1d4ed8 100%); border-radius: 8px; padding: 12px 20px; color: white">
<span style="font-weight: 600">PRIMARY</span>
</div>
</div>
<div style="margin-left: 180px; background: #f1f5f9; border-radius: 8px; padding: 16px; max-width: 400px">
<div style="color: #1e293b; font-weight: 600; margin-bottom: 8px">Primary Actions:</div>
<div style="color: #475569; font-size: 14px; line-height: 1.8">
<div>1. Execute transaction locally</div>
<div>2. Write to binary log (MySQL) or WAL (PostgreSQL)</div>
<div>3. Commit transaction</div>
<div>4. Send log events to replicas</div>
</div>
</div>
<div style="display: flex; align-items: flex-start; gap: 16px; margin-left: 180px; flex-wrap: wrap">
<div style="display: flex; flex-direction: column; align-items: center; gap: 8px">
<div style="color: #22c55e; font-size: 14px">binlog/WAL</div>
<div style="display: flex; gap: 24px">
<span style="color: #22c55e; font-size: 18px">↓</span>
<span style="color: #22c55e; font-size: 18px">↓</span>
<span style="color: #22c55e; font-size: 18px">↓</span>
</div>
</div>
</div>
<div style="display: flex; gap: 16px; margin-left: 140px; flex-wrap: wrap">
<div style="background: linear-gradient(135deg, #f0fdf4 0%, #dcfce7 100%); border-radius: 8px; padding: 12px 20px">
<div style="color: #166534; font-weight: 600">Replica 1</div>
<div style="color: #22c55e; font-size: 11px">lag: 50ms</div>
</div>
<div style="background: linear-gradient(135deg, #f0fdf4 0%, #dcfce7 100%); border-radius: 8px; padding: 12px 20px">
<div style="color: #166534; font-weight: 600">Replica 2</div>
<div style="color: #22c55e; font-size: 11px">lag: 120ms</div>
</div>
<div style="background: linear-gradient(135deg, #fef2f2 0%, #fee2e2 100%); border-radius: 8px; padding: 12px 20px">
<div style="color: #991b1b; font-weight: 600">Replica 3</div>
<div style="color: #ef4444; font-size: 11px">lag: 5s (degraded)</div>
</div>
</div>
</div>
</div>
<h3 id="internal-replication-architecture">Internal Replication Architecture</h3>
<pre><code class="language-python">class ReplicationManager:
    &quot;&quot;&quot;
    Production replication manager for single-primary topology.

    Key Components:
    - Binary Log: Sequential record of all data modifications
    - Relay Log: Replica's local copy of received binary log events
    - Replication Thread: Applies relay log events to replica data

    Internal State Machine:
    1. IO Thread: Connects to primary, receives binlog events
    2. SQL Thread: Reads relay log, applies changes locally
    3. Position Tracking: Maintains binlog position for resume
    &quot;&quot;&quot;

    def __init__(self, role: str, primary_host: str = None):
        self.role = role  # 'primary' or 'replica'
        self.primary_host = primary_host
        self.binlog_position = BinlogPosition(file='', pos=0)
        self.relay_log = RelayLog()
        self.replication_state = 'disconnected'

    def setup_as_replica(self, primary_host: str, binlog_file: str, position: int):
        &quot;&quot;&quot;
        Configure this node as a replica of the specified primary.

        CHANGE MASTER TO equivalent in MySQL:
        1. Record primary connection details
        2. Set starting binlog position
        3. Start IO and SQL threads

        Edge Case: If position is wrong, replica may skip or duplicate transactions.
        Always use GTID (Global Transaction ID) when available for position-agnostic replication.
        &quot;&quot;&quot;
        self.primary_host = primary_host
        self.binlog_position = BinlogPosition(file=binlog_file, pos=position)

        # Start replication threads
        self._start_io_thread()
        self._start_sql_thread()

    def _start_io_thread(self):
        &quot;&quot;&quot;
        IO thread connects to primary and streams binlog events.

        Protocol (MySQL):
        1. Send COM_BINLOG_DUMP with position
        2. Receive binlog events as network packets
        3. Write events to local relay log
        4. Update master.info with new position

        Failure Handling:
        - Network timeout: Reconnect with exponential backoff
        - Primary failover: Wait for new primary, re-connect
        - Binlog purged: Requires full re-sync (snapshot + replay)
        &quot;&quot;&quot;
        async def io_thread_loop():
            while True:
                try:
                    connection = await self._connect_to_primary()
                    async for event in connection.stream_binlog(self.binlog_position):
                        self.relay_log.write(event)
                        self.binlog_position = event.next_position
                except ConnectionError:
                    await asyncio.sleep(self._backoff_delay())

        asyncio.create_task(io_thread_loop())

    def _start_sql_thread(self):
        &quot;&quot;&quot;
        SQL thread reads relay log and applies changes locally.

        Apply Process:
        1. Read event from relay log
        2. Parse event type (INSERT, UPDATE, DELETE, DDL)
        3. Apply to local storage engine
        4. Update relay-log.info position

        Critical Edge Cases:
        - Non-deterministic functions (RAND(), NOW()): Row-based replication solves this
        - DDL statements: May require table lock, blocking reads
        - Large transactions: Apply in chunks to avoid memory exhaustion
        &quot;&quot;&quot;
        async def sql_thread_loop():
            while True:
                event = await self.relay_log.read_next()
                if event:
                    await self._apply_event(event)
                else:
                    await asyncio.sleep(0.001)  # No events, brief pause

        asyncio.create_task(sql_thread_loop())

    async def _apply_event(self, event: BinlogEvent):
        &quot;&quot;&quot;Apply a single binlog event to local storage.&quot;&quot;&quot;
        if event.type == EventType.WRITE_ROWS:
            await self.storage.insert(event.table, event.rows)
        elif event.type == EventType.UPDATE_ROWS:
            await self.storage.update(event.table, event.before, event.after)
        elif event.type == EventType.DELETE_ROWS:
            await self.storage.delete(event.table, event.rows)
        elif event.type == EventType.QUERY:
            # DDL statements (CREATE TABLE, ALTER TABLE, etc.)
            await self.storage.execute_ddl(event.query)
</code></pre>
<h3 id="edge-cases-and-failure-modes">Edge Cases and Failure Modes</h3>
<div style="background: #fef3c7;border-radius: 12px; padding: 24px; margin: 20px 0">
<h4 style="color: #92400e; margin-top: 0">Critical Edge Cases in Master-Slave Replication</h4>
<div style="display: grid; gap: 12px">
<div style="background: white; padding: 16px; border-radius: 8px">
<strong style="color: #92400e">Primary Crash Before Binlog Flush</strong>
<p style="color: #475569; margin: 8px 0 0 0; font-size: 14px">Transaction commits to storage but binlog event not yet written. After failover, promoted replica is missing this transaction. Solution: Use <span style="color: #10b981; font-weight: 600">sync_binlog=1</span> (MySQL) to flush binlog on every commit, at cost of write performance.</p>
</div>
<div style="background: white; padding: 16px; border-radius: 8px">
<strong style="color: #92400e">Binlog Purged Before Replica Caught Up</strong>
<p style="color: #475569; margin: 8px 0 0 0; font-size: 14px">Primary rotates and deletes old binlog files. Slow replica needs events from deleted file. Solution: Monitor replica lag, set <code>expire_logs_days</code> conservatively, or use GTID for position-independent replay.</p>
</div>
<div style="background: white; padding: 16px; border-radius: 8px">
<strong style="color: #92400e">Replica Drift Due to Direct Writes</strong>
<p style="color: #475569; margin: 8px 0 0 0; font-size: 14px">Someone writes directly to replica (e.g., for analytics). Data diverges from primary. Replication breaks on conflicting changes. Solution: Set <code>read_only=ON</code> on all replicas, enforce at network level.</p>
</div>
<div style="background: white; padding: 16px; border-radius: 8px">
<strong style="color: #92400e">Split-Brain After Network Partition</strong>
<p style="color: #475569; margin: 8px 0 0 0; font-size: 14px">Network partition isolates primary. Automated failover promotes replica. Partition heals - now two primaries. Solution: Use <span style="color: #10b981; font-weight: 600">fencing</span> (STONITH) to shut down old primary, or quorum-based [[consensus-algorithms]](/topic/system-design/consensus-algorithms).</p>
</div>
</div>
</div>
<h3 id="master-slave-interview-questions-3-levels-deep">Master-Slave Interview Questions (3 Levels Deep)</h3>
<div style="background: #eff6ff;border-radius: 12px; padding: 24px; margin: 20px 0">
<h4 style="color: #1e40af; margin-top: 0">Level 1: What is master-slave replication and when would you use it?</h4>
<p style="color: #1e293b; line-height: 1.7"><strong>Answer:</strong> Master-slave replication is a database topology where one primary node accepts all writes and replicates changes to one or more read-only replica nodes. Use it when you need: (1) Read scalability - distribute read traffic across replicas, (2) High availability - promote a replica if primary fails, (3) Geographic distribution - place replicas close to users, (4) Backup isolation - take backups from replica without impacting primary. The trade-off is eventual consistency on reads from replicas.</p>
<div style="background: white; border-radius: 8px; padding: 20px; margin-top: 16px">
<h5 style="color: #1e40af; margin-top: 0">Level 2: How do you handle the "read-your-writes" consistency problem in a read-replica architecture?</h5>
<p style="color: #1e293b; line-height: 1.7"><strong>Answer:</strong> <span style="color: #10b981; font-weight: 600">Read-your-writes consistency</span> ensures users always see their own recent writes. Solutions: (1) <strong>Sticky sessions</strong>: Route user's reads to primary for N seconds after their writes. (2) <strong>Causal consistency tokens</strong>: Track write position (LSN/GTID), only read from replicas that have reached that position. (3) <strong>Synchronous replication</strong>: Wait for at least one replica to confirm before returning success (adds latency). (4) <strong>Application-level caching</strong>: Cache written data client-side, merge with replica reads. Best approach depends on use case - social media can tolerate lag, banking cannot. See [[caching]](/topic/system-design/caching) for cache-aside patterns.</p>
<div style="background: #f8fafc; border-radius: 8px; padding: 16px; margin-top: 12px">
<h6 style="color: #1e40af; margin-top: 0">Level 3: Your primary fails and you need to promote a replica. How do you ensure no data loss and prevent split-brain?</h6>
<p style="color: #1e293b; line-height: 1.7; font-size: 14px"><strong>Answer:</strong> This requires coordinated failover with multiple safety mechanisms. (1) <strong>Data loss prevention</strong>: Before failover, check all replicas' replication lag. Promote the replica with lowest lag (most up-to-date). If using semi-synchronous replication, at least one replica has all committed transactions. If primary is reachable but unhealthy, drain remaining binlog before shutdown. (2) <strong>Split-brain prevention</strong>: Use <span style="color: #10b981; font-weight: 600">fencing</span> (STONITH - Shoot The Other Node In The Head) to forcibly power off old primary before promotion. Use <span style="color: #10b981; font-weight: 600">quorum-based leader election</span> - only promote if majority of nodes agree old primary is down. Implement <span style="color: #10b981; font-weight: 600">epoch numbers</span> - increment epoch on promotion, reject writes from lower epochs. (3) <strong>Client redirection</strong>: Update DNS (slow, TTL issues), virtual IP failover (fast, same network), or service discovery update. (4) <strong>Post-failover</strong>: When old primary recovers, it must rejoin as replica, not as primary. Validate data consistency with checksum comparison. Tools: MySQL Orchestrator, PostgreSQL Patroni, or cloud-managed failover (RDS Multi-AZ). Related: [[distributed-locking]](/topic/system-design/distributed-locking) for coordinating failover.</p>
</div>
</div>
</div>
<hr />
<h2 id="section-2-multi-master-multi-primary-replication">Section 2: Multi-Master (Multi-Primary) Replication</h2>
<h3 id="deep-mechanics-1">Deep Mechanics</h3>
<p><span style="color: #10b981; font-weight: 600">Multi-master replication</span> allows multiple nodes to accept writes simultaneously. Each primary replicates its changes to other primaries, creating a mesh of bidirectional replication. This provides write availability during partitions but introduces the challenge of <span style="color: #10b981; font-weight: 600">write conflicts</span>.</p>
<div style="background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%); border-radius: 16px; padding: 32px; margin: 20px 0">
<div style="text-align: center; color: #1e293b; font-size: 18px; font-weight: 600; margin-bottom: 24px; padding-bottom: 16px">MULTI-MASTER REPLICATION WITH CONFLICT DETECTION</div>
<div style="display: flex; justify-content: center; align-items: center; gap: 40px; flex-wrap: wrap">
<div style="display: flex; flex-direction: column; align-items: center; gap: 16px">
<div style="background: linear-gradient(135deg, #3b82f6 0%, #1d4ed8 100%); color: white; padding: 16px 28px; border-radius: 12px; font-weight: 600">
  Region US-EAST
</div>
<div style="background: #dbeafe; padding: 12px 20px; border-radius: 8px; font-size: 14px; color: #1e40af">
  Primary A
</div>
<div style="background: #fef3c7; padding: 8px 16px; border-radius: 6px; font-size: 12px; color: #92400e">
  User: Alice = 100
</div>
</div>
<div style="display: flex; flex-direction: column; align-items: center; gap: 8px">
<div style="color: #6366f1; font-size: 12px">bidirectional sync</div>
<div style="color: #6366f1; font-size: 24px">⟷</div>
<div style="background: #fef2f2; padding: 8px 16px; border-radius: 6px">
<div style="color: #991b1b; font-size: 12px; font-weight: 600">CONFLICT!</div>
<div style="color: #7f1d1d; font-size: 11px">Both wrote same row</div>
</div>
</div>
<div style="display: flex; flex-direction: column; align-items: center; gap: 16px">
<div style="background: linear-gradient(135deg, #10b981 0%, #059669 100%); color: white; padding: 16px 28px; border-radius: 12px; font-weight: 600">
  Region EU-WEST
</div>
<div style="background: #dcfce7; padding: 12px 20px; border-radius: 8px; font-size: 14px; color: #166534">
  Primary B
</div>
<div style="background: #fef3c7; padding: 8px 16px; border-radius: 6px; font-size: 12px; color: #92400e">
  User: Alice = 150
</div>
</div>
</div>
<div style="margin-top: 24px; text-align: center; color: #64748b; font-size: 13px">
    Concurrent writes to the same row create conflicts requiring resolution strategy
</div>
</div>
<h3 id="when-to-use-multi-master">When to Use Multi-Master</h3>
<div style="background: #f8fafc;border-radius: 12px; padding: 24px; margin: 20px 0">
<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 24px">
<div>
<div style="background: #dcfce7; padding: 16px; border-radius: 8px; margin-bottom: 12px">
<strong style="color: #166534">Good Use Cases</strong>
<ul style="color: #475569; margin: 8px 0 0 0; padding-left: 20px; font-size: 14px">
<li>Geographic write locality (reduce latency)</li>
<li>Write availability during network partitions</li>
<li>Collaborative editing (with CRDTs)</li>
<li>Mobile offline-first applications</li>
</ul>
</div>
</div>
<div>
<div style="background: #fef2f2; padding: 16px; border-radius: 8px; margin-bottom: 12px">
<strong style="color: #991b1b">Poor Use Cases</strong>
<ul style="color: #475569; margin: 8px 0 0 0; padding-left: 20px; font-size: 14px">
<li>Strong consistency requirements</li>
<li>Financial transactions</li>
<li>Inventory management</li>
<li>Anything needing ACID guarantees</li>
</ul>
</div>
</div>
</div>
</div>
<h3 id="conflict-resolution-strategies">Conflict Resolution Strategies</h3>
<div style="background: linear-gradient(135deg, #7c3aed 0%, #a78bfa 100%); border-radius: 16px; padding: 28px; margin: 24px 0; color: white">
<h4 style="margin-top: 0; color: #f8fafc">Conflict Resolution Approaches</h4>
<div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 16px; margin-top: 16px">
<div style="background: rgba(255,255,255,0.15); padding: 16px; border-radius: 8px">
<strong style="color: white">Last-Write-Wins (LWW)</strong>
<p style="color: #e0e7ff; margin: 8px 0 0 0; font-size: 13px">Timestamp-based. Higher timestamp wins. Simple but causes data loss. Requires synchronized clocks. Used by: Cassandra, DynamoDB.</p>
</div>
<div style="background: rgba(255,255,255,0.15); padding: 16px; border-radius: 8px">
<strong style="color: white">Custom Merge Functions</strong>
<p style="color: #e0e7ff; margin: 8px 0 0 0; font-size: 13px">Application-specific logic. E.g., for counters: add deltas. For sets: union. Requires domain knowledge. Used by: CouchDB.</p>
</div>
<div style="background: rgba(255,255,255,0.15); padding: 16px; border-radius: 8px">
<strong style="color: white">CRDTs (Conflict-free Replicated Data Types)</strong>
<p style="color: #e0e7ff; margin: 8px 0 0 0; font-size: 13px">Mathematically guaranteed convergence. G-Counter, PN-Counter, OR-Set. No conflicts by design. Used by: Riak, Redis CRDT.</p>
</div>
<div style="background: rgba(255,255,255,0.15); padding: 16px; border-radius: 8px">
<strong style="color: white">Operational Transform (OT)</strong>
<p style="color: #e0e7ff; margin: 8px 0 0 0; font-size: 13px">Transform operations against concurrent ops. Complex implementation. Used by: Google Docs, real-time collaboration.</p>
</div>
</div>
</div>
<h3 id="conflict-resolution-implementation">Conflict Resolution Implementation</h3>
<pre><code class="language-python">from typing import Any, Callable, Dict, List, Optional
from dataclasses import dataclass
from datetime import datetime
import hashlib


@dataclass
class VersionedValue:
    &quot;&quot;&quot;Value with vector clock for conflict detection.&quot;&quot;&quot;
    value: Any
    vector_clock: Dict[str, int]  # node_id -&gt; logical_timestamp
    physical_timestamp: datetime
    node_id: str


class ConflictResolver:
    &quot;&quot;&quot;
    Multi-strategy conflict resolution for multi-master replication.

    Conflict Detection:
    - Vector clocks detect concurrent writes (neither happened-before the other)
    - Physical timestamps break ties but have clock skew issues

    Resolution Strategies:
    - LWW: Simple but loses data
    - Merge: Preserves data but complex
    - CRDT: No conflicts by design
    &quot;&quot;&quot;

    def __init__(self, strategy: str = 'lww', merge_fn: Callable = None):
        self.strategy = strategy
        self.merge_fn = merge_fn

    def detect_conflict(self, v1: VersionedValue, v2: VersionedValue) -&gt; bool:
        &quot;&quot;&quot;
        Detect if two versions are concurrent (conflict).

        Using vector clocks:
        - v1 &lt; v2: v1[i] &lt;= v2[i] for all i, and v1[j] &lt; v2[j] for some j
        - v1 &gt; v2: opposite
        - v1 || v2 (concurrent): neither v1 &lt; v2 nor v1 &gt; v2
        &quot;&quot;&quot;
        v1_less = False
        v2_less = False

        all_nodes = set(v1.vector_clock.keys()) | set(v2.vector_clock.keys())

        for node in all_nodes:
            t1 = v1.vector_clock.get(node, 0)
            t2 = v2.vector_clock.get(node, 0)

            if t1 &lt; t2:
                v1_less = True
            elif t1 &gt; t2:
                v2_less = True

        # Concurrent if neither dominates
        return v1_less and v2_less

    def resolve(self, versions: List[VersionedValue]) -&gt; VersionedValue:
        &quot;&quot;&quot;
        Resolve conflict between multiple concurrent versions.

        Edge Cases:
        - Empty versions list: raise error
        - Single version: return as-is
        - No actual conflict: return latest by vector clock
        &quot;&quot;&quot;
        if not versions:
            raise ValueError(&quot;No versions to resolve&quot;)

        if len(versions) == 1:
            return versions[0]

        # Check if actually concurrent
        concurrent_versions = self._find_concurrent(versions)

        if len(concurrent_versions) == 1:
            return concurrent_versions[0]

        # Apply resolution strategy
        if self.strategy == 'lww':
            return self._resolve_lww(concurrent_versions)
        elif self.strategy == 'merge':
            return self._resolve_merge(concurrent_versions)
        elif self.strategy == 'crdt':
            return self._resolve_crdt(concurrent_versions)
        else:
            raise ValueError(f&quot;Unknown strategy: {self.strategy}&quot;)

    def _resolve_lww(self, versions: List[VersionedValue]) -&gt; VersionedValue:
        &quot;&quot;&quot;
        Last-Write-Wins resolution.

        Primary: Use physical timestamp
        Tiebreaker: Use node_id hash (deterministic)

        WARNING: This loses data! Use only when acceptable.
        &quot;&quot;&quot;
        def lww_key(v: VersionedValue) -&gt; tuple:
            # Physical timestamp, then node_id hash as tiebreaker
            node_hash = hashlib.md5(v.node_id.encode()).hexdigest()
            return (v.physical_timestamp, node_hash)

        winner = max(versions, key=lww_key)

        # Log the losers for audit
        losers = [v for v in versions if v != winner]
        for loser in losers:
            logger.warning(f&quot;LWW: Discarding version from {loser.node_id}: {loser.value}&quot;)

        return winner

    def _resolve_merge(self, versions: List[VersionedValue]) -&gt; VersionedValue:
        &quot;&quot;&quot;
        Custom merge function resolution.

        Examples:
        - Counter: Sum all increments
        - Set: Union of all elements
        - Document: Three-way merge with common ancestor
        &quot;&quot;&quot;
        if not self.merge_fn:
            raise ValueError(&quot;Merge strategy requires merge_fn&quot;)

        # Extract values
        values = [v.value for v in versions]

        # Apply custom merge
        merged_value = self.merge_fn(values)

        # Create new vector clock (merge all clocks, take max per node)
        merged_clock = {}
        for v in versions:
            for node, ts in v.vector_clock.items():
                merged_clock[node] = max(merged_clock.get(node, 0), ts)

        return VersionedValue(
            value=merged_value,
            vector_clock=merged_clock,
            physical_timestamp=datetime.utcnow(),
            node_id='merged'
        )


class GCounter:
    &quot;&quot;&quot;
    Grow-only Counter CRDT.

    State: Map of node_id -&gt; count
    Increment: Increase local node's count
    Value: Sum of all counts
    Merge: Take max of each node's count

    Guaranteed to converge - additions are never lost.
    &quot;&quot;&quot;

    def __init__(self, node_id: str):
        self.node_id = node_id
        self.counts: Dict[str, int] = {}

    def increment(self, amount: int = 1):
        &quot;&quot;&quot;Increment local counter.&quot;&quot;&quot;
        current = self.counts.get(self.node_id, 0)
        self.counts[self.node_id] = current + amount

    def value(self) -&gt; int:
        &quot;&quot;&quot;Get total count across all nodes.&quot;&quot;&quot;
        return sum(self.counts.values())

    def merge(self, other: 'GCounter') -&gt; 'GCounter':
        &quot;&quot;&quot;
        Merge with another counter.

        Commutativity: merge(A, B) = merge(B, A)
        Associativity: merge(merge(A, B), C) = merge(A, merge(B, C))
        Idempotency: merge(A, A) = A

        These properties guarantee eventual consistency.
        &quot;&quot;&quot;
        result = GCounter(self.node_id)
        all_nodes = set(self.counts.keys()) | set(other.counts.keys())

        for node in all_nodes:
            result.counts[node] = max(
                self.counts.get(node, 0),
                other.counts.get(node, 0)
            )

        return result


class LWWRegister:
    &quot;&quot;&quot;
    Last-Writer-Wins Register CRDT.

    State: (value, timestamp, node_id)
    Update: Set new value with current timestamp
    Merge: Keep value with highest (timestamp, node_id)

    Simple but may lose updates. Suitable for settings/preferences.
    &quot;&quot;&quot;

    def __init__(self, node_id: str):
        self.node_id = node_id
        self.value = None
        self.timestamp = 0

    def set(self, value: Any, timestamp: int = None):
        &quot;&quot;&quot;Set register value.&quot;&quot;&quot;
        if timestamp is None:
            timestamp = int(datetime.utcnow().timestamp() * 1000000)
        self.value = value
        self.timestamp = timestamp

    def get(self) -&gt; Any:
        &quot;&quot;&quot;Get current value.&quot;&quot;&quot;
        return self.value

    def merge(self, other: 'LWWRegister') -&gt; 'LWWRegister':
        &quot;&quot;&quot;Merge keeping highest timestamp wins.&quot;&quot;&quot;
        if other.timestamp &gt; self.timestamp:
            result = LWWRegister(self.node_id)
            result.value = other.value
            result.timestamp = other.timestamp
            return result
        elif other.timestamp == self.timestamp:
            # Tiebreaker: lexicographically higher node_id wins
            if other.node_id &gt; self.node_id:
                result = LWWRegister(self.node_id)
                result.value = other.value
                result.timestamp = other.timestamp
                return result
        return self
</code></pre>
<h3 id="multi-master-interview-questions-3-levels-deep">Multi-Master Interview Questions (3 Levels Deep)</h3>
<div style="background: #eff6ff;border-radius: 12px; padding: 24px; margin: 20px 0">
<h4 style="color: #1e40af; margin-top: 0">Level 1: What are the trade-offs between single-primary and multi-primary replication?</h4>
<p style="color: #1e293b; line-height: 1.7"><strong>Answer:</strong> Single-primary guarantees no write conflicts - all writes go to one node, providing strong consistency. Multi-primary allows writes at any node, providing better write availability and lower write latency (local writes), but requires conflict resolution. Single-primary has a write throughput ceiling (one node); multi-primary can theoretically scale writes. Single-primary failover has a brief write outage; multi-primary continues serving writes during partitions. Choose single-primary for transactional workloads; multi-primary for global distribution with eventual consistency tolerance.</p>
<div style="background: white; border-radius: 8px; padding: 20px; margin-top: 16px">
<h5 style="color: #1e40af; margin-top: 0">Level 2: Explain the Last-Write-Wins conflict resolution strategy and its limitations.</h5>
<p style="color: #1e293b; line-height: 1.7"><strong>Answer:</strong> <span style="color: #10b981; font-weight: 600">Last-Write-Wins (LWW)</span> resolves conflicts by comparing timestamps - the write with the highest timestamp wins, others are discarded. Limitations: (1) <strong>Data loss</strong>: Concurrent updates are silently dropped. If User A sets balance=100 and User B sets balance=200 concurrently, one is lost. (2) <strong>Clock synchronization</strong>: Requires synchronized clocks (NTP), but clock skew can cause "earlier" writes to win. (3) <strong>Causality violation</strong>: Doesn't respect happens-before relationships. A reply might be kept while the original message is deleted. (4) <strong>Tiebreaker ambiguity</strong>: Equal timestamps need deterministic tiebreaker (node ID hash). Use cases: Configuration/settings (overwrite is fine), caching (stale data acceptable). Avoid for: Financial transactions, inventory, messaging.</p>
<div style="background: #f8fafc; border-radius: 8px; padding: 16px; margin-top: 12px">
<h6 style="color: #1e40af; margin-top: 0">Level 3: How would you design a shopping cart that works across multiple data centers with multi-master replication?</h6>
<p style="color: #1e293b; line-height: 1.7; font-size: 14px"><strong>Answer:</strong> Use <span style="color: #10b981; font-weight: 600">CRDTs</span> to make the cart conflict-free by design. Model the cart as an <strong>OR-Set (Observed-Remove Set)</strong> where each item addition includes a unique tag. Structure: <code>cart = {(item_id, quantity, add_tag, remove_tags)}</code>. (1) <strong>Add item</strong>: Generate unique tag, add (item, qty, tag, {}) to set. (2) <strong>Remove item</strong>: Don't delete - add all observed add_tags to remove_tags. (3) <strong>Merge</strong>: Union all additions, union all removals. Item is present if any add_tag is not in remove_tags. (4) <strong>Quantity changes</strong>: Model as PN-Counter per item (positive/negative counters). Alternatively, use <strong>operation-based replication</strong>: replicate operations (add_item, remove_item, update_qty) instead of state, apply operations idempotently using operation IDs. Edge cases: (a) Checkout must happen at single DC with inventory check - use [[distributed-locking]](/topic/system-design/distributed-locking) or route checkout to home region. (b) Price changes during cart lifetime - resolve at checkout, not in cart. Companies using this: Amazon (eventual consistency carts), Walmart.</p>
</div>
</div>
</div>
<hr />
<h2 id="section-3-synchronous-vs-asynchronous-replication">Section 3: Synchronous vs Asynchronous Replication</h2>
<h3 id="deep-mechanics-2">Deep Mechanics</h3>
<p>The choice between <span style="color: #10b981; font-weight: 600">synchronous</span> and <span style="color: #10b981; font-weight: 600">asynchronous replication</span> is the fundamental trade-off in database replication design. It determines the balance between data safety (durability) and write performance (latency).</p>
<div style="background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%); border-radius: 16px; padding: 32px; margin: 20px 0">
<div style="text-align: center; color: #1e293b; font-size: 18px; font-weight: 600; margin-bottom: 24px; padding-bottom: 16px">SYNCHRONOUS VS ASYNCHRONOUS REPLICATION</div>
<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 24px">
<div style="background: linear-gradient(135deg, #eff6ff 0%, #dbeafe 100%); border-radius: 12px; padding: 20px">
<div style="color: #1e40af; font-weight: 600; font-size: 16px; margin-bottom: 16px; text-align: center">SYNCHRONOUS</div>
<div style="display: flex; flex-direction: column; gap: 8px; font-size: 13px">
<div style="display: flex; align-items: center; gap: 8px">
<span style="background: #3b82f6; color: white; width: 20px; height: 20px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-size: 11px">1</span>
<span style="color: #1e293b">Client sends write</span>
</div>
<div style="display: flex; align-items: center; gap: 8px">
<span style="background: #3b82f6; color: white; width: 20px; height: 20px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-size: 11px">2</span>
<span style="color: #1e293b">Primary writes locally</span>
</div>
<div style="display: flex; align-items: center; gap: 8px">
<span style="background: #3b82f6; color: white; width: 20px; height: 20px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-size: 11px">3</span>
<span style="color: #1e293b">Primary sends to replica(s)</span>
</div>
<div style="display: flex; align-items: center; gap: 8px">
<span style="background: #f59e0b; color: white; width: 20px; height: 20px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-size: 11px">4</span>
<span style="color: #92400e; font-weight: 600">WAIT for replica ACK</span>
</div>
<div style="display: flex; align-items: center; gap: 8px">
<span style="background: #3b82f6; color: white; width: 20px; height: 20px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-size: 11px">5</span>
<span style="color: #1e293b">Return success to client</span>
</div>
</div>
<div style="margin-top: 16px; background: rgba(34, 197, 94, 0.15); border-radius: 6px; padding: 12px">
<div style="color: #166534; font-size: 12px; font-weight: 600">GUARANTEES:</div>
<div style="color: #166534; font-size: 12px">Zero data loss on primary failure</div>
<div style="color: #166534; font-size: 12px">Strong consistency</div>
</div>
<div style="margin-top: 8px; background: rgba(239, 68, 68, 0.15); border-radius: 6px; padding: 12px">
<div style="color: #991b1b; font-size: 12px; font-weight: 600">COSTS:</div>
<div style="color: #991b1b; font-size: 12px">+2x network RTT per write</div>
<div style="color: #991b1b; font-size: 12px">Replica failure blocks writes</div>
</div>
</div>
<div style="background: linear-gradient(135deg, #fef3c7 0%, #fde68a 100%); border-radius: 12px; padding: 20px">
<div style="color: #92400e; font-weight: 600; font-size: 16px; margin-bottom: 16px; text-align: center">ASYNCHRONOUS</div>
<div style="display: flex; flex-direction: column; gap: 8px; font-size: 13px">
<div style="display: flex; align-items: center; gap: 8px">
<span style="background: #f59e0b; color: white; width: 20px; height: 20px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-size: 11px">1</span>
<span style="color: #1e293b">Client sends write</span>
</div>
<div style="display: flex; align-items: center; gap: 8px">
<span style="background: #f59e0b; color: white; width: 20px; height: 20px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-size: 11px">2</span>
<span style="color: #1e293b">Primary writes locally</span>
</div>
<div style="display: flex; align-items: center; gap: 8px">
<span style="background: #22c55e; color: white; width: 20px; height: 20px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-size: 11px">3</span>
<span style="color: #166534; font-weight: 600">Return success IMMEDIATELY</span>
</div>
<div style="display: flex; align-items: center; gap: 8px">
<span style="background: #94a3b8; color: white; width: 20px; height: 20px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-size: 11px">4</span>
<span style="color: #64748b">Send to replica (background)</span>
</div>
<div style="display: flex; align-items: center; gap: 8px">
<span style="background: #94a3b8; color: white; width: 20px; height: 20px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-size: 11px">5</span>
<span style="color: #64748b">Replica applies eventually</span>
</div>
</div>
<div style="margin-top: 16px; background: rgba(34, 197, 94, 0.15); border-radius: 6px; padding: 12px">
<div style="color: #166534; font-size: 12px; font-weight: 600">GUARANTEES:</div>
<div style="color: #166534; font-size: 12px">Low write latency</div>
<div style="color: #166534; font-size: 12px">Replica failure doesn't block</div>
</div>
<div style="margin-top: 8px; background: rgba(239, 68, 68, 0.15); border-radius: 6px; padding: 12px">
<div style="color: #991b1b; font-size: 12px; font-weight: 600">COSTS:</div>
<div style="color: #991b1b; font-size: 12px">Data loss window (replication lag)</div>
<div style="color: #991b1b; font-size: 12px">Eventual consistency only</div>
</div>
</div>
</div>
</div>
<h3 id="semi-synchronous-replication">Semi-Synchronous Replication</h3>
<p><span style="color: #10b981; font-weight: 600">Semi-synchronous replication</span> is a hybrid approach that balances durability and performance. The primary waits for acknowledgment from at least one replica before returning success, but not all replicas.</p>
<div style="background: linear-gradient(135deg, #f0fdf4 0%, #dcfce7 100%); border-radius: 12px; padding: 24px; margin: 20px 0">
<h4 style="color: #166534; margin-top: 0">Semi-Synchronous (MySQL rpl_semi_sync)</h4>
<div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 16px; margin-top: 16px">
<div style="background: white; padding: 16px; border-radius: 8px; text-align: center">
<div style="color: #1e293b; font-weight: 600">AFTER_SYNC</div>
<div style="color: #475569; font-size: 13px; margin-top: 8px">Wait for replica ACK before storage engine commit. Guarantees replica has transaction on failover.</div>
</div>
<div style="background: white; padding: 16px; border-radius: 8px; text-align: center">
<div style="color: #1e293b; font-weight: 600">AFTER_COMMIT</div>
<div style="color: #475569; font-size: 13px; margin-top: 8px">Wait for replica ACK after commit. Client sees success before replication. Window for data loss.</div>
</div>
<div style="background: white; padding: 16px; border-radius: 8px; text-align: center">
<div style="color: #1e293b; font-weight: 600">Fallback</div>
<div style="color: #475569; font-size: 13px; margin-top: 8px">If no replica ACKs within timeout, fall back to async. Prevents write stall but loses guarantee.</div>
</div>
</div>
</div>
<h3 id="replication-mode-implementation">Replication Mode Implementation</h3>
<pre><code class="language-python">from enum import Enum
from typing import List, Optional
import asyncio
import time


class ReplicationMode(Enum):
    ASYNC = &quot;async&quot;
    SEMI_SYNC = &quot;semi_sync&quot;
    SYNC = &quot;sync&quot;


class ReplicationConfig:
    &quot;&quot;&quot;
    Replication configuration with tunable consistency.

    Key Parameters:
    - mode: async, semi_sync, or sync
    - min_acks: Minimum replica acknowledgments required (for semi-sync)
    - ack_timeout_ms: How long to wait for acks before fallback
    - sync_binlog: Whether to fsync binlog on each commit
    &quot;&quot;&quot;

    def __init__(
        self,
        mode: ReplicationMode = ReplicationMode.ASYNC,
        min_acks: int = 1,
        ack_timeout_ms: int = 1000,
        sync_binlog: bool = True,
        sync_relay_log: bool = True
    ):
        self.mode = mode
        self.min_acks = min_acks
        self.ack_timeout_ms = ack_timeout_ms
        self.sync_binlog = sync_binlog
        self.sync_relay_log = sync_relay_log


class ReplicationCoordinator:
    &quot;&quot;&quot;
    Coordinates replication across multiple replicas.

    Responsibilities:
    - Send transactions to replicas
    - Collect acknowledgments
    - Enforce replication mode guarantees
    - Handle replica failures gracefully
    &quot;&quot;&quot;

    def __init__(self, config: ReplicationConfig, replicas: List['ReplicaConnection']):
        self.config = config
        self.replicas = replicas
        self.ack_waiters: Dict[int, asyncio.Event] = {}  # txn_id -&gt; event
        self.ack_counts: Dict[int, int] = {}  # txn_id -&gt; count

    async def replicate_transaction(
        self,
        txn_id: int,
        binlog_events: List[bytes]
    ) -&gt; ReplicationResult:
        &quot;&quot;&quot;
        Replicate transaction according to configured mode.

        Returns when replication guarantees are satisfied.
        &quot;&quot;&quot;
        start_time = time.monotonic()

        # Initialize ack tracking
        self.ack_counts[txn_id] = 0
        self.ack_waiters[txn_id] = asyncio.Event()

        try:
            if self.config.mode == ReplicationMode.ASYNC:
                return await self._replicate_async(txn_id, binlog_events)
            elif self.config.mode == ReplicationMode.SEMI_SYNC:
                return await self._replicate_semi_sync(txn_id, binlog_events)
            else:  # SYNC
                return await self._replicate_sync(txn_id, binlog_events)
        finally:
            # Cleanup
            elapsed_ms = (time.monotonic() - start_time) * 1000
            metrics.record_replication_latency(self.config.mode, elapsed_ms)
            del self.ack_waiters[txn_id]
            del self.ack_counts[txn_id]

    async def _replicate_async(
        self,
        txn_id: int,
        binlog_events: List[bytes]
    ) -&gt; ReplicationResult:
        &quot;&quot;&quot;
        Async replication: Fire and forget.

        Don't wait for any acks. Best performance, weakest durability.
        &quot;&quot;&quot;
        # Send to all replicas without waiting
        for replica in self.replicas:
            asyncio.create_task(self._send_to_replica(replica, txn_id, binlog_events))

        return ReplicationResult(
            success=True,
            acks_received=0,
            mode=ReplicationMode.ASYNC
        )

    async def _replicate_semi_sync(
        self,
        txn_id: int,
        binlog_events: List[bytes]
    ) -&gt; ReplicationResult:
        &quot;&quot;&quot;
        Semi-sync replication: Wait for min_acks or timeout.

        Provides durability guarantee if at least one replica acks.
        Falls back to async on timeout (configurable behavior).
        &quot;&quot;&quot;
        # Send to all replicas
        send_tasks = [
            asyncio.create_task(self._send_to_replica(replica, txn_id, binlog_events))
            for replica in self.replicas
        ]

        # Wait for minimum acks or timeout
        try:
            await asyncio.wait_for(
                self._wait_for_acks(txn_id, self.config.min_acks),
                timeout=self.config.ack_timeout_ms / 1000
            )

            return ReplicationResult(
                success=True,
                acks_received=self.ack_counts[txn_id],
                mode=ReplicationMode.SEMI_SYNC
            )

        except asyncio.TimeoutError:
            # Fallback to async
            logger.warning(f&quot;Semi-sync timeout for txn {txn_id}, falling back to async&quot;)
            metrics.increment(&quot;replication.semi_sync.fallback&quot;)

            return ReplicationResult(
                success=True,
                acks_received=self.ack_counts[txn_id],
                mode=ReplicationMode.ASYNC,
                fallback=True
            )

    async def _replicate_sync(
        self,
        txn_id: int,
        binlog_events: List[bytes]
    ) -&gt; ReplicationResult:
        &quot;&quot;&quot;
        Sync replication: Wait for ALL replicas.

        Strongest durability but highest latency.
        Write fails if any replica is unavailable.

        WARNING: Not recommended for production - one slow replica
        blocks all writes. Use semi-sync or quorum-based instead.
        &quot;&quot;&quot;
        # Send to all replicas
        send_tasks = [
            asyncio.create_task(self._send_to_replica(replica, txn_id, binlog_events))
            for replica in self.replicas
        ]

        # Wait for ALL acks
        try:
            await asyncio.wait_for(
                self._wait_for_acks(txn_id, len(self.replicas)),
                timeout=self.config.ack_timeout_ms / 1000
            )

            return ReplicationResult(
                success=True,
                acks_received=len(self.replicas),
                mode=ReplicationMode.SYNC
            )

        except asyncio.TimeoutError:
            # Sync mode doesn't fall back - fail the transaction
            logger.error(f&quot;Sync replication failed for txn {txn_id}&quot;)
            return ReplicationResult(
                success=False,
                acks_received=self.ack_counts[txn_id],
                mode=ReplicationMode.SYNC,
                error=&quot;Not all replicas acknowledged&quot;
            )

    async def _send_to_replica(
        self,
        replica: 'ReplicaConnection',
        txn_id: int,
        binlog_events: List[bytes]
    ):
        &quot;&quot;&quot;Send transaction to a single replica and handle ack.&quot;&quot;&quot;
        try:
            ack = await replica.send_binlog_events(binlog_events)
            if ack.success:
                self.ack_counts[txn_id] += 1
                if self.ack_counts[txn_id] &gt;= self.config.min_acks:
                    self.ack_waiters[txn_id].set()

        except ConnectionError as e:
            logger.warning(f&quot;Failed to replicate to {replica.host}: {e}&quot;)
            metrics.increment(&quot;replication.send.failure&quot;)

    async def _wait_for_acks(self, txn_id: int, required_acks: int):
        &quot;&quot;&quot;Wait until required number of acks received.&quot;&quot;&quot;
        while self.ack_counts[txn_id] &lt; required_acks:
            await self.ack_waiters[txn_id].wait()
            self.ack_waiters[txn_id].clear()
</code></pre>
<h3 id="sync-vs-async-interview-questions-3-levels-deep">Sync vs Async Interview Questions (3 Levels Deep)</h3>
<div style="background: #eff6ff;border-radius: 12px; padding: 24px; margin: 20px 0">
<h4 style="color: #1e40af; margin-top: 0">Level 1: What is the difference between synchronous and asynchronous replication?</h4>
<p style="color: #1e293b; line-height: 1.7"><strong>Answer:</strong> In <span style="color: #10b981; font-weight: 600">synchronous replication</span>, the primary waits for replicas to confirm they've received and persisted the data before acknowledging the write to the client. This guarantees no data loss on primary failure but adds latency. In <span style="color: #10b981; font-weight: 600">asynchronous replication</span>, the primary acknowledges writes immediately after local commit, replicating in the background. This provides low latency but creates a window where committed data could be lost if the primary fails before replication completes.</p>
<div style="background: white; border-radius: 8px; padding: 20px; margin-top: 16px">
<h5 style="color: #1e40af; margin-top: 0">Level 2: How does semi-synchronous replication work and when should you use it?</h5>
<p style="color: #1e293b; line-height: 1.7"><strong>Answer:</strong> <span style="color: #10b981; font-weight: 600">Semi-synchronous replication</span> is a hybrid: the primary waits for at least one replica to acknowledge before returning success to the client. If no replica responds within a timeout, it can fall back to async mode (configurable). Use semi-sync when: (1) You need durability guarantees stronger than async but can't tolerate sync's latency. (2) You have multiple replicas and one slow replica shouldn't block all writes. (3) You're willing to accept degraded mode (async fallback) during replica failures. MySQL's semi-sync has two variants: <code>AFTER_SYNC</code> (wait before engine commit) and <code>AFTER_COMMIT</code> (wait after engine commit). AFTER_SYNC is safer as clients don't see committed data before replication. Trade-off: during fallback, you lose the durability guarantee. Monitor fallback rate and alert.</p>
<div style="background: #f8fafc; border-radius: 8px; padding: 16px; margin-top: 12px">
<h6 style="color: #1e40af; margin-top: 0">Level 3: You're designing a financial system that needs strong durability but also 10ms write latency with replicas in different regions (100ms RTT). How do you architect this?</h6>
<p style="color: #1e293b; line-height: 1.7; font-size: 14px"><strong>Answer:</strong> You can't have both 10ms latency AND cross-region synchronous replication (100ms RTT makes this impossible). Solution architecture: (1) <strong>Local synchronous replication</strong>: Deploy 3 replicas in the same region/availability zone (1-2ms RTT). Use synchronous replication within region - writes complete in ~15ms with 2x local RTT. (2) <strong>Cross-region async replication</strong>: Replicate asynchronously to disaster recovery region. Accept data loss window equal to replication lag (~100-500ms). (3) <strong>Quorum writes</strong>: Use [[consensus-algorithms]](/topic/system-design/consensus-algorithms) (Raft/Paxos) with 3 local nodes. Write succeeds when 2/3 acknowledge (majority quorum). Survives one node failure without data loss. (4) <strong>Commit semantics</strong>: Client can request <code>COMMIT_SYNC</code> (wait for local quorum) or <code>COMMIT_DURABLE</code> (wait for cross-region, higher latency). (5) <strong>Failure handling</strong>: Local region failure fails over to DR region - accept potential data loss for transactions in flight. Implement idempotency keys and reconciliation for critical transactions. Examples: Spanner uses Paxos per shard; CockroachDB uses Raft per range. See [[cap-theorem]](/topic/system-design/cap-theorem) for theoretical constraints.</p>
</div>
</div>
</div>
<hr />
<h2 id="section-4-replication-lag">Section 4: Replication Lag</h2>
<h3 id="deep-mechanics-3">Deep Mechanics</h3>
<p><span style="color: #10b981; font-weight: 600">Replication lag</span> is the delay between when a transaction commits on the primary and when it becomes visible on a replica. This lag is inherent in asynchronous replication and creates consistency challenges.</p>
<div style="background: linear-gradient(135deg, #fef2f2 0%, #fee2e2 100%); border-radius: 12px; padding: 24px; margin: 20px 0">
<h4 style="color: #991b1b; margin-top: 0">The Replication Lag Problem</h4>
<div style="background: white; border-radius: 8px; padding: 16px; margin-bottom: 16px">
<div style="color: #1e293b; font-weight: 600; margin-bottom: 12px">Scenario: User Updates Profile</div>
<div style="font-family: monospace; font-size: 13px; color: #475569; line-height: 1.8">
<div>T=0ms: User writes "name=Alice" to PRIMARY</div>
<div>T=1ms: PRIMARY confirms "Success!" to user</div>
<div>T=2ms: User reads from REPLICA (load balanced)</div>
<div>T=3ms: REPLICA still shows "name=Bob" (replication lag!)</div>
<div style="color: #ef4444; font-weight: 600; margin-top: 8px">Result: User sees old data immediately after update - confusion!</div>
</div>
</div>
<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 16px">
<div style="background: white; border-radius: 8px; padding: 16px">
<div style="color: #991b1b; font-weight: 600; margin-bottom: 8px">Causes of Lag</div>
<ul style="color: #475569; margin: 0; padding-left: 20px; font-size: 13px">
<li>Network latency (especially cross-region)</li>
<li>Replica under heavy read load</li>
<li>Large transactions (long to apply)</li>
<li>DDL operations blocking replication</li>
<li>Single-threaded SQL apply (MySQL)</li>
</ul>
</div>
<div style="background: white; border-radius: 8px; padding: 16px">
<div style="color: #166534; font-weight: 600; margin-bottom: 8px">Monitoring Metrics</div>
<ul style="color: #475569; margin: 0; padding-left: 20px; font-size: 13px">
<li><code>Seconds_Behind_Master</code> (MySQL)</li>
<li><code>pg_stat_replication.replay_lag</code> (PostgreSQL)</li>
<li>Binlog position delta</li>
<li>GTID gap (MySQL 5.6+)</li>
<li>Write timestamp vs replica timestamp</li>
</ul>
</div>
</div>
</div>
<h3 id="lag-mitigation-strategies">Lag Mitigation Strategies</h3>
<div style="background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%); border-radius: 16px; padding: 32px; margin: 20px 0">
<div style="text-align: center; color: #1e293b; font-size: 18px; font-weight: 600; margin-bottom: 24px; padding-bottom: 16px">REPLICATION LAG MITIGATION STRATEGIES</div>
<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px">
<div style="background: linear-gradient(135deg, #eff6ff 0%, #dbeafe 100%); border-radius: 12px; padding: 20px">
<div style="color: #1e40af; font-weight: 600; margin-bottom: 12px">1. Read-Your-Writes Routing</div>
<div style="color: #475569; font-size: 14px">Track recent write timestamps per session. Route reads to primary for N seconds after write, then allow replica reads.</div>
<div style="background: white; padding: 12px; border-radius: 6px; margin-top: 12px; font-family: monospace; font-size: 12px; color: #1e293b">
  if (now - last_write) &lt; 5s:<br>
  &nbsp;&nbsp;return read_from_primary()<br>
  else:<br>
  &nbsp;&nbsp;return read_from_replica()
</div>
</div>
<div style="background: linear-gradient(135deg, #f0fdf4 0%, #dcfce7 100%); border-radius: 12px; padding: 20px">
<div style="color: #166534; font-weight: 600; margin-bottom: 12px">2. Causal Consistency Tokens</div>
<div style="color: #475569; font-size: 14px">Return write position (LSN/GTID) with write response. Client includes token with subsequent reads. Replica waits until caught up.</div>
<div style="background: white; padding: 12px; border-radius: 6px; margin-top: 12px; font-family: monospace; font-size: 12px; color: #1e293b">
  write_response.token = gtid<br>
  read(token) => wait_for(replica.gtid >= token)
</div>
</div>
<div style="background: linear-gradient(135deg, #fef3c7 0%, #fde68a 100%); border-radius: 12px; padding: 20px">
<div style="color: #92400e; font-weight: 600; margin-bottom: 12px">3. Lag-Aware Load Balancing</div>
<div style="color: #475569; font-size: 14px">Monitor each replica's lag. Exclude replicas exceeding threshold. Route to lowest-lag replica for time-sensitive reads.</div>
<div style="background: white; padding: 12px; border-radius: 6px; margin-top: 12px; font-family: monospace; font-size: 12px; color: #1e293b">
  replicas = filter(r.lag &lt; 1s)<br>
  return min(replicas, key=lag)
</div>
</div>
<div style="background: linear-gradient(135deg, #fce7f3 0%, #fbcfe8 100%); border-radius: 12px; padding: 20px">
<div style="color: #9d174d; font-weight: 600; margin-bottom: 12px">4. Parallel Replication</div>
<div style="color: #475569; font-size: 14px">Apply transactions in parallel on replica. MySQL 5.7+ supports parallel apply per schema. MySQL 8.0+ supports writeset-based parallelism.</div>
<div style="background: white; padding: 12px; border-radius: 6px; margin-top: 12px; font-family: monospace; font-size: 12px; color: #1e293b">
  slave_parallel_workers = 16<br>
  slave_parallel_type = LOGICAL_CLOCK
</div>
</div>
</div>
</div>
<h3 id="replication-lag-implementation">Replication Lag Implementation</h3>
<pre><code>        ```python
        from dataclasses import dataclass
        from typing import Dict, List, Optional
        import time
        import asyncio


        @dataclass
        class ReplicaState:
        host: str
        port: int
        lag_seconds: float
        lag_bytes: int
        gtid_executed: str
        last_heartbeat: float
        is_healthy: bool


        class ReplicationLagMonitor:
        &quot;&quot;&quot;
        Monitor and track replication lag across replicas.

        Metrics tracked:
        - Seconds behind: Time difference between primary commit and replica apply
        - Bytes behind: Binlog position difference (more accurate for bursts)
        - GTID gap: Missing transaction IDs (MySQL 5.6+)

        Why multiple metrics:
        - Seconds_Behind_Master can be misleading during large transactions
        - Bytes behind shows actual replication backlog
        - GTID gap shows exact missing transactions
        &quot;&quot;&quot;

        def __init__(
        self,
        replicas: List['ReplicaConnection'],
        lag_threshold_seconds: float = 5.0,
        check_interval_seconds: float = 1.0
        ):
        self.replicas = replicas
        self.lag_threshold = lag_threshold_seconds
        self.check_interval = check_interval_seconds
        self.replica_states: Dict[str, ReplicaState] = {}

        async def start_monitoring(self):
        &quot;&quot;&quot;Start background lag monitoring.&quot;&quot;&quot;
        while True:
        await self._check_all_replicas()
        await asyncio.sleep(self.check_interval)

        async def _check_all_replicas(self):
        &quot;&quot;&quot;Check lag on all replicas concurrently.&quot;&quot;&quot;
        tasks = [
        self._check_replica(replica)
        for replica in self.replicas
        ]
        await asyncio.gather(*tasks, return_exceptions=True)

        async def _check_replica(self, replica: 'ReplicaConnection'):
        &quot;&quot;&quot;
        Check replication lag for a single replica.

        MySQL query:
        SHOW SLAVE STATUS returns:
        - Seconds_Behind_Master: Estimated lag in seconds
        - Relay_Master_Log_File, Exec_Master_Log_Pos: Current position
        - Retrieved_Gtid_Set, Executed_Gtid_Set: GTID-based tracking
        &quot;&quot;&quot;
        try:
        status = await replica.query(&quot;SHOW SLAVE STATUS&quot;)

        lag_seconds = status.get('Seconds_Behind_Master', 999)

        # Handle NULL (replication not running)
        if lag_seconds is None:
        lag_seconds = 999
        is_healthy = False
        else:
        is_healthy = lag_seconds &lt;= self.lag_threshold

        self.replica_states[replica.host] = ReplicaState(
        host=replica.host,
        port=replica.port,
        lag_seconds=lag_seconds,
        lag_bytes=self._calculate_bytes_behind(status),
        gtid_executed=status.get('Executed_Gtid_Set', ''),
        last_heartbeat=time.time(),
        is_healthy=is_healthy
        )

        # Alert on high lag
        if lag_seconds &gt; self.lag_threshold:
        await self._alert_high_lag(replica.host, lag_seconds)

        except Exception as e:
        logger.error(f&quot;Failed to check replica {replica.host}: {e}&quot;)
        self.replica_states[replica.host] = ReplicaState(
        host=replica.host,
        port=replica.port,
        lag_seconds=999,
        lag_bytes=-1,
        gtid_executed='',
        last_heartbeat=time.time(),
        is_healthy=False
        )

        def get_healthy_replicas(self) -&gt; List[ReplicaState]:
        &quot;&quot;&quot;Return list of replicas with acceptable lag.&quot;&quot;&quot;
        return [
        state for state in self.replica_states.values()
        if state.is_healthy
        ]

        def get_lowest_lag_replica(self) -&gt; Optional[ReplicaState]:
        &quot;&quot;&quot;Return replica with lowest lag.&quot;&quot;&quot;
        healthy = self.get_healthy_replicas()
        if not healthy:
        return None
        return min(healthy, key=lambda r: r.lag_seconds)


        class CausalConsistencyRouter:
        &quot;&quot;&quot;
        Router that enforces causal consistency using position tokens.

        How it works:
        1. Write returns position token (GTID or binlog position)
        2. Client includes token in subsequent read
        3. Router finds replica that has reached that position
        4. If no replica caught up, either wait or route to primary
        &quot;&quot;&quot;

        def __init__(
        self,
        primary: 'DatabaseConnection',
        replicas: List['DatabaseConnection'],
        wait_timeout_ms: int = 100
        ):
        self.primary = primary
        self.replicas = replicas
        self.wait_timeout_ms = wait_timeout_ms

        async def write(self, query: str) -&gt; WriteResult:
        &quot;&quot;&quot;
        Execute write and return position token.

        Token format (MySQL GTID):
        &quot;source_uuid:transaction_id&quot;
        Example: &quot;3E11FA47-71CA-11E1-9E33-C80AA9429562:23&quot;
        &quot;&quot;&quot;
        result = await self.primary.execute(query)

        # Get GTID of committed transaction
        gtid_result = await self.primary.query(&quot;SELECT @@global.gtid_executed&quot;)
        gtid = gtid_result['@@global.gtid_executed']

        return WriteResult(
        success=True,
        rows_affected=result.rows_affected,
        position_token=gtid
        )

        async def read(
        self,
        query: str,
        position_token: Optional[str] = None,
        consistency: str = 'eventual'
        ) -&gt; ReadResult:
        &quot;&quot;&quot;
        Execute read with optional consistency guarantee.

        Consistency levels:
        - 'eventual': Read from any replica (fastest)
        - 'causal': Read from replica that has seen position_token
        - 'strong': Always read from primary
        &quot;&quot;&quot;
        if consistency == 'strong':
        return await self._read_from_primary(query)

        if consistency == 'causal' and position_token:
        return await self._read_causal(query, position_token)

        # Eventual consistency - any replica
        return await self._read_from_replica(query)

        async def _read_causal(self, query: str, position_token: str) -&gt; ReadResult:
        &quot;&quot;&quot;
        Read with causal consistency guarantee.

        Strategy:
        1. Check each replica's executed GTID set
        2. If replica has executed the token's GTID, read from it
        3. If none caught up, wait briefly then retry
        4. Fall back to primary if timeout exceeded
        &quot;&quot;&quot;
        start_time = time.monotonic()

        while True:
        for replica in self.replicas:
        if await self._replica_has_position(replica, position_token):
        return await replica.query(query)

        # Check timeout
        elapsed_ms = (time.monotonic() - start_time) * 1000
        if elapsed_ms &gt; self.wait_timeout_ms:
        # Fall back to primary
        logger.debug(f&quot;Causal read falling back to primary after {elapsed_ms}ms&quot;)
        metrics.increment(&quot;causal_read.primary_fallback&quot;)
        return await self._read_from_primary(query)

        # Wait and retry
        await asyncio.sleep(0.005)  # 5ms

        async def _replica_has_position(
        self,
        replica: 'DatabaseConnection',
        position_token: str
        ) -&gt; bool:
        &quot;&quot;&quot;
        Check if replica has executed up to given GTID.

        MySQL function: WAIT_FOR_EXECUTED_GTID_SET(gtid_set, timeout)
        Returns 0 if caught up, 1 if timeout, -1 if error
        &quot;&quot;&quot;
        try:
        # Quick check without waiting
        result = await replica.query(
        f&quot;SELECT WAIT_FOR_EXECUTED_GTID_SET('{position_token}', 0)&quot;
        )
        return result == 0
        except Exception:
        return False
        ```
</code></pre>
<h3 id="replication-lag-interview-questions-3-levels-deep">Replication Lag Interview Questions (3 Levels Deep)</h3>
<div style="background: #eff6ff;border-radius: 12px; padding: 24px; margin: 20px 0">
<h4 style="color: #1e40af; margin-top: 0">Level 1: What is replication lag and why is it problematic?</h4>
<p style="color: #1e293b; line-height: 1.7"><strong>Answer:</strong> <span style="color: #10b981; font-weight: 600">Replication lag</span> is the delay between when a transaction commits on the primary and when it's applied on a replica. It's problematic because it causes stale reads: a user who just updated their profile might see old data when their read is load-balanced to a lagging replica. In write-heavy systems, lag can grow during peak traffic, causing reads to return data that's seconds or minutes old. During failover, lag determines potential data loss - if the promoted replica was 30 seconds behind, those 30 seconds of transactions may be lost.</p>
<div style="background: white; border-radius: 8px; padding: 20px; margin-top: 16px">
<h5 style="color: #1e40af; margin-top: 0">Level 2: How do you implement "read-your-writes" consistency in a replicated database?</h5>
<p style="color: #1e293b; line-height: 1.7"><strong>Answer:</strong> Multiple approaches with different trade-offs: (1) <strong>Sticky sessions to primary</strong>: After a write, route that user's reads to primary for N seconds. Simple but increases primary load. (2) <strong>Position-based routing</strong>: Write returns a position token (GTID/LSN). Subsequent reads include this token; route to replicas that have reached that position. More complex but precise. (3) <strong>Synchronous commit</strong>: Wait for replica acknowledgment before returning success. Guarantees replica has data but adds latency. (4) <strong>Application-level caching</strong>: Cache written data locally, merge with replica reads. Works for simple cases but complex for related entities. (5) <strong>Monotonic reads</strong>: Track highest read position per session, only route to replicas at or past that position. Best practice: combine approach 2 (position tokens) with fallback to primary. See [[caching]](/topic/system-design/caching) for cache patterns.</p>
<div style="background: #f8fafc; border-radius: 8px; padding: 16px; margin-top: 12px">
<h6 style="color: #1e40af; margin-top: 0">Level 3: Your MySQL replica is consistently 10+ seconds behind during peak hours, causing user complaints. Walk through your debugging and remediation process.</h6>
<p style="color: #1e293b; line-height: 1.7; font-size: 14px"><strong>Answer:</strong> Systematic debugging approach: (1) <strong>Identify bottleneck type</strong>: Check <code>SHOW SLAVE STATUS</code> - is <code>Seconds_Behind_Master</code> increasing (falling behind) or stable (steady-state lag)? Check <code>Slave_IO_Running</code> and <code>Slave_SQL_Running</code>. (2) <strong>IO thread vs SQL thread</strong>: If <code>Master_Log_File</code> matches <code>Relay_Master_Log_File</code>, IO thread is keeping up - bottleneck is SQL apply. If they differ, network or primary is the bottleneck. (3) <strong>SQL thread analysis</strong>: Run <code>SHOW PROCESSLIST</code> on replica. Is SQL thread stuck on one query? Large transactions (ALTER TABLE, bulk INSERT) block replication. Check for lock waits from replica reads. (4) <strong>Remediation for slow apply</strong>: Enable parallel replication (<code>slave_parallel_workers=16</code>, <code>slave_parallel_type=LOGICAL_CLOCK</code>). Move replica to SSD for faster disk writes. Reduce replica read load. (5) <strong>Remediation for large transactions</strong>: Break bulk operations into smaller batches on primary. Use <code>pt-online-schema-change</code> for DDL. (6) <strong>Remediation for network</strong>: Increase <code>slave_net_timeout</code>. Use compression (<code>slave_compressed_protocol=1</code>). Place replica in same datacenter. (7) <strong>Long-term</strong>: If single replica can't keep up, add more replicas to distribute read load. Consider [[database-sharding]](/topic/system-design/database-sharding) to reduce per-shard write volume. Implement [[circuit-breaker]](/topic/design-patterns/circuit-breaker) to route away from lagging replicas.</p>
</div>
</div>
</div>
<hr />
<h2 id="section-5-failover-strategies">Section 5: Failover Strategies</h2>
<h3 id="deep-mechanics-4">Deep Mechanics</h3>
<p><span style="color: #10b981; font-weight: 600">Failover</span> is the process of promoting a replica to become the new primary when the current primary fails. It's one of the most critical operations in a replicated database system.</p>
<div style="background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%); border-radius: 16px; padding: 32px; margin: 20px 0">
<div style="text-align: center; color: #1e293b; font-size: 18px; font-weight: 600; margin-bottom: 24px; padding-bottom: 16px">AUTOMATIC FAILOVER PROCESS</div>
<div style="display: flex; flex-direction: column; gap: 24px">
<div style="display: flex; gap: 24px; align-items: center; flex-wrap: wrap">
<div style="background: linear-gradient(135deg, #dcfce7 0%, #bbf7d0 100%); border-radius: 12px; padding: 20px; min-width: 150px; text-align: center">
<div style="color: #166534; font-weight: 600">PRIMARY</div>
<div style="color: #22c55e; font-size: 12px">(Active)</div>
</div>
<div style="display: flex; flex-direction: column; gap: 8px">
<div style="display: flex; align-items: center; gap: 12px">
<span style="color: #6366f1; font-size: 16px">→</span>
<div style="background: linear-gradient(135deg, #f1f5f9 0%, #e2e8f0 100%); border-radius: 10px; padding: 14px; min-width: 120px; text-align: center">
<div style="color: #475569; font-weight: 500">Replica 1</div>
<div style="color: #22c55e; font-size: 11px">lag: 0.1s</div>
</div>
</div>
<div style="display: flex; align-items: center; gap: 12px">
<span style="color: #6366f1; font-size: 16px">→</span>
<div style="background: linear-gradient(135deg, #f1f5f9 0%, #e2e8f0 100%); border-radius: 10px; padding: 14px; min-width: 120px; text-align: center">
<div style="color: #475569; font-weight: 500">Replica 2</div>
<div style="color: #f59e0b; font-size: 11px">lag: 2.5s</div>
</div>
</div>
</div>
<div style="color: #64748b; font-size: 14px; margin-left: auto">BEFORE FAILURE</div>
</div>
<div style="display: flex; align-items: center; gap: 16px; flex-wrap: wrap">
<div style="background: linear-gradient(135deg, #fef2f2 0%, #fee2e2 100%); border-radius: 12px; padding: 20px; min-width: 150px; text-align: center">
<div style="color: #991b1b; font-weight: 600">PRIMARY</div>
<div style="color: #ef4444; font-size: 12px">FAILED!</div>
</div>
<div style="flex: 1; background: linear-gradient(135deg, #dbeafe 0%, #bfdbfe 100%); border-radius: 12px; padding: 16px">
<div style="color: #1e40af; font-weight: 600; margin-bottom: 8px">Failover Steps:</div>
<div style="color: #1e293b; font-size: 13px; line-height: 1.8">
<div>1. Detect failure (heartbeat timeout 10-30s)</div>
<div>2. Fence old primary (STONITH)</div>
<div>3. Select best replica (lowest lag)</div>
<div>4. Wait for replica to apply remaining relay log</div>
<div>5. Promote replica to primary (read-write)</div>
<div>6. Reconfigure other replicas to follow new primary</div>
<div>7. Update DNS/VIP/proxy routing</div>
</div>
</div>
</div>
<div style="display: flex; gap: 24px; align-items: center; flex-wrap: wrap">
<div style="background: linear-gradient(135deg, #f1f5f9 0%, #e2e8f0 100%); border-radius: 12px; padding: 20px; min-width: 150px; text-align: center;opacity: 0.6">
<div style="color: #475569; font-weight: 600; text-decoration: line-through">OLD PRIMARY</div>
<div style="color: #94a3b8; font-size: 12px">(Fenced)</div>
</div>
<div style="display: flex; flex-direction: column; gap: 8px">
<div style="display: flex; align-items: center; gap: 12px">
<div style="background: linear-gradient(135deg, #dcfce7 0%, #bbf7d0 100%); border-radius: 10px; padding: 14px; min-width: 130px; text-align: center">
<div style="color: #166534; font-weight: 600">NEW PRIMARY</div>
<div style="color: #22c55e; font-size: 11px">(Promoted Replica 1)</div>
</div>
<span style="color: #6366f1; font-size: 16px">→</span>
<div style="background: linear-gradient(135deg, #f1f5f9 0%, #e2e8f0 100%); border-radius: 10px; padding: 14px; min-width: 120px; text-align: center">
<div style="color: #475569; font-weight: 500">Replica 2</div>
<div style="color: #64748b; font-size: 11px">(Reconfigured)</div>
</div>
</div>
</div>
<div style="color: #64748b; font-size: 14px; margin-left: auto">AFTER FAILOVER</div>
</div>
</div>
</div>
<h3 id="split-brain-prevention">Split-Brain Prevention</h3>
<div style="background: #fef2f2;border-radius: 12px; padding: 24px; margin: 20px 0">
<h4 style="color: #991b1b; margin-top: 0">Split-Brain: The Worst-Case Scenario</h4>
<p style="color: #7f1d1d; margin-bottom: 16px"><span style="color: #10b981; font-weight: 600">Split-brain</span> occurs when network partition causes two nodes to both believe they are the primary, accepting writes independently. This causes data divergence that's extremely difficult to reconcile.</p>
<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 16px">
<div style="background: white; padding: 16px; border-radius: 8px">
<div style="color: #991b1b; font-weight: 600; margin-bottom: 8px">Prevention Techniques</div>
<ul style="color: #475569; margin: 0; padding-left: 20px; font-size: 13px">
<li><strong>STONITH</strong>: Shoot The Other Node In The Head - forcibly power off old primary</li>
<li><strong>Quorum</strong>: Require majority agreement before promotion</li>
<li><strong>Fencing</strong>: Block old primary's access to shared storage</li>
<li><strong>Lease-based leadership</strong>: Primary must renew lease periodically</li>
</ul>
</div>
<div style="background: white; padding: 16px; border-radius: 8px">
<div style="color: #166534; font-weight: 600; margin-bottom: 8px">Detection & Recovery</div>
<ul style="color: #475569; margin: 0; padding-left: 20px; font-size: 13px">
<li><strong>Epoch numbers</strong>: Reject writes from lower epoch</li>
<li><strong>Global transaction IDs</strong>: Detect divergent transactions</li>
<li><strong>Conflict detection</strong>: Compare binlogs after network heals</li>
<li><strong>Manual reconciliation</strong>: DBA decides which transactions to keep</li>
</ul>
</div>
</div>
</div>
<h3 id="failover-implementation">Failover Implementation</h3>
<pre><code>        ```python
        from enum import Enum
        from typing import List, Optional
        import asyncio


        class FailoverState(Enum):
        NORMAL = &quot;normal&quot;
        DETECTING = &quot;detecting&quot;
        FENCING = &quot;fencing&quot;
        PROMOTING = &quot;promoting&quot;
        RECONFIGURING = &quot;reconfiguring&quot;
        COMPLETED = &quot;completed&quot;
        FAILED = &quot;failed&quot;


        class FailoverOrchestrator:
        &quot;&quot;&quot;
        Orchestrates automatic failover for MySQL/PostgreSQL.

        Safety Requirements:
        1. Only one primary at any time (no split-brain)
        2. Minimize data loss (promote most up-to-date replica)
        3. Minimize downtime (fast detection and promotion)

        Implementation based on:
        - MySQL Orchestrator patterns
        - PostgreSQL Patroni patterns
        &quot;&quot;&quot;

        def __init__(
        self,
        primary: 'DatabaseNode',
        replicas: List['DatabaseNode'],
        quorum_nodes: List['QuorumNode'],
        config: 'FailoverConfig'
        ):
        self.primary = primary
        self.replicas = replicas
        self.quorum = quorum_nodes
        self.config = config
        self.state = FailoverState.NORMAL
        self.current_epoch = 0

        async def monitor_primary(self):
        &quot;&quot;&quot;
        Continuous health monitoring of primary.

        Health checks:
        1. TCP connection (is process alive?)
        2. SQL ping (is database responding?)
        3. Replication status (is it accepting writes?)
        &quot;&quot;&quot;
        consecutive_failures = 0

        while True:
        try:
        is_healthy = await self._check_primary_health()

        if is_healthy:
        consecutive_failures = 0
        else:
        consecutive_failures += 1

        if consecutive_failures &gt;= self.config.failure_threshold:
        await self._initiate_failover()
        consecutive_failures = 0

        except Exception as e:
        logger.error(f&quot;Health check error: {e}&quot;)
        consecutive_failures += 1

        await asyncio.sleep(self.config.check_interval_seconds)

        async def _check_primary_health(self) -&gt; bool:
        &quot;&quot;&quot;Check if primary is healthy and accepting writes.&quot;&quot;&quot;
        try:
        # TCP check
        connected = await self.primary.tcp_ping(timeout=1.0)
        if not connected:
        return False

        # SQL check
        result = await self.primary.query(
        &quot;SELECT 1&quot;,
        timeout=self.config.query_timeout_seconds
        )
        if result != 1:
        return False

        # Write check (optional, more expensive)
        if self.config.check_write_ability:
        await self.primary.execute(
        &quot;UPDATE heartbeat SET ts = NOW() WHERE id = 1&quot;
        )

        return True

        except Exception:
        return False

        async def _initiate_failover(self):
        &quot;&quot;&quot;
        Execute failover procedure.

        Critical: This must be idempotent - can be called multiple times
        if previous attempt failed partway through.
        &quot;&quot;&quot;
        logger.warning(&quot;Initiating failover procedure&quot;)
        self.state = FailoverState.DETECTING

        try:
        # Step 1: Confirm primary is actually dead (avoid false positives)
        if await self._confirm_primary_failure():

        # Step 2: Acquire quorum lock (prevents concurrent failovers)
        if await self._acquire_quorum_lock():

        # Step 3: Fence old primary
        self.state = FailoverState.FENCING
        await self._fence_old_primary()

        # Step 4: Select best replica
        best_replica = await self._select_best_replica()

        if best_replica:
        # Step 5: Promote
        self.state = FailoverState.PROMOTING
        await self._promote_replica(best_replica)

        # Step 6: Reconfigure other replicas
        self.state = FailoverState.RECONFIGURING
        await self._reconfigure_replicas(best_replica)

        # Step 7: Update routing
        await self._update_routing(best_replica)

        self.state = FailoverState.COMPLETED
        self.primary = best_replica
        logger.info(f&quot;Failover completed. New primary: {best_replica.host}&quot;)
        else:
        raise FailoverError(&quot;No eligible replica found&quot;)
        else:
        logger.info(&quot;Could not acquire quorum lock - another node handling failover&quot;)
        else:
        logger.info(&quot;Primary recovered during detection - aborting failover&quot;)
        self.state = FailoverState.NORMAL

        except Exception as e:
        self.state = FailoverState.FAILED
        logger.error(f&quot;Failover failed: {e}&quot;)
        await self._alert_oncall(f&quot;Failover failed: {e}&quot;)
        raise

        async def _confirm_primary_failure(self) -&gt; bool:
        &quot;&quot;&quot;
        Confirm primary is actually dead, not just slow.

        Check from multiple vantage points to avoid false positives
        due to network partition between orchestrator and primary.
        &quot;&quot;&quot;
        # Check from orchestrator
        local_check = not await self._check_primary_health()

        # Check from quorum nodes
        quorum_checks = await asyncio.gather(*[
        node.check_primary(self.primary.host)
        for node in self.quorum
        ], return_exceptions=True)

        dead_count = sum(1 for check in quorum_checks if check is False)

        # Primary is dead if majority of quorum agrees
        return dead_count &gt; len(self.quorum) // 2

        async def _acquire_quorum_lock(self) -&gt; bool:
        &quot;&quot;&quot;
        Acquire distributed lock for failover operation.

        Uses consensus (Raft/etcd) to ensure only one node
        performs failover at a time.
        &quot;&quot;&quot;
        self.current_epoch += 1

        try:
        lock = await self.quorum[0].acquire_lock(
        key=f&quot;failover-lock-{self.primary.cluster_id}&quot;,
        value=f&quot;{self.config.orchestrator_id}:{self.current_epoch}&quot;,
        ttl_seconds=60
        )
        return lock.acquired
        except Exception:
        return False

        async def _fence_old_primary(self):
        &quot;&quot;&quot;
        Fence (isolate) old primary to prevent split-brain.

        Fencing methods:
        1. STONITH: Power off via IPMI/BMC
        2. Network: Block traffic via firewall rules
        3. Storage: Revoke storage access
        4. Application: Shutdown database process
        &quot;&quot;&quot;
        logger.info(f&quot;Fencing old primary {self.primary.host}&quot;)

        # Try graceful shutdown first
        try:
        await self.primary.shutdown(timeout=5.0)
        except Exception:
        pass

        # Force STONITH if configured
        if self.config.stonith_enabled:
        await self._execute_stonith(self.primary)

        # Revoke storage access
        if self.config.shared_storage:
        await self._revoke_storage_access(self.primary)

        async def _select_best_replica(self) -&gt; Optional['DatabaseNode']:
        &quot;&quot;&quot;
        Select the best replica to promote.

        Selection criteria (in order):
        1. Most up-to-date (lowest lag)
        2. Same datacenter as old primary (if possible)
        3. Healthy and responsive
        4. Sufficient resources (CPU, memory, disk)
        &quot;&quot;&quot;
        candidates = []

        for replica in self.replicas:
        try:
        status = await replica.get_replication_status()

        if not status.sql_thread_running:
        continue

        candidates.append({
        'node': replica,
        'lag_seconds': status.seconds_behind,
        'lag_bytes': status.bytes_behind,
        'gtid_executed': status.gtid_executed,
        'same_dc': replica.datacenter == self.primary.datacenter,
        'is_healthy': await replica.health_check()
        })

        except Exception as e:
        logger.warning(f&quot;Could not evaluate replica {replica.host}: {e}&quot;)

        # Filter healthy candidates
        healthy = [c for c in candidates if c['is_healthy']]

        if not healthy:
        return None

        # Sort by lag (primary), then same_dc (secondary)
        healthy.sort(key=lambda c: (c['lag_seconds'], not c['same_dc']))

        best = healthy[0]

        # Log potential data loss
        if best['lag_seconds'] &gt; 0:
        logger.warning(
        f&quot;Promoting replica with {best['lag_seconds']}s lag. &quot;
        f&quot;Potential data loss: {best['lag_bytes']} bytes&quot;
        )

        return best['node']

        async def _promote_replica(self, replica: 'DatabaseNode'):
        &quot;&quot;&quot;
        Promote replica to primary.

        Steps:
        1. Wait for relay log to be fully applied
        2. Stop replication (STOP SLAVE)
        3. Reset slave configuration (RESET SLAVE ALL)
        4. Enable writes (SET GLOBAL read_only = OFF)
        &quot;&quot;&quot;
        logger.info(f&quot;Promoting {replica.host} to primary&quot;)

        # Wait for relay log to catch up
        await replica.wait_for_relay_log_applied(timeout=30.0)

        # MySQL-specific promotion
        await replica.execute(&quot;STOP SLAVE&quot;)
        await replica.execute(&quot;RESET SLAVE ALL&quot;)
        await replica.execute(&quot;SET GLOBAL read_only = OFF&quot;)
        await replica.execute(&quot;SET GLOBAL super_read_only = OFF&quot;)

        # Record new epoch
        await replica.execute(
        f&quot;INSERT INTO replication_epoch (epoch, promoted_at) &quot;
        f&quot;VALUES ({self.current_epoch}, NOW())&quot;
        )
        ```
</code></pre>
<hr />
<h2 id="common-pitfalls">Common Pitfalls</h2>
<div style="background: linear-gradient(135deg, #fef2f2 0%, #fee2e2 100%); border-radius: 12px; padding: 24px; margin: 20px 0">
<h4 style="color: #991b1b; margin-top: 0">Critical Replication Anti-Patterns</h4>
<div style="display: grid; gap: 16px">
<div style="background: white; padding: 16px; border-radius: 8px">
<strong style="color: #991b1b">1. Ignoring Replication Lag</strong>
<p style="color: #475569; margin: 8px 0 0 0; font-size: 14px"><strong>Problem:</strong> Reading from replica immediately after write returns stale data. Users see "their update didn't work." <strong>Solution:</strong> Implement read-your-writes consistency with position tokens or sticky sessions. Monitor lag and route around lagging replicas.</p>
</div>
<div style="background: white; padding: 16px; border-radius: 8px">
<strong style="color: #991b1b">2. Split-Brain on Network Partition</strong>
<p style="color: #475569; margin: 8px 0 0 0; font-size: 14px"><strong>Problem:</strong> Both sides of partition think they're primary; both accept writes; data diverges. <strong>Solution:</strong> Use quorum-based leader election with [[consensus-algorithms]](/topic/system-design/consensus-algorithms). Implement STONITH fencing. Use epoch/term numbers to reject stale primaries.</p>
</div>
<div style="background: white; padding: 16px; border-radius: 8px">
<strong style="color: #991b1b">3. Promoting Lagged Replica</strong>
<p style="color: #475569; margin: 8px 0 0 0; font-size: 14px"><strong>Problem:</strong> Failover promotes replica that's minutes behind; recently committed transactions are lost. <strong>Solution:</strong> Always select replica with lowest lag. Wait for relay log to be fully applied before promotion. Use semi-sync replication for zero data loss guarantee.</p>
</div>
<div style="background: white; padding: 16px; border-radius: 8px">
<strong style="color: #991b1b">4. Not Testing Failover</strong>
<p style="color: #475569; margin: 8px 0 0 0; font-size: 14px"><strong>Problem:</strong> First failover is during a real outage. Process is broken, takes hours. <strong>Solution:</strong> Regular failover drills (monthly). Chaos engineering: randomly kill primary in production. Document runbooks for manual failover.</p>
</div>
<div style="background: white; padding: 16px; border-radius: 8px">
<strong style="color: #991b1b">5. Synchronous Replication Over WAN</strong>
<p style="color: #475569; margin: 8px 0 0 0; font-size: 14px"><strong>Problem:</strong> Cross-region sync replication adds 100ms+ to every write; application becomes unusable. <strong>Solution:</strong> Use sync replication only within datacenter. Use async or semi-sync for cross-region DR. Accept RPO > 0 for geo-distributed systems.</p>
</div>
<div style="background: white; padding: 16px; border-radius: 8px">
<strong style="color: #991b1b">6. Writing to Replicas</strong>
<p style="color: #475569; margin: 8px 0 0 0; font-size: 14px"><strong>Problem:</strong> Developer or script writes directly to replica; data diverges from primary; replication breaks. <strong>Solution:</strong> Set <code>read_only=ON</code> on all replicas. Use network-level access control. Monitor for unexpected writes.</p>
</div>
</div>
</div>
<hr />
<h2 id="quick-reference">Quick Reference</h2>
<div style="background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%); border-radius: 16px; padding: 28px; margin: 24px 0">
<h4 style="color: #1e293b; margin-top: 0; font-size: 18px">Database Replication Cheat Sheet</h4>
<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 24px">
<div>
<div style="color: #1e293b; font-weight: 600; margin-bottom: 12px;padding-bottom: 8px">Topology Selection</div>
<div style="font-size: 14px; color: #475569; line-height: 1.8">
<div><strong>Single-Primary:</strong> Strong consistency, simple, most use cases</div>
<div><strong>Multi-Primary:</strong> Write locality, conflict resolution needed</div>
<div><strong>Chain:</strong> Reduce primary load, increased total lag</div>
</div>
</div>
<div>
<div style="color: #1e293b; font-weight: 600; margin-bottom: 12px;padding-bottom: 8px">Replication Mode</div>
<div style="font-size: 14px; color: #475569; line-height: 1.8">
<div><strong>Sync:</strong> Zero data loss, high latency</div>
<div><strong>Semi-sync:</strong> Balanced, 1+ replica ack</div>
<div><strong>Async:</strong> Low latency, data loss window</div>
</div>
</div>
<div>
<div style="color: #1e293b; font-weight: 600; margin-bottom: 12px;padding-bottom: 8px">Conflict Resolution</div>
<div style="font-size: 14px; color: #475569; line-height: 1.8">
<div><strong>LWW:</strong> Simple, loses data</div>
<div><strong>Custom merge:</strong> Domain-specific, complex</div>
<div><strong>CRDTs:</strong> Conflict-free by design</div>
</div>
</div>
<div>
<div style="color: #1e293b; font-weight: 600; margin-bottom: 12px;padding-bottom: 8px">Key Metrics</div>
<div style="font-size: 14px; color: #475569; line-height: 1.8">
<div><strong>Lag:</strong> Seconds_Behind_Master &lt; 5s</div>
<div><strong>GTID gap:</strong> Should be 0</div>
<div><strong>Replication threads:</strong> Both running</div>
</div>
</div>
</div>
<div style="margin-top: 24px; padding-top: 16px">
<div style="color: #1e293b; font-weight: 600; margin-bottom: 12px">Failover Checklist</div>
<div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 16px; font-size: 13px">
<div style="color: #475569">
<div>[ ] Confirm primary dead</div>
<div>[ ] Acquire quorum lock</div>
<div>[ ] Fence old primary</div>
</div>
<div style="color: #475569">
<div>[ ] Select lowest-lag replica</div>
<div>[ ] Wait for relay log apply</div>
<div>[ ] Promote to primary</div>
</div>
<div style="color: #475569">
<div>[ ] Reconfigure other replicas</div>
<div>[ ] Update DNS/routing</div>
<div>[ ] Verify write path</div>
</div>
</div>
</div>
</div>
<hr />
<h2 id="related-topics">Related Topics</h2>
<pre><code>        - [[database-sharding]](/topic/system-design/database-sharding) - Horizontal partitioning for write scalability
        - [[cap-theorem]](/topic/system-design/cap-theorem) - Understanding consistency trade-offs
        - [[consensus-algorithms]](/topic/system-design/consensus-algorithms) - Raft, Paxos for leader election
        - [[distributed-locking]](/topic/system-design/distributed-locking) - Coordinating failover operations
        - [[caching]](/topic/system-design/caching) - Cache-aside patterns with replicas
        - [[event-sourcing]](/topic/system-design/event-sourcing) - Alternative to traditional replication
        - [[circuit-breaker]](/topic/design-patterns/circuit-breaker) - Handling replica failures gracefully
</code></pre>
