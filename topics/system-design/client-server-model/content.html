<h1 id="client-server-model">Client-Server Model</h1>
<h2 id="overview">Overview</h2>
<p>The <span>Client-Server Model</span> is the foundational architecture pattern for distributed computing. It divides computing tasks between <span>service providers (servers)</span> that host resources and <span>service requesters (clients)</span> that consume them. This simple yet powerful paradigm underpins virtually every internet application from web browsing to mobile apps to IoT devices.</p>
<div>
<h4>Why This Matters in Interviews</h4>
<div>
<div>
<div>Surface-level answer:</div>
<div>"Client sends requests, server responds"</div>
</div>
<div>
<div>Interview-winning answer:</div>
<div>"Understanding the trade-offs between REST's statelessness enabling horizontal scaling versus WebSocket's persistent connections for real-time features, and when to use SSE for unidirectional server push"</div>
</div>
</div>
</div>
<div>
<h3>CLIENT-SERVER MODEL</h3>
<div>
<div>
<div>
<span>CLIENT</span>
</div>
<div>Initiates requests</div>
<div>Browser, Mobile App</div>
</div>
<div>
<div>Request --></div>
<div>
<span>Network</span>
</div>
<div><-- Response</div>
</div>
<div>
<div>
<span>SERVER</span>
</div>
<div>Provides services</div>
<div>API, Database</div>
</div>
</div>
</div>
<p><strong>The Simple Explanation</strong>: Think of it like a restaurant. You (the client) make requests (&quot;I'll have the pasta&quot;), and the kitchen (the server) processes your request and returns what you asked for. The waiter is like the network - carrying messages back and forth. You don't need to know how to cook; the kitchen doesn't need to know who you are beyond your order.</p>
<hr />
<h2 id="why-it-matters-real-company-examples">Why It Matters: Real Company Examples</h2>
<p>The client-server model is so fundamental that every tech company uses it:</p>
<div>
<h3>CLIENT-SERVER IN THE REAL WORLD</h3>
<div>
<div>
<div>Google Search</div>
<div>Your browser (client) sends a search query. Google's servers process billions of web pages and return ranked results in milliseconds.</div>
</div>
<div>
<div>Instagram</div>
<div>Mobile app (thick client) stores images locally, communicates with servers for feed, likes, and comments. Servers handle storage and social graph.</div>
</div>
<div>
<div>Slack</div>
<div>Desktop/mobile clients maintain WebSocket connections for real-time messaging. Servers coordinate message delivery across thousands of organizations.</div>
</div>
<div>
<div>Stripe</div>
<div>Your application (client) calls Stripe's API (server) to process payments. Stripe handles PCI compliance, fraud detection, and bank connections.</div>
</div>
</div>
</div>
<hr />
<h2 id="request-response-model-deep-dive">Request-Response Model Deep Dive</h2>
<p>The <span>request-response model</span> is the fundamental interaction pattern in client-server architectures. Understanding its mechanics is crucial for designing performant, reliable systems.</p>
<div>
<h3>REQUEST-RESPONSE LIFECYCLE</h3>
<div>
<div>
<div>1</div>
<div>
<div>Client initiates request</div>
<div>HTTP GET /api/users/123</div>
</div>
</div>
<div>
<div>2</div>
<div>
<div>DNS resolves hostname</div>
<div>api.example.com -> 93.184.216.34</div>
</div>
</div>
<div>
<div>3</div>
<div>
<div>TCP connection established</div>
<div>Three-way handshake (SYN, SYN-ACK, ACK)</div>
</div>
</div>
<div>
<div>4</div>
<div>
<div>Server processes request</div>
<div>Query database, apply business logic</div>
</div>
</div>
<div>
<div>5</div>
<div>
<div>Server sends response</div>
<div>HTTP 200 OK + JSON payload</div>
</div>
</div>
<div>
<div>6</div>
<div>
<div>Client processes response</div>
<div>Display data to user</div>
</div>
</div>
</div>
</div>
<h3 id="latency-breakdown">Latency Breakdown</h3>
<p>Understanding where time is spent in a request helps optimize performance. See <a href="/topics/system-design/latency-throughput">[Latency and Throughput]</a> for deeper analysis.</p>
<div>
<h4>Request Latency Components</h4>
<div>
<div>
<div>DNS Lookup</div>
<div>
<span>~50ms</span>
</div>
</div>
<div>
<div>TCP Handshake</div>
<div>
<span>~1 RTT</span>
</div>
</div>
<div>
<div>TLS Handshake</div>
<div>
<span>~2 RTT (TLS 1.2)</span>
</div>
</div>
<div>
<div>Request Transfer</div>
<div>
<span>~10ms</span>
</div>
</div>
<div>
<div>Server Processing</div>
<div>
<span>Variable (10-500ms)</span>
</div>
</div>
<div>
<div>Response Transfer</div>
<div>
<span>Size dependent</span>
</div>
</div>
</div>
<div>
<div><strong>Optimization:</strong> HTTP/2 multiplexing, TLS 1.3 (1 RTT), connection pooling, and CDN caching reduce these costs significantly.</div>
</div>
</div>
<h3 id="request-methods-and-semantics">Request Methods and Semantics</h3>
<div>
<h4>HTTP Methods Deep Dive</h4>
<div>
<div>
<div>GET</div>
<div>Safe, Idempotent, Cacheable</div>
<div>Retrieve resource without side effects. Can be cached by browsers and CDNs.</div>
</div>
<div>
<div>POST</div>
<div>Not Safe, Not Idempotent</div>
<div>Create new resources. Multiple calls create multiple resources.</div>
</div>
<div>
<div>PUT</div>
<div>Not Safe, Idempotent</div>
<div>Replace entire resource. Same request yields same result.</div>
</div>
<div>
<div>PATCH</div>
<div>Not Safe, Not Idempotent*</div>
<div>Partial update. Semantics depend on implementation.</div>
</div>
<div>
<div>DELETE</div>
<div>Not Safe, Idempotent</div>
<div>Remove resource. Deleting twice should not fail.</div>
</div>
</div>
</div>
<hr />
<h2 id="client-types">Client Types</h2>
<div>
<h3>CLIENT TYPES</h3>
<div>
<div>
<div>Thin Client</div>
<div>Minimal processing, relies on server for most logic</div>
<div>
<div>Examples:</div>
<ul>
<li>Traditional web browsers</li>
<li>Terminal clients</li>
<li>Streaming devices</li>
</ul>
</div>
</div>
<div>
<div>Thick Client</div>
<div>Significant local processing and storage capabilities</div>
<div>
<div>Examples:</div>
<ul>
<li>Desktop applications</li>
<li>Mobile apps (offline-capable)</li>
<li>Gaming clients</li>
</ul>
</div>
</div>
<div>
<div>Hybrid Client (SPA)</div>
<div>Best of both worlds - rich UI with server data</div>
<div>
<div>Examples:</div>
<ul>
<li>React/Vue applications</li>
<li>Progressive Web Apps</li>
<li>Electron apps</li>
</ul>
</div>
</div>
</div>
</div>
<hr />
<h2 id="server-types-and-multi-tier-architecture">Server Types and Multi-Tier Architecture</h2>
<div>
<h3>THREE-TIER ARCHITECTURE</h3>
<div>
<div>
<div>PRESENTATION TIER</div>
<div>Client Interface (Browser, Mobile App)</div>
<div>HTML, CSS, JavaScript, React, Flutter</div>
</div>
<div>HTTP / HTTPS</div>
<div>
<div>APPLICATION TIER</div>
<div>Business Logic (API Server)</div>
<div>Node.js, Python, Java, Go</div>
</div>
<div>SQL / Internal APIs</div>
<div>
<div>DATA TIER</div>
<div>Data Storage (Database)</div>
<div>PostgreSQL, MongoDB, Redis</div>
</div>
</div>
</div>
<h3 id="benefits-of-multi-tier-architecture">Benefits of Multi-Tier Architecture</h3>
<table>
<thead>
<tr>
<th>Benefit</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Separation of concerns</strong></td>
<td>Each tier has specific responsibility</td>
</tr>
<tr>
<td><strong>Independent scaling</strong></td>
<td>Scale each tier based on demand</td>
</tr>
<tr>
<td><strong>Security</strong></td>
<td>Database not directly exposed to internet</td>
</tr>
<tr>
<td><strong>Maintainability</strong></td>
<td>Update one tier without affecting others</td>
</tr>
<tr>
<td><strong>Technology flexibility</strong></td>
<td>Use best tool for each tier</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="stateless-vs-stateful-servers">Stateless vs Stateful Servers</h2>
<p>Understanding <span>stateless</span> vs <span>stateful</span> server design is crucial for building scalable systems. This directly impacts your ability to implement <a href="/topics/system-design/load-balancing">[Load Balancing]</a> and horizontal scaling.</p>
<div>
<h3>STATELESS vs STATEFUL</h3>
<div>
<div>
<div>STATELESS SERVER (Recommended)</div>
<div>Each request contains all information needed. Server does not remember previous requests.</div>
<div>
  GET /api/orders<br>
  Authorization: Bearer eyJhbG...<br>
<span># Token contains user identity</span>
</div>
<div>
<div>+ Easy to scale horizontally</div>
<div>+ Any server can handle any request</div>
<div>+ Simple load balancing</div>
<div>+ Better fault tolerance</div>
</div>
</div>
<div>
<div>STATEFUL SERVER (Avoid)</div>
<div>Server maintains session data between requests. Client must connect to same server.</div>
<div>
  GET /api/orders<br>
  Cookie: session_id=abc123<br>
<span># Server looks up session</span>
</div>
<div>
<div>- Harder to scale</div>
<div>- Requires sticky sessions</div>
<div>- Complex load balancing</div>
<div>- Session replication needed</div>
</div>
</div>
</div>
</div>
<h3 id="critical-assumption">Critical Assumption</h3>
<div>
<h4>When Stateful is Acceptable</h4>
<div>
Stateful servers are appropriate when: (1) <span>Connection state is inherent</span> to the protocol (WebSockets, gaming servers), (2) <span>Session affinity</span> provides significant performance benefits (in-memory caching of user data), or (3) <span>Coordination overhead</span> of external state stores exceeds the complexity of sticky sessions.
</div>
</div>
<h3 id="externalizing-state">Externalizing State</h3>
<pre><code>    ```python
    # STATEFUL (Hard to scale)
    class StatefulServer:
    def __init__(self):
    self.sessions = {}  # State stored in memory

    def handle_request(self, session_id, data):
    if session_id in self.sessions:
    return self.sessions[session_id]
    return None

    # STATELESS (Easy to scale)
    class StatelessServer:
    def __init__(self, redis_client):
    self.redis = redis_client  # External state store

    def handle_request(self, token, data):
    # Validate token (contains user info)
    user = jwt.decode(token)

    # Get any needed state from external store
    session = self.redis.get(f&quot;session:{user.id}&quot;)

    return process_request(user, session, data)
    ```
</code></pre>
<div>
<h4>State Externalization Strategies</h4>
<div>
<div>
<div>Redis/Memcached</div>
<div>Session storage, caching</div>
</div>
<div>
<div>JWT Tokens</div>
<div>Self-contained auth state</div>
</div>
<div>
<div>Database</div>
<div>Persistent state storage</div>
</div>
<div>
<div>Message Queues</div>
<div>Async state coordination</div>
</div>
</div>
</div>
<hr />
<h2 id="rest-architecture-deep-dive">REST Architecture Deep Dive</h2>
<p><span>REST (Representational State Transfer)</span> is the dominant architectural style for web APIs. Understanding its constraints is essential for <a href="/topics/system-design/api-design">[API Design]</a>.</p>
<div>
<h3>REST ARCHITECTURAL CONSTRAINTS</h3>
<div>
<div>
<div>1. Client-Server Separation</div>
<div>UI concerns separate from data storage. Enables independent evolution and improved scalability.</div>
</div>
<div>
<div>2. Statelessness</div>
<div>Each request contains all context needed. No session state on server. Enables horizontal scaling.</div>
</div>
<div>
<div>3. Cacheability</div>
<div>Responses must declare themselves cacheable or not. Enables CDNs and client caching.</div>
</div>
<div>
<div>4. Uniform Interface</div>
<div>Standardized resource identification, manipulation through representations, self-descriptive messages.</div>
</div>
<div>
<div>5. Layered System</div>
<div>Client cannot tell if connected directly to server or intermediary. Enables load balancers, proxies.</div>
</div>
<div>
<div>6. Code on Demand (Optional)</div>
<div>Servers can extend client functionality by transferring executable code (JavaScript).</div>
</div>
</div>
</div>
<h3 id="rest-maturity-model-richardson">REST Maturity Model (Richardson)</h3>
<div>
<h4>REST Maturity Levels</h4>
<div>
<div>
<div>Level 0</div>
<div>
<div>The Swamp of POX</div>
<div>Single endpoint, HTTP as transport only. POST /api with action in body.</div>
</div>
</div>
<div>
<div>Level 1</div>
<div>
<div>Resources</div>
<div>Individual resources with unique URIs. POST /users, POST /orders.</div>
</div>
</div>
<div>
<div>Level 2</div>
<div>
<div>HTTP Verbs</div>
<div>Proper use of GET, POST, PUT, DELETE. Status codes for responses.</div>
</div>
</div>
<div>
<div>Level 3</div>
<div>
<div>Hypermedia Controls (HATEOAS)</div>
<div>Responses include links to related actions. Self-documenting API.</div>
</div>
</div>
</div>
</div>
<h3 id="rest-api-example">REST API Example</h3>
<pre><code>    ```python
    from flask import Flask, request, jsonify
    from functools import wraps
    import jwt

    app = Flask(__name__)

    # RESTful resource: Users
    # Follows REST constraints: stateless, uniform interface, cacheable

    @app.route('/api/v1/users', methods=['GET'])
    def list_users():
    &quot;&quot;&quot;GET collection - retrieve all users (paginated)&quot;&quot;&quot;
    page = request.args.get('page', 1, type=int)
    per_page = request.args.get('per_page', 20, type=int)

    users = User.query.paginate(page=page, per_page=per_page)

    return jsonify({
    'data': [u.to_dict() for u in users.items],
    'meta': {
    'page': page,
    'per_page': per_page,
    'total': users.total,
    'pages': users.pages
    },
    'links': {
    'self': f'/api/v1/users?page={page}',
    'next': f'/api/v1/users?page={page+1}' if users.has_next else None,
    'prev': f'/api/v1/users?page={page-1}' if users.has_prev else None
    }
    }), 200, {'Cache-Control': 'private, max-age=60'}

    @app.route('/api/v1/users/&lt;int:user_id&gt;', methods=['GET'])
      def get_user(user_id):
      &quot;&quot;&quot;GET resource - retrieve single user&quot;&quot;&quot;
      user = User.query.get_or_404(user_id)

      response = jsonify({
      'data': user.to_dict(),
      'links': {
      'self': f'/api/v1/users/{user_id}',
      'orders': f'/api/v1/users/{user_id}/orders',
      'update': {'href': f'/api/v1/users/{user_id}', 'method': 'PUT'},
      'delete': {'href': f'/api/v1/users/{user_id}', 'method': 'DELETE'}
      }
      })
      response.headers['ETag'] = user.etag
      response.headers['Cache-Control'] = 'private, max-age=300'
      return response

      @app.route('/api/v1/users', methods=['POST'])
      def create_user():
      &quot;&quot;&quot;POST collection - create new user&quot;&quot;&quot;
      data = request.get_json()

      user = User(
      name=data['name'],
      email=data['email']
      )
      db.session.add(user)
      db.session.commit()

      return jsonify({
      'data': user.to_dict(),
      'links': {'self': f'/api/v1/users/{user.id}'}
      }), 201, {'Location': f'/api/v1/users/{user.id}'}

      @app.route('/api/v1/users/&lt;int:user_id&gt;', methods=['PUT'])
        def update_user(user_id):
        &quot;&quot;&quot;PUT resource - replace entire user&quot;&quot;&quot;
        user = User.query.get_or_404(user_id)

        # Optimistic locking with ETag
        if_match = request.headers.get('If-Match')
        if if_match and if_match != user.etag:
        return jsonify({'error': 'Resource modified'}), 412

        data = request.get_json()
        user.name = data['name']
        user.email = data['email']
        db.session.commit()

        return jsonify({'data': user.to_dict()})

        @app.route('/api/v1/users/&lt;int:user_id&gt;', methods=['DELETE'])
          def delete_user(user_id):
          &quot;&quot;&quot;DELETE resource - remove user&quot;&quot;&quot;
          user = User.query.get_or_404(user_id)
          db.session.delete(user)
          db.session.commit()

          return '', 204  # No content
          ```
</code></pre>
<hr />
<h2 id="websockets-deep-dive">WebSockets Deep Dive</h2>
<p><span>WebSockets</span> provide full-duplex communication channels over a single TCP connection. Unlike HTTP's request-response model, WebSockets allow both client and server to send messages independently at any time.</p>
<div>
<h3>WEBSOCKET CONNECTION LIFECYCLE</h3>
<div>
<div>
<div>
<span>CLIENT</span>
</div>
</div>
<div>
<div>
<span>HTTP Upgrade --></span>
</div>
<div>
<span><-- 101 Switching</span>
</div>
<div>
<span>Persistent Connection</span>
</div>
<div>
<span>Message <--></span>
</div>
<div>
<span>Message <--></span>
</div>
<div>
<span>Close --></span>
</div>
</div>
<div>
<div>
<span>SERVER</span>
</div>
</div>
</div>
</div>
<h3 id="websocket-handshake">WebSocket Handshake</h3>
<pre><code>          ```http
          # Client Request (HTTP Upgrade)
          GET /chat HTTP/1.1
          Host: server.example.com
          Upgrade: websocket
          Connection: Upgrade
          Sec-WebSocket-Key: dGhlIHNhbXBsZSBub25jZQ==
          Sec-WebSocket-Version: 13
          Origin: http://example.com

          # Server Response
          HTTP/1.1 101 Switching Protocols
          Upgrade: websocket
          Connection: Upgrade
          Sec-WebSocket-Accept: s3pPLMBiTxaQ9kYGzzhZRbK+xOo=
          ```
</code></pre>
<h3 id="websocket-vs-http-comparison">WebSocket vs HTTP Comparison</h3>
<div>
<h4>HTTP vs WebSocket</h4>
<div>
<div>
<div>HTTP (Request-Response)</div>
<ul>
<li>Half-duplex communication</li>
<li>New connection per request*</li>
<li>Headers sent with each request</li>
<li>Stateless by design</li>
<li>Easy to cache and scale</li>
<li>Works with CDNs, proxies</li>
</ul>
<div>
*HTTP/2 uses multiplexing over single connection
</div>
</div>
<div>
<div>WebSocket (Bidirectional)</div>
<ul>
<li>Full-duplex communication</li>
<li>Persistent connection</li>
<li>Minimal per-message overhead</li>
<li>Inherently stateful</li>
<li>Complex to scale horizontally</li>
<li>Requires sticky sessions or pub/sub</li>
</ul>
<div>
  Ideal for: chat, gaming, live data, collaborative editing
</div>
</div>
</div>
</div>
<h3 id="websocket-server-implementation">WebSocket Server Implementation</h3>
<pre><code>          ```python
          import asyncio
          import websockets
          import json
          import redis.asyncio as redis

          class WebSocketServer:
          def __init__(self):
          self.connections = {}  # user_id -&gt; websocket
          self.redis = None

          async def start(self):
          # Connect to Redis for cross-server messaging
          self.redis = await redis.from_url(&quot;redis://localhost&quot;)

          # Subscribe to messages from other servers
          pubsub = self.redis.pubsub()
          await pubsub.subscribe(&quot;broadcast&quot;)
          asyncio.create_task(self.redis_listener(pubsub))

          # Start WebSocket server
          async with websockets.serve(self.handler, &quot;localhost&quot;, 8765):
          await asyncio.Future()  # Run forever

          async def handler(self, websocket, path):
          &quot;&quot;&quot;Handle individual WebSocket connection&quot;&quot;&quot;
          user_id = None
          try:
          # Authenticate on connect
          auth_message = await websocket.recv()
          user_id = await self.authenticate(auth_message)

          if not user_id:
          await websocket.close(1008, &quot;Unauthorized&quot;)
          return

          # Register connection
          self.connections[user_id] = websocket
          await self.redis.sadd(&quot;online_users&quot;, user_id)

          # Handle messages
          async for message in websocket:
          await self.handle_message(user_id, message)

          except websockets.ConnectionClosed:
          pass
          finally:
          # Cleanup on disconnect
          if user_id:
          del self.connections[user_id]
          await self.redis.srem(&quot;online_users&quot;, user_id)

          async def handle_message(self, sender_id, raw_message):
          &quot;&quot;&quot;Process incoming message&quot;&quot;&quot;
          message = json.loads(raw_message)

          if message['type'] == 'direct':
          # Send to specific user
          await self.send_to_user(
          message['recipient_id'],
          {
          'type': 'message',
          'from': sender_id,
          'content': message['content'],
          'timestamp': time.time()
          }
          )
          elif message['type'] == 'broadcast':
          # Broadcast to all users via Redis
          await self.redis.publish(&quot;broadcast&quot;, json.dumps({
          'from': sender_id,
          'content': message['content']
          }))

          async def send_to_user(self, user_id, message):
          &quot;&quot;&quot;Send message to user (local or remote)&quot;&quot;&quot;
          if user_id in self.connections:
          # User on this server
          await self.connections[user_id].send(json.dumps(message))
          else:
          # User on different server - route via Redis
          await self.redis.publish(f&quot;user:{user_id}&quot;, json.dumps(message))

          async def redis_listener(self, pubsub):
          &quot;&quot;&quot;Listen for messages from other servers&quot;&quot;&quot;
          async for message in pubsub.listen():
          if message['type'] == 'message':
          data = json.loads(message['data'])
          # Broadcast to all local connections
          for ws in self.connections.values():
          await ws.send(json.dumps(data))
          ```
</code></pre>
<h3 id="javascript-websocket-client">JavaScript WebSocket Client</h3>
<pre><code>          ```javascript
          class WebSocketClient {
          constructor(url) {
          this.url = url;
          this.ws = null;
          this.reconnectAttempts = 0;
          this.maxReconnectAttempts = 5;
          this.messageHandlers = new Map();
          }

          connect(authToken) {
          return new Promise((resolve, reject) =&gt; {
          this.ws = new WebSocket(this.url);

          this.ws.onopen = () =&gt; {
          console.log('WebSocket connected');
          this.reconnectAttempts = 0;

          // Send authentication
          this.send({ type: 'auth', token: authToken });
          resolve();
          };

          this.ws.onmessage = (event) =&gt; {
          const message = JSON.parse(event.data);
          this.handleMessage(message);
          };

          this.ws.onclose = (event) =&gt; {
          console.log('WebSocket closed:', event.code, event.reason);
          if (!event.wasClean) {
          this.reconnect(authToken);
          }
          };

          this.ws.onerror = (error) =&gt; {
          console.error('WebSocket error:', error);
          reject(error);
          };
          });
          }

          reconnect(authToken) {
          if (this.reconnectAttempts &gt;= this.maxReconnectAttempts) {
          console.error('Max reconnection attempts reached');
          return;
          }

          // Exponential backoff
          const delay = Math.min(1000 * Math.pow(2, this.reconnectAttempts), 30000);
          this.reconnectAttempts++;

          console.log(`Reconnecting in ${delay}ms (attempt ${this.reconnectAttempts})`);
          setTimeout(() =&gt; this.connect(authToken), delay);
          }

          send(message) {
          if (this.ws?.readyState === WebSocket.OPEN) {
          this.ws.send(JSON.stringify(message));
          } else {
          console.warn('WebSocket not connected');
          }
          }

          on(type, handler) {
          this.messageHandlers.set(type, handler);
          }

          handleMessage(message) {
          const handler = this.messageHandlers.get(message.type);
          if (handler) {
          handler(message);
          }
          }

          disconnect() {
          if (this.ws) {
          this.ws.close(1000, 'Client disconnect');
          }
          }
          }

          // Usage
          const ws = new WebSocketClient('wss://api.example.com/ws');
          await ws.connect(authToken);

          ws.on('message', (msg) =&gt; {
          console.log('New message:', msg.content);
          });

          ws.send({ type: 'direct', recipient_id: '456', content: 'Hello!' });
          ```
</code></pre>
<h3 id="scaling-websockets">Scaling WebSockets</h3>
<div>
<h4>WebSocket Scaling Architecture</h4>
<div>
<div>
<div>
<span>Client 1</span>
</div>
<div>
<span>Client 2</span>
</div>
<div>
<span>Client 3</span>
</div>
<div>
<span>Client 4</span>
</div>
</div>
<div>Persistent WS Connections</div>
<div>
<span>Load Balancer (Sticky Sessions / IP Hash)</span>
</div>
<div>
<div>
<div>WS Server 1</div>
<div>Clients 1,2</div>
</div>
<div>
<div>WS Server 2</div>
<div>Clients 3,4</div>
</div>
</div>
<div>Pub/Sub for Cross-Server Messaging</div>
<div>
<span>Redis Pub/Sub</span>
</div>
</div>
</div>
<p>See <a href="/topics/system-design/message-queues">[Message Queues]</a> for more on pub/sub patterns.</p>
<hr />
<h2 id="server-sent-events-sse">Server-Sent Events (SSE)</h2>
<p><span>Server-Sent Events (SSE)</span> provide a lightweight mechanism for servers to push data to clients over HTTP. Unlike WebSockets, SSE is unidirectional (server to client only) but simpler to implement and works over standard HTTP.</p>
<div>
<h3>COMMUNICATION PATTERNS COMPARISON</h3>
<div>
<div>
<div>HTTP Polling</div>
<div>
<div>
<span>C</span>
<span>--></span>
<span>S</span>
</div>
<div>
<span>C</span>
<span><--</span>
<span>S</span>
</div>
</div>
<div>Repeated requests at intervals. Simple but inefficient.</div>
</div>
<div>
<div>Long Polling</div>
<div>
<div>
<span>C</span>
<span>--> (hold)</span>
<span>S</span>
</div>
<div>
<span>C</span>
<span><-- (when ready)</span>
<span>S</span>
</div>
</div>
<div>Server holds request until data available. Fallback option.</div>
</div>
<div>
<div>SSE</div>
<div>
<div>
<span>C</span>
<span>--> (subscribe)</span>
<span>S</span>
</div>
<div>
<span>C</span>
<span><-- stream</span>
<span>S</span>
</div>
</div>
<div>Server pushes events. Auto-reconnect. Text-based.</div>
</div>
<div>
<div>WebSocket</div>
<div>
<div>
<span>C</span>
<span><--></span>
<span>S</span>
</div>
</div>
<div>Full duplex. Binary/text. Lowest latency.</div>
</div>
</div>
</div>
<h3 id="when-to-use-sse-vs-websocket">When to Use SSE vs WebSocket</h3>
<div>
<h4>SSE vs WebSocket Decision Guide</h4>
<div>
<div>
<div>Use SSE When:</div>
<ul>
<li>Server-to-client only (notifications, feeds)</li>
<li>Need auto-reconnection with last-event-id</li>
<li>Want to use standard HTTP infrastructure</li>
<li>Text/JSON data only is sufficient</li>
<li>Simplicity is more important than features</li>
</ul>
<div>
<div><strong>Examples:</strong> Live stock prices, social media feeds, notifications, live scores</div>
</div>
</div>
<div>
<div>Use WebSocket When:</div>
<ul>
<li>Bidirectional communication needed</li>
<li>Binary data transfer required</li>
<li>Lowest possible latency critical</li>
<li>High message frequency (gaming)</li>
<li>Need custom protocols</li>
</ul>
<div>
<div><strong>Examples:</strong> Chat apps, multiplayer games, collaborative editors, trading platforms</div>
</div>
</div>
</div>
</div>
<h3 id="sse-server-implementation">SSE Server Implementation</h3>
<pre><code>          ```python
          from flask import Flask, Response, request
          import json
          import time
          import queue
          import threading

          app = Flask(__name__)

          # Per-user message queues
          user_queues = {}

          def event_stream(user_id):
          &quot;&quot;&quot;Generator that yields SSE events&quot;&quot;&quot;
          q = queue.Queue()
          user_queues[user_id] = q

          try:
          while True:
          # Block until message available or timeout
          try:
          message = q.get(timeout=30)
          yield format_sse(message)
          except queue.Empty:
          # Send heartbeat to keep connection alive
          yield format_sse({'type': 'heartbeat'}, event='ping')
          finally:
          # Cleanup on disconnect
          del user_queues[user_id]

          def format_sse(data, event=None, id=None, retry=None):
          &quot;&quot;&quot;Format data as SSE message&quot;&quot;&quot;
          lines = []
          if id:
          lines.append(f'id: {id}')
          if event:
          lines.append(f'event: {event}')
          if retry:
          lines.append(f'retry: {retry}')
          lines.append(f'data: {json.dumps(data)}')
          return '\n'.join(lines) + '\n\n'

          @app.route('/api/events')
          def sse_stream():
          &quot;&quot;&quot;SSE endpoint for real-time updates&quot;&quot;&quot;
          user_id = get_current_user_id()

          response = Response(
          event_stream(user_id),
          mimetype='text/event-stream',
          headers={
          'Cache-Control': 'no-cache',
          'Connection': 'keep-alive',
          'X-Accel-Buffering': 'no'  # Disable nginx buffering
          }
          )
          return response

          @app.route('/api/notify', methods=['POST'])
          def send_notification():
          &quot;&quot;&quot;Send notification to user via SSE&quot;&quot;&quot;
          data = request.get_json()
          user_id = data['user_id']

          if user_id in user_queues:
          user_queues[user_id].put({
          'type': 'notification',
          'title': data['title'],
          'body': data['body'],
          'timestamp': time.time()
          })
          return {'status': 'sent'}
          return {'status': 'user_offline'}, 404
          ```
</code></pre>
<h3 id="sse-client-implementation">SSE Client Implementation</h3>
<pre><code>          ```javascript
          class SSEClient {
          constructor(url) {
          this.url = url;
          this.eventSource = null;
          this.handlers = new Map();
          }

          connect(authToken) {
          // SSE doesn't support custom headers, use query param or cookies
          const urlWithAuth = `${this.url}?token=${authToken}`;

          this.eventSource = new EventSource(urlWithAuth);

          this.eventSource.onopen = () =&gt; {
          console.log('SSE connected');
          };

          // Default message handler
          this.eventSource.onmessage = (event) =&gt; {
          const data = JSON.parse(event.data);
          this.handleEvent('message', data);
          };

          // Named event handlers
          this.eventSource.addEventListener('notification', (event) =&gt; {
          const data = JSON.parse(event.data);
          this.handleEvent('notification', data);
          });

          this.eventSource.addEventListener('ping', (event) =&gt; {
          // Heartbeat received, connection is alive
          });

          this.eventSource.onerror = (error) =&gt; {
          console.error('SSE error:', error);
          // EventSource will auto-reconnect
          // Use Last-Event-ID header to resume from last event
          };
          }

          on(eventType, handler) {
          this.handlers.set(eventType, handler);
          }

          handleEvent(type, data) {
          const handler = this.handlers.get(type);
          if (handler) {
          handler(data);
          }
          }

          disconnect() {
          if (this.eventSource) {
          this.eventSource.close();
          }
          }
          }

          // Usage
          const sse = new SSEClient('/api/events');
          sse.connect(authToken);

          sse.on('notification', (data) =&gt; {
          showNotification(data.title, data.body);
          });
          ```
</code></pre>
<hr />
<h2 id="communication-patterns-summary">Communication Patterns Summary</h2>
<div>
<h3>CHOOSING THE RIGHT PATTERN</h3>
<table>
  <thead>
<tr>
<th>Pattern</th>
<th>Direction</th>
<th>Latency</th>
<th>Complexity</th>
<th>Best For</th>
</tr>
  </thead>
  <tbody>
<tr>
<td>REST</td>
<td>Request-Response</td>
<td>Medium</td>
<td>Low</td>
<td>CRUD APIs, cacheable resources</td>
</tr>
<tr>
<td>HTTP Polling</td>
<td>Client-initiated</td>
<td>High</td>
<td>Low</td>
<td>Infrequent updates, simplicity</td>
</tr>
<tr>
<td>Long Polling</td>
<td>Server-timed response</td>
<td>Medium</td>
<td>Medium</td>
<td>Fallback when SSE/WS unavailable</td>
</tr>
<tr>
<td>SSE</td>
<td>Server-to-Client</td>
<td>Low</td>
<td>Low</td>
<td>Live feeds, notifications</td>
</tr>
<tr>
<td>WebSocket</td>
<td>Bidirectional</td>
<td>Lowest</td>
<td>High</td>
<td>Chat, gaming, collaboration</td>
</tr>
  </tbody>
</table>
</div>
<hr />
<h2 id="common-pitfalls">Common Pitfalls</h2>
<div>
<div>1. Storing Session in Server Memory</div>
<div>Storing sessions in server memory prevents horizontal scaling. When you add servers, sessions are not shared. Use external stores like Redis. See [[Caching]](/topics/system-design/caching) for session storage patterns.</div>
</div>
<div>
<div>2. Tight Coupling Between Client and Server</div>
<div>Changing server response structure breaks clients. Use API versioning (v1, v2) and maintain backward compatibility. Consider GraphQL for flexibility. See [[API Design]](/topics/system-design/api-design) for versioning strategies.</div>
</div>
<div>
<div>3. No Timeout Handling</div>
<div>Clients should always set timeouts. Server may be slow or unresponsive. Without timeouts, clients hang indefinitely, degrading user experience. Implement [[Circuit Breaker]](/topics/system-design/circuit-breaker) patterns for resilience.</div>
</div>
<div>
<div>4. Ignoring Network Failures</div>
<div>Network requests can fail. Clients should implement retry logic with exponential backoff. Show meaningful error messages to users.</div>
</div>
<div>
<div>5. WebSocket Connection Leaks</div>
<div>Not properly closing WebSocket connections leads to resource exhaustion. Implement heartbeats to detect dead connections and clean up resources on disconnect.</div>
</div>
<div>
<div>6. SSE Without Heartbeats</div>
<div>Proxies and load balancers may close idle SSE connections. Send periodic heartbeat events to keep connections alive.</div>
</div>
<hr />
<h2 id="3-level-interview-questions">3-Level Interview Questions</h2>
<h3 id="level-1-fundamentals">Level 1: Fundamentals</h3>
<div>
<p><strong>Q1: What is the difference between stateless and stateful servers?</strong></p>
<p><span>A:</span> A <span>stateless server</span> does not store any client session information between requests - each request must contain all information needed to process it (typically via tokens). A <span>stateful server</span> maintains session data between requests, requiring clients to connect to the same server (sticky sessions).</p>
<p><strong>Why does this matter?</strong> Stateless servers enable horizontal scaling because any server can handle any request. Load balancers can use simple round-robin. Stateful servers require sticky sessions or session replication, making scaling complex.</p>
<details>
<summary>Follow-up L1.1: How do stateless servers handle user authentication?</summary>
<div>
<p><strong>A:</strong> Stateless servers use <span>self-contained tokens</span> (JWT) that include user identity and claims. The token is signed by the server, so any server can validate it without querying a central session store. Each request includes the token in the Authorization header.</p>
<pre><code>                ```
                Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
                ```
</code></pre>
<details>
<summary>Follow-up L1.1.1: What are the security trade-offs of JWT vs server-side sessions?</summary>
<div>
<p><strong>A:</strong><br />
- <strong>JWT Pros</strong>: Stateless, no database lookup, works across services<br />
- <strong>JWT Cons</strong>: Cannot be revoked until expiry (unless using blocklist), token size larger than session ID, secrets must be managed carefully<br />
- <strong>Server-side sessions Pros</strong>: Instant revocation, smaller cookie size, can store arbitrary data<br />
- <strong>Server-side sessions Cons</strong>: Requires shared storage (Redis), database lookup per request, harder to scale</p>
<p><strong>Best Practice</strong>: Use short-lived JWTs (15 min) with refresh tokens stored server-side. This balances scalability with revocation capability.</p>
</div>
</details>
</div>
</details>
<details>
<summary>Follow-up L1.2: When is stateful acceptable?</summary>
<div>
<p><strong>A:</strong> Stateful is acceptable when:</p>
<ol>
<li><strong>Connection state is inherent</strong> - WebSocket connections, game servers, streaming</li>
<li><strong>Performance justifies complexity</strong> - In-memory caching of user data provides significant speedup</li>
<li><strong>Coordination overhead exceeds benefits</strong> - External state stores add latency that matters</li>
</ol>
<details>
<summary>Follow-up L1.2.1: How do you scale stateful WebSocket servers?</summary>
<div>
<p><strong>A:</strong> Use a <span>pub/sub layer</span> (Redis Pub/Sub, Kafka) to coordinate between servers:</p>
<ol>
<li>Each server maintains connections to its clients</li>
<li>When a message needs to reach a user on another server, publish to a channel</li>
<li>All servers subscribe and deliver messages to their local clients</li>
<li>Use sticky sessions at the load balancer (IP hash or cookie) to maintain connection affinity</li>
<li>Store connection metadata (user -&gt; server mapping) in Redis for routing</li>
</ol>
</div>
</details>
</div>
</details>
</div>
<div>
<p><strong>Q2: Explain the REST architectural constraints.</strong></p>
<p><span>A:</span> REST (Representational State Transfer) has six constraints:</p>
<ol>
<li><strong>Client-Server</strong> - Separation of concerns between UI and data</li>
<li><strong>Stateless</strong> - Each request contains all context needed</li>
<li><strong>Cacheable</strong> - Responses must define cacheability</li>
<li><strong>Uniform Interface</strong> - Standardized resource identification and manipulation</li>
<li><strong>Layered System</strong> - Client doesn't know if connected directly to server</li>
<li><strong>Code on Demand</strong> (optional) - Server can send executable code</li>
</ol>
<details>
<summary>Follow-up L1.1: What is HATEOAS and why is it rarely implemented?</summary>
<div>
<p><strong>A:</strong> <span>HATEOAS</span> (Hypermedia as the Engine of Application State) means responses include links to related actions, making APIs self-documenting and discoverable.</p>
<pre><code>                ```json
                {
                &quot;order_id&quot;: 123,
                &quot;status&quot;: &quot;pending&quot;,
                &quot;_links&quot;: {
                &quot;self&quot;: &quot;/orders/123&quot;,
                &quot;cancel&quot;: {&quot;href&quot;: &quot;/orders/123&quot;, &quot;method&quot;: &quot;DELETE&quot;},
                &quot;pay&quot;: {&quot;href&quot;: &quot;/orders/123/payment&quot;, &quot;method&quot;: &quot;POST&quot;}
                }
                }
                ```
</code></pre>
<p><strong>Why rarely implemented:</strong><br />
- Adds response size and complexity<br />
- Most clients are tightly coupled anyway (mobile apps)<br />
- GraphQL provides similar flexibility differently<br />
- Caching becomes harder with dynamic links</p>
<details>
<summary>Follow-up L1.1.1: How does GraphQL solve the problems HATEOAS addresses?</summary>
<div>
<p><strong>A:</strong> GraphQL addresses discoverability and flexibility differently:<br />
- <strong>Schema introspection</strong> replaces HATEOAS links for discoverability<br />
- <strong>Client-specified queries</strong> let clients request exactly what they need<br />
- <strong>Single endpoint</strong> with typed schema vs multiple REST endpoints<br />
- <strong>Strong typing</strong> provides compile-time validation</p>
<p>However, GraphQL has trade-offs: harder to cache at HTTP level, complexity in authorization, potential for expensive queries (N+1 problems without DataLoader).</p>
</div>
</details>
</div>
</details>
</div>
<h3 id="level-2-design--trade-offs">Level 2: Design &amp; Trade-offs</h3>
<div>
<p><strong>Q3: When would you choose WebSocket over SSE?</strong></p>
<p><span>A:</span> Choose <span>WebSocket</span> when you need:<br />
- <strong>Bidirectional communication</strong> (chat, gaming)<br />
- <strong>Binary data transfer</strong> (file sharing, video)<br />
- <strong>Lowest possible latency</strong> (trading, gaming)<br />
- <strong>High message frequency</strong> in both directions</p>
<p>Choose <span>SSE</span> when:<br />
- Server-to-client push is sufficient (notifications, feeds)<br />
- You want built-in reconnection with last-event-id<br />
- Standard HTTP infrastructure compatibility matters<br />
- Simplicity is preferred</p>
<details>
<summary>Follow-up L2.1: How does SSE handle reconnection and missed messages?</summary>
<div>
<p><strong>A:</strong> SSE has built-in reconnection support:</p>
<ol>
<li>
<p><strong>Auto-reconnect</strong>: Browser automatically reconnects on disconnect</p>
</li>
<li>
<p><strong>Last-Event-ID</strong>: Server assigns IDs to events; on reconnect, browser sends <code>Last-Event-ID</code> header</p>
</li>
<li>
<p><strong>Server resumes</strong>: Server queries stored events after that ID and resends</p>
<pre><code>           ```python
           def format_sse(data, event_id):
           return f&quot;id: {event_id}\ndata: {json.dumps(data)}\n\n&quot;

           # On reconnect, client sends: Last-Event-ID: 12345
           last_id = request.headers.get('Last-Event-ID')
           if last_id:
           missed = Event.query.filter(Event.id &gt; last_id).all()
           for event in missed:
           yield format_sse(event.data, event.id)
           ```
</code></pre>
</li>
</ol>
<details>
<summary>Follow-up L2.1.1: What are the challenges of implementing this at scale?</summary>
<div>
<p><strong>A:</strong> Challenges at scale:</p>
<ol>
<li><strong>Event storage</strong>: Must persist events for replay window (Redis with TTL, or dedicated event store)</li>
<li><strong>Ordering</strong>: Events must be globally ordered across servers (use Redis atomic increment for IDs)</li>
<li><strong>Memory</strong>: Long replay windows increase storage requirements</li>
<li><strong>Cleanup</strong>: Need efficient event expiration</li>
<li><strong>Load balancer affinity</strong>: May not be needed since reconnection includes last-event-id</li>
</ol>
<p><strong>Solutions:</strong><br />
- Use Kafka for ordered, persistent event storage with consumer offset tracking<br />
- Implement sliding window (last N events or time-based)<br />
- Use consistent hashing for event storage partitioning</p>
</div>
</details>
</div>
</details>
<details>
<summary>Follow-up L2.2: What infrastructure considerations differ between SSE and WebSocket?</summary>
<div>
<p><strong>A:</strong> Key infrastructure differences:</p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>SSE</th>
<th>WebSocket</th>
</tr>
</thead>
<tbody>
<tr>
<td>Load Balancer</td>
<td>Standard HTTP, no special config</td>
<td>Needs WS support, sticky sessions</td>
</tr>
<tr>
<td>Proxy/CDN</td>
<td>Works with HTTP proxies</td>
<td>May need configuration</td>
</tr>
<tr>
<td>Firewalls</td>
<td>Port 80/443, rarely blocked</td>
<td>May be blocked by corporate firewalls</td>
</tr>
<tr>
<td>TLS</td>
<td>Standard HTTPS</td>
<td>WSS requires similar setup</td>
</tr>
<tr>
<td>Connection limits</td>
<td>HTTP/2 multiplexing helps</td>
<td>Each connection is separate</td>
</tr>
</tbody>
</table>
<details>
<summary>Follow-up L2.2.1: How do you handle WebSocket connections behind corporate proxies that don't support upgrades?</summary>
<div>
<p><strong>A:</strong> Strategies for WebSocket fallback:</p>
<ol>
<li><strong>Long polling fallback</strong>: Libraries like Socket.IO auto-fallback to HTTP long polling</li>
<li><strong>HTTPS WebSocket</strong>: WSS (port 443) is less likely to be blocked than WS (port 80)</li>
<li><strong>Tunneling</strong>: Wrap WebSocket in HTTPS CONNECT tunnel</li>
<li><strong>Alternative ports</strong>: Some proxies allow WebSocket on non-standard ports</li>
</ol>
<p><strong>Socket.IO example:</strong><br />
<code>javascript const io = require('socket.io')(server, { transports: ['websocket', 'polling'],  // Try WS first, fallback to polling upgrade: true  // Upgrade polling to WS when possible }); </code></p>
</div>
</details>
</div>
</details>
</div>
<div>
<p><strong>Q4: Design a notification system that delivers messages in real-time but also handles offline users.</strong></p>
<p><span>A:</span> This requires combining <span>real-time delivery</span> with <span>persistent storage</span>:</p>
<ol>
<li><strong>Dual write</strong>: When notification created, write to database AND publish to real-time channel</li>
<li><strong>Online users</strong>: Receive via WebSocket/SSE immediately</li>
<li><strong>Offline users</strong>: On next connection, query database for unread notifications</li>
<li><strong>Read receipts</strong>: Mark delivered when user acknowledges</li>
</ol>
<details>
<summary>Follow-up L2.1: How do you ensure exactly-once delivery?</summary>
<div>
<p><strong>A:</strong> True exactly-once is impossible in distributed systems; achieve <span>effectively-once</span> through:</p>
<ol>
<li>
<p><strong>Idempotent delivery</strong>: Include notification ID, client deduplicates</p>
</li>
<li>
<p><strong>Acknowledgment</strong>: Client ACKs receipt, server marks delivered</p>
</li>
<li>
<p><strong>Outbox pattern</strong>: Write notification + outbox entry in same transaction, separate process publishes</p>
<pre><code>           ```python
           # Outbox pattern
           with db.transaction():
           notification = Notification.create(user_id, content)
           OutboxEvent.create(
           aggregate_id=notification.id,
           event_type='notification.created',
           payload=notification.to_dict()
           )

           # Separate publisher reads outbox, publishes, marks processed
           ```
</code></pre>
</li>
</ol>
<details>
<summary>Follow-up L2.1.1: How does the outbox pattern prevent duplicates when the publisher crashes?</summary>
<div>
<p><strong>A:</strong> The outbox pattern provides at-least-once semantics:</p>
<ol>
<li>Publisher processes outbox entries in order</li>
<li>For each entry: publish to message queue, mark as processed</li>
<li>If crash between publish and mark, entry reprocessed on restart</li>
<li>Consumers must be idempotent (dedupe by ID)</li>
</ol>
<p><strong>Preventing duplicates at consumer:</strong><br />
```python<br />
def handle_notification(event):<br />
# Idempotent check<br />
if Delivery.exists(event.id):<br />
return  # Already processed</p>
<pre><code>                    # Process
                    deliver_to_user(event)

                    # Mark processed (last, so crash causes redelivery)
                    Delivery.create(event.id)
                    ```
</code></pre>
<p><strong>Ordering guarantee</strong>: Single partition per user ensures order within user's notifications.</p>
</div>
</details>
</div>
</details>
</div>
<h3 id="level-3-advanced-architecture">Level 3: Advanced Architecture</h3>
<div>
<p><strong>Q5: Design the client-server architecture for a real-time collaborative document editor like Google Docs.</strong></p>
<p><span>A:</span> This is a complex system requiring:</p>
<ol>
<li><strong>Real-time sync</strong>: WebSockets for bidirectional communication</li>
<li><strong>Conflict resolution</strong>: Operational Transformation (OT) or CRDTs</li>
<li><strong>Persistence</strong>: Database for document storage, change log</li>
<li><strong>Presence</strong>: Show who's viewing/editing where</li>
</ol>
<details>
<summary>Follow-up L3.1: How would you implement operational transformation at scale?</summary>
<div>
<p><strong>A:</strong> <span>Operational Transformation (OT)</span> transforms concurrent operations to maintain consistency:</p>
<ol>
<li><strong>Client sends operation</strong> with base version</li>
<li><strong>Server transforms</strong> operation against any concurrent operations</li>
<li><strong>Server broadcasts</strong> transformed operation to other clients</li>
<li><strong>Clients transform</strong> their pending operations against received operations</li>
</ol>
<p><strong>At scale:</strong><br />
- <strong>Single document owner</strong>: Route all ops for a document to same server (consistent hashing)<br />
- <strong>Operation log</strong>: Store operation history for transformation and replay<br />
- <strong>Checkpointing</strong>: Periodically snapshot document state to limit replay</p>
<details>
<summary>Follow-up L3.1.1: What are the trade-offs between OT and CRDTs for this use case?</summary>
<div>
<p><strong>A:</strong></p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>OT</th>
<th>CRDT</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Complexity</strong></td>
<td>Complex transformation logic</td>
<td>Complex data structures</td>
</tr>
<tr>
<td><strong>Server requirement</strong></td>
<td>Needs central server for ordering</td>
<td>Can work peer-to-peer</td>
</tr>
<tr>
<td><strong>Latency</strong></td>
<td>Server round-trip for confirmation</td>
<td>Immediate local application</td>
</tr>
<tr>
<td><strong>Memory</strong></td>
<td>Store operation log</td>
<td>Metadata per element</td>
</tr>
<tr>
<td><strong>Consistency</strong></td>
<td>Strong (with server)</td>
<td>Eventual</td>
</tr>
</tbody>
</table>
<p><strong>Google Docs uses OT</strong> because:<br />
- Central server already needed for auth/storage<br />
- Strong consistency preferred for documents<br />
- Well-understood implementation from Wave</p>
<p><strong>CRDTs better for:</strong><br />
- Peer-to-peer apps (offline-first)<br />
- Eventually consistent systems<br />
- When server coordination is expensive</p>
</div>
</details>
</div>
</details>
<details>
<summary>Follow-up L3.2: How do you handle offline editing?</summary>
<div>
<p><strong>A:</strong> Offline editing requires:</p>
<ol>
<li>
<p><strong>Local storage</strong>: IndexedDB stores document and operation queue</p>
</li>
<li>
<p><strong>Operation queue</strong>: Collect operations while offline</p>
</li>
<li>
<p><strong>Sync on reconnect</strong>: Send queued operations, receive missed operations</p>
</li>
<li>
<p><strong>Conflict resolution</strong>: Transform queued ops against received ops</p>
<pre><code>           ```javascript
           class OfflineManager {
           async queueOperation(op) {
           await this.operationQueue.add(op);
           this.applyLocally(op);  // Optimistic local update
           }

           async onReconnect() {
           const queued = await this.operationQueue.getAll();
           const serverVersion = await this.fetchServerVersion();

           // Transform queued ops against server state
           const transformed = this.transformAgainst(queued, serverVersion);

           // Send transformed operations
           await this.sendOperations(transformed);
           await this.operationQueue.clear();
           }
           }
           ```
</code></pre>
</li>
</ol>
<details>
<summary>Follow-up L3.2.1: How do you handle the case where two users make conflicting edits to the same paragraph offline?</summary>
<div>
<p><strong>A:</strong> This is the hardest case. Options:</p>
<ol>
<li><strong>Automatic merge</strong> (OT/CRDT): Both edits preserved, may result in nonsensical text</li>
<li><strong>Last-write-wins</strong>: Lose one user's changes (bad UX)</li>
<li><strong>Branch and merge</strong>: Create conflict branches, prompt user to resolve</li>
<li><strong>Paragraph-level locking</strong>: Warn when both editing same section</li>
</ol>
<p><strong>Best practice for documents:</strong><br />
- Use character-level OT (Google Docs approach)<br />
- Both edits merge character by character<br />
- Result may be weird but no data lost<br />
- Show &quot;concurrent edit&quot; notification<br />
- Keep undo history for recovery</p>
<pre><code>                    ```
                    User A (offline): &quot;Hello World&quot; -&gt; &quot;Hello Beautiful World&quot;
                    User B (offline): &quot;Hello World&quot; -&gt; &quot;Hello Big World&quot;

                    After sync (character-level merge):
                    &quot;Hello Beautiful Big World&quot; or &quot;Hello Big Beautiful World&quot;
                    (depends on operation ordering)
                    ```
</code></pre>
</div>
</details>
</div>
</details>
</div>
<div>
<p><strong>Q6: How would you design a system that needs to handle 10 million concurrent WebSocket connections?</strong></p>
<p><span>A:</span> This requires careful architecture across multiple dimensions:</p>
<ol>
<li><strong>Connection handling</strong>: ~100K connections per server, need 100+ servers</li>
<li><strong>Message routing</strong>: Pub/sub layer (Redis Cluster, Kafka)</li>
<li><strong>State management</strong>: External store for session data</li>
<li><strong>Load balancing</strong>: Layer 4 (TCP) for WebSocket, sticky sessions</li>
</ol>
<details>
<summary>Follow-up L3.1: How do you handle the memory requirements for 100K connections per server?</summary>
<div>
<p><strong>A:</strong> Memory optimization strategies:</p>
<ol>
<li><strong>Event-driven architecture</strong>: Use epoll/kqueue, not thread-per-connection</li>
<li><strong>Minimize per-connection state</strong>: Store only connection ID and user ID in memory</li>
<li><strong>Buffer management</strong>: Small read/write buffers, expand on demand</li>
<li><strong>Language choice</strong>: Go/Rust/C++ for lower memory footprint than JVM/Node</li>
</ol>
<p><strong>Calculation:</strong><br />
- Base connection: ~10KB (TCP buffers + metadata)<br />
- 100K connections: ~1GB<br />
- Application state: +2KB per connection = +200MB<br />
- Safety margin: 2x = ~2.5GB per 100K connections</p>
<details>
<summary>Follow-up L3.1.1: How do you handle graceful shutdown without dropping 100K connections?</summary>
<div>
<p><strong>A:</strong> Graceful shutdown strategy:</p>
<ol>
<li>
<p><strong>Stop accepting new connections</strong>: Remove from load balancer pool</p>
</li>
<li>
<p><strong>Drain existing connections</strong>:<br />
- Send &quot;reconnect&quot; message to clients with alternate server<br />
- Wait for clients to disconnect (timeout: 30s)</p>
</li>
<li>
<p><strong>Force close</strong>: Remaining connections closed</p>
</li>
<li>
<p><strong>Health checks</strong>: Load balancer detects draining status</p>
<pre><code>               ```go
               func gracefulShutdown(server *WebSocketServer) {
               // 1. Stop accepting new
               server.StopAccepting()

               // 2. Notify clients
               server.BroadcastReconnect(&quot;ws://other-server:8080&quot;)

               // 3. Wait for drain
               deadline := time.After(30 * time.Second)
               for server.ConnectionCount() &gt; 0 {
               select {
               case &lt;-deadline:
               log.Warn(&quot;Forcing shutdown with %d connections&quot;,
               server.ConnectionCount())
               return
               case &lt;-time.After(100 * time.Millisecond):
               continue
               }
               }
               }
               ```
</code></pre>
</li>
</ol>
<p><strong>Client-side</strong>: Clients should handle reconnect messages gracefully, reconnecting with backoff.</p>
</div>
</details>
</div>
</details>
<details>
<summary>Follow-up L3.2: How do you route a message to a specific user when they could be on any of 100 servers?</summary>
<div>
<p><strong>A:</strong> Two main approaches:</p>
<p><strong>1. User-Server Registry (Redis)</strong><br />
```python<br />
# On connect<br />
redis.set(f&quot;user:{user_id}:server&quot;, server_id, ex=connection_timeout)</p>
<pre><code>                # To send message
                server_id = redis.get(f&quot;user:{user_id}:server&quot;)
                if server_id:
                redis.publish(f&quot;server:{server_id}&quot;, message)
                ```
</code></pre>
<p><strong>2. Pub/Sub Per-User Channel</strong><br />
```python<br />
# Each server subscribes to its user channels<br />
async def on_connect(user_id, websocket):<br />
await redis.subscribe(f&quot;user:{user_id}&quot;)</p>
<pre><code>                # To send message (no lookup needed)
                await redis.publish(f&quot;user:{user_id}&quot;, message)
                # Server subscribed to that user's channel receives and delivers
                ```
</code></pre>
<details>
<summary>Follow-up L3.2.1: What are the trade-offs of Redis Pub/Sub vs Kafka for this use case?</summary>
<div>
<p><strong>A:</strong></p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Redis Pub/Sub</th>
<th>Kafka</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Delivery</strong></td>
<td>Fire-and-forget</td>
<td>Persistent, replayable</td>
</tr>
<tr>
<td><strong>Ordering</strong></td>
<td>Per connection</td>
<td>Per partition</td>
</tr>
<tr>
<td><strong>Scale</strong></td>
<td>Single node limit ~1M msgs/s</td>
<td>Horizontal scale, millions msgs/s</td>
</tr>
<tr>
<td><strong>Memory</strong></td>
<td>Messages in memory briefly</td>
<td>Disk-based log</td>
</tr>
<tr>
<td><strong>Offline users</strong></td>
<td>Message lost</td>
<td>Can replay from offset</td>
</tr>
<tr>
<td><strong>Complexity</strong></td>
<td>Simple</td>
<td>Complex (topics, partitions, consumers)</td>
</tr>
</tbody>
</table>
<p><strong>Recommendation:</strong><br />
- <strong>Redis Pub/Sub</strong>: Real-time only, offline handled separately<br />
- <strong>Kafka</strong>: Need persistence, ordering, replay (chat history, audit)<br />
- <strong>Hybrid</strong>: Kafka for persistence, Redis for real-time routing</p>
<p>For 10M connections with reliable delivery:</p>
<ol>
<li>Write message to Kafka (persistent)</li>
<li>Publish to Redis (real-time delivery attempt)</li>
<li>On reconnect, consumer reads Kafka from last offset</li>
</ol>
</div>
</details>
</div>
</details>
</div>
<hr />
<h2 id="client-server-vs-alternatives">Client-Server vs Alternatives</h2>
<table>
<thead>
<tr>
<th>Architecture</th>
<th>Description</th>
<th>When to Use</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Client-Server</strong></td>
<td>Clear client/server roles</td>
<td>Most web/mobile apps</td>
</tr>
<tr>
<td><strong>Peer-to-Peer</strong></td>
<td>All nodes equal</td>
<td>File sharing, blockchain</td>
</tr>
<tr>
<td><strong>Serverless</strong></td>
<td>No managed servers</td>
<td>Event-driven, sporadic traffic</td>
</tr>
<tr>
<td><strong>Edge Computing</strong></td>
<td>Processing at network edge</td>
<td>IoT, low-latency needs</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="related-topics">Related Topics</h2>
<pre><code>          - [[Load Balancing]](/topics/system-design/load-balancing) - Distributing requests across servers
          - [[API Design]](/topics/system-design/api-design) - REST, GraphQL, and API patterns
          - [[API Gateway]](/topics/system-design/api-gateway) - Central entry point for microservices
          - [[Caching]](/topics/system-design/caching) - Reducing server load and latency
          - [[Network Protocols]](/topics/system-design/network-protocols) - TCP, UDP, HTTP/2, QUIC
          - [[Message Queues]](/topics/system-design/message-queues) - Async communication patterns
          - [[Circuit Breaker]](/topics/system-design/circuit-breaker) - Handling failures gracefully
          - [[Rate Limiting]](/topics/system-design/rate-limiting) - Protecting servers from overload
          - [[Connection Pooling]](/topics/system-design/connection-pooling) - Efficient resource management
</code></pre>
