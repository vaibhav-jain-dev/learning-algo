<h1 id="rate-limiting">Rate Limiting</h1>
<nav class="toc">
<h2 id="table-of-contents">Table of Contents</h2>
<div>
<div>
<h4>Fundamentals</h4>
<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#why-this-matters">Why This Matters</a></li>
<li><a href="#how-it-works">How It Works</a></li>
<li><a href="#rate-limit-response-headers">Response Headers</a></li>
</ul>
</div>
<div>
<h4>Algorithms</h4>
<ul>
<li><a href="#rate-limiting-algorithms">Algorithm Comparison</a></li>
<li><a href="#token-bucket-algorithm">Token Bucket</a></li>
<li><a href="#leaky-bucket-algorithm">Leaky Bucket</a></li>
<li><a href="#sliding-window-algorithm">Sliding Window</a></li>
<li><a href="#fixed-window-problem">Fixed Window Problem</a></li>
</ul>
</div>
<div>
<h4>Implementation</h4>
<ul>
<li><a href="#implementation">Code Examples</a></li>
<li><a href="#distributed-rate-limiter-with-redis">Distributed with Redis</a></li>
<li><a href="#http-middleware">HTTP Middleware</a></li>
</ul>
</div>
<div>
<h4>Production Concerns</h4>
<ul>
<li><a href="#edge-cases-failure-modes">Edge Cases & Failure Modes</a></li>
<li><a href="#real-life-failure-story">Real-Life Failure Story</a></li>
<li><a href="#common-mistakes">Common Mistakes</a></li>
<li><a href="#interview-questions">Interview Questions</a></li>
</ul>
</div>
</div>
</nav>
<hr />
<h2 id="overview">Overview</h2>
<p>Rate limiting controls how many requests a client can make to your API within a specific time window. Think of it like a bouncer at a club - they let people in at a controlled pace to prevent overcrowding, regardless of how many people are waiting in line.</p>
<p>When a client exceeds their limit, the server responds with HTTP 429 (Too Many Requests) and typically includes headers indicating when they can retry.</p>
<hr />
<h2 id="why-this-matters">Why This Matters</h2>
<h3 id="real-company-examples">Real Company Examples</h3>
<div>
<h4>Companies Using Rate Limiting</h4>
<div>
<div>
<div>Twitter/X - API Access</div>
<div>Twitter's API has strict rate limits (15-900 requests per 15-minute window depending on endpoint). This prevents bots from scraping all tweets and ensures fair access for legitimate developers.</div>
</div>
<div>
<div>Stripe - Payment APIs</div>
<div>Stripe rate limits API requests to protect against runaway scripts that could create thousands of charges. Different endpoints have different limits based on cost and risk.</div>
</div>
<div>
<div>GitHub - API and Git Operations</div>
<div>GitHub limits authenticated users to 5,000 requests/hour. During CI/CD spikes, this prevents a single user's pipeline from overwhelming shared infrastructure.</div>
</div>
</div>
</div>
<p><strong>Why Rate Limit?</strong></p>
<ul>
<li><strong>Protect infrastructure</strong>: Prevent a single user from consuming all server resources</li>
<li><strong>Ensure fairness</strong>: Give all users equitable access to limited resources</li>
<li><strong>Control costs</strong>: Limit expensive operations (AI inference, third-party API calls)</li>
<li><strong>Prevent abuse</strong>: Make brute-force attacks and scraping impractical</li>
<li><strong>Enforce business models</strong>: Differentiate free vs paid tier capabilities</li>
</ul>
<hr />
<h2 id="how-it-works">How It Works</h2>
<h3 id="rate-limiting-flow">Rate Limiting Flow</h3>
<div class="diagram-container">
<div class="flow-diagram">
<div class="flow-row">
<div class="flow-box primary">
<div class="flow-box-title">Client Request</div>
<div class="flow-box-subtitle">API Call Arrives</div>
</div>
</div>
<div class="flow-arrow vertical">&#9660;</div>
<div class="flow-row">
<div class="flow-box info">
<div class="flow-box-title">1. Identify Client</div>
<div class="flow-box-subtitle">API Key / User ID / IP</div>
</div>
</div>
<div class="flow-arrow vertical">&#9660;</div>
<div class="flow-row">
<div class="flow-box warning">
<div class="flow-box-title">2. Check Limit</div>
<div class="flow-box-subtitle">Query Rate Limiter</div>
</div>
</div>
<div class="flow-arrow vertical">&#9660;</div>
<div class="flow-row">
<div>
<div class="flow-box success">
<div class="flow-box-title">Under Limit</div>
<div class="flow-box-subtitle">Increment Counter</div>
</div>
<div class="flow-arrow vertical">&#9660;</div>
<div class="flow-box success">
<div class="flow-box-title">Process Request</div>
<div class="flow-box-subtitle">Return 200 + Headers</div>
</div>
</div>
<div>
<div class="flow-box error">
<div class="flow-box-title">Over Limit</div>
<div class="flow-box-subtitle">Reject Request</div>
</div>
<div class="flow-arrow vertical">&#9660;</div>
<div class="flow-box error">
<div class="flow-box-title">Return 429</div>
<div class="flow-box-subtitle">+ Retry-After Header</div>
</div>
</div>
</div>
</div>
</div>
<h3 id="rate-limit-response-headers">Rate Limit Response Headers</h3>
<div>
<h4>Standard Rate Limit Headers</h4>
<div>
<div><span>X-RateLimit-Limit:</span> 100</div>
<div>Maximum requests allowed in window</div>
<div><span>X-RateLimit-Remaining:</span> 45</div>
<div>Requests remaining in current window</div>
<div><span>X-RateLimit-Reset:</span> 1642089600</div>
<div>Unix timestamp when window resets</div>
<div><span>Retry-After:</span> 30</div>
<div>Seconds until client can retry (only on 429)</div>
</div>
</div>
<hr />
<h2 id="rate-limiting-algorithms">Rate Limiting Algorithms</h2>
<div>
<h4>Algorithm Comparison</h4>
<div>
<table>
<tr>
<th>Algorithm</th>
<th>Allows Bursts</th>
<th>Memory</th>
<th>Accuracy</th>
<th>Best For</th>
</tr>
<tr>
<td>Token Bucket</td>
<td>Yes (controlled)</td>
<td>O(1)</td>
<td>Good</td>
<td>APIs with burst tolerance</td>
</tr>
<tr>
<td>Leaky Bucket</td>
<td>No</td>
<td>O(n)</td>
<td>Good</td>
<td>Smooth output rate</td>
</tr>
<tr>
<td>Fixed Window</td>
<td>Edge bursts</td>
<td>O(1)</td>
<td>Low</td>
<td>Simple use cases</td>
</tr>
<tr>
<td>Sliding Window Log</td>
<td>No</td>
<td>O(n)</td>
<td>High</td>
<td>Precise tracking</td>
</tr>
<tr>
<td>Sliding Window Counter</td>
<td>Weighted</td>
<td>O(1)</td>
<td>High</td>
<td>Production APIs</td>
</tr>
</table>
</div>
</div>
<h3 id="token-bucket-algorithm">Token Bucket Algorithm</h3>
<div class="diagram-container">
<div class="flow-diagram">
<div>
<div>Token Bucket Visualization</div>
<div>Tokens refill at a constant rate; requests consume tokens</div>
</div>
<div>
<div>
<div class="flow-box success">
<div class="flow-box-title">Refill</div>
<div class="flow-box-subtitle">10 tokens/sec</div>
</div>
<div class="flow-arrow horizontal">&#9654;</div>
<div>
<div>45</div>
<div>tokens available</div>
<div>capacity: 100</div>
</div>
<div class="flow-arrow horizontal">&#9654;</div>
<div class="flow-box warning">
<div class="flow-box-title">Request</div>
<div class="flow-box-subtitle">-1 token</div>
</div>
</div>
<div>
<div>
<div>
<div>Tokens Available</div>
<div>Request allowed</div>
</div>
<div>
<div>No Tokens</div>
<div>Request rejected (429)</div>
</div>
</div>
</div>
<div>
<div><strong>Key Insight:</strong> Allows burst traffic up to bucket capacity, then enforces average rate. Great for APIs where occasional spikes are acceptable.</div>
</div>
</div>
</div>
</div>
<h3 id="leaky-bucket-algorithm">Leaky Bucket Algorithm</h3>
<div class="diagram-container">
<div class="flow-diagram">
<div>
<div>Leaky Bucket Visualization</div>
<div>Requests queue up; processed at a constant rate</div>
</div>
<div>
<div>
<div>
<div class="flow-box primary">
<div class="flow-box-title">Incoming</div>
<div class="flow-box-subtitle">Requests</div>
</div>
<div class="flow-arrow vertical">&#9660;</div>
</div>
<div>
<div>QUEUE</div>
<div>
<div>Request 1</div>
<div>Request 2</div>
<div>Request 3</div>
</div>
<div></div>
</div>
<div>
<div>&#9654;</div>
<div>Constant<br/>drain rate</div>
</div>
<div>
<div class="flow-box success">
<div class="flow-box-title">Processed</div>
<div class="flow-box-subtitle">10 req/sec</div>
</div>
</div>
</div>
<div>
<div>
<div><strong>Queue Full:</strong> New requests rejected immediately</div>
</div>
<div>
<div><strong>Best For:</strong> Smooth output rate to protect downstream services</div>
</div>
</div>
</div>
</div>
</div>
<h3 id="sliding-window-algorithm">Sliding Window Algorithm</h3>
<div class="diagram-container">
<div class="flow-diagram">
<div>
<div>Sliding Window Counter</div>
<div>Weighted average of current and previous window</div>
</div>
<div>
<div>
<div>
<div>Previous Window</div>
<div>
<div>90</div>
<div>requests</div>
</div>
</div>
<div>
<div>Current Window</div>
<div>
<div>10</div>
<div>requests</div>
</div>
</div>
</div>
<div>
<div>
<div>Window Progress: 25% into current window</div>
<div>
<div></div>
</div>
</div>
</div>
<div>
<div>
<div>Weighted Count Formula:</div>
<div>count = prev * (1 - progress) + curr</div>
<div>= 90 * 0.75 + 10 = <strong>77.5</strong></div>
<div>Under 100 limit - Request allowed!</div>
</div>
</div>
</div>
</div>
</div>
<h3 id="fixed-window-problem">Fixed Window Problem</h3>
<div>
<h4>Why Fixed Windows Have Edge Burst Problems</h4>
<div>
<div>The Problem</div>
<div>
<div>
<div>Window 1 (0:00-1:00)</div>
<div>90 requests at 0:59</div>
</div>
<div>
<div>Window 2 (1:00-2:00)</div>
<div>100 requests at 1:01</div>
</div>
</div>
<div>
<div>190 requests in 2 seconds - nearly 2x the intended limit!</div>
</div>
</div>
<div>
<div>Solution: Sliding Window Counter</div>
<div>
  Weight the previous window based on how much of it overlaps with the current sliding window.
<div>
  count = prev_window * (1 - progress) + curr_window<br>
  count = 90 * 0.75 + 10 = 77.5 (under 100 limit)
</div>
</div>
</div>
</div>
<hr />
<h2 id="edge-cases-failure-modes">Edge Cases & Failure Modes</h2>
<div class="diagram-container">
<div class="flow-diagram">
<div>
<div>Rate Limiter Failure Decision Tree</div>
<div>What happens when the rate limiter itself fails?</div>
</div>
<div class="flow-row">
<div class="flow-box error">
<div class="flow-box-title">Rate Limiter Unavailable</div>
<div class="flow-box-subtitle">Redis down, network partition</div>
</div>
</div>
<div class="flow-arrow vertical">&#9660;</div>
<div class="flow-row">
<div>
<div class="flow-box success">
<div class="flow-box-title">Fail Open</div>
<div class="flow-box-subtitle">Allow all requests</div>
</div>
<div>
<div><strong>Pro:</strong> High availability</div>
<div><strong>Con:</strong> No protection</div>
</div>
</div>
<div>
<div class="flow-box error">
<div class="flow-box-title">Fail Closed</div>
<div class="flow-box-subtitle">Reject all requests</div>
</div>
<div>
<div><strong>Pro:</strong> Maximum safety</div>
<div><strong>Con:</strong> Service outage</div>
</div>
</div>
<div>
<div class="flow-box warning">
<div class="flow-box-title">Local Fallback</div>
<div class="flow-box-subtitle">Use cached limits</div>
</div>
<div>
<div><strong>Pro:</strong> Balanced approach</div>
<div><strong>Con:</strong> Stale data</div>
</div>
</div>
</div>
</div>
</div>
<div>
<h4>Critical Edge Cases</h4>
<div>
<div>
<div>1. Clock Skew in Distributed Systems</div>
<div>Different servers have different times, causing inconsistent window calculations.</div>
<div>
<div><strong>Solution:</strong> Use centralized time source (Redis server time) or NTP synchronization. Never rely on application server clocks.</div>
</div>
</div>
<div>
<div>2. Race Conditions in Check-and-Increment</div>
<div>Multiple requests check limit simultaneously before any increment happens.</div>
<div>
<div><strong>Solution:</strong> Use atomic operations (Redis Lua scripts, INCR with TTL) to ensure check and increment happen atomically.</div>
</div>
</div>
<div>
<div>3. Thundering Herd on Window Reset</div>
<div>All queued requests flood in when the rate limit window resets.</div>
<div>
<div><strong>Solution:</strong> Use sliding windows instead of fixed windows, or add jitter to client retry logic.</div>
</div>
</div>
<div>
<div>4. Hot Key Problem</div>
<div>One client (e.g., large enterprise customer) creates disproportionate load on a single Redis key.</div>
<div>
<div><strong>Solution:</strong> Shard rate limit keys across multiple Redis instances, or use local caching with periodic sync.</div>
</div>
</div>
</div>
</div>
<h3 id="distributed-rate-limiting-challenges">Distributed Rate Limiting Challenges</h3>
<div class="diagram-container">
<div class="flow-diagram">
<div>
<div>Distributed Rate Limiting Architecture</div>
<div>Multiple app servers sharing rate limit state</div>
</div>
<div>
<div class="flow-row">
<div class="flow-box info">
<div class="flow-box-title">App 1</div>
<div class="flow-box-subtitle">Server</div>
</div>
<div class="flow-box info">
<div class="flow-box-title">App 2</div>
<div class="flow-box-subtitle">Server</div>
</div>
<div class="flow-box info">
<div class="flow-box-title">App 3</div>
<div class="flow-box-subtitle">Server</div>
</div>
</div>
<div class="flow-arrow vertical">&#9660;</div>
<div class="flow-box primary">
<div class="flow-box-title">Redis Cluster</div>
<div class="flow-box-subtitle">Centralized Rate Limit State</div>
</div>
<div>
<div>Consistency vs Performance Tradeoffs</div>
<div>
<div>
<div>Strong Consistency</div>
<div>Every request hits Redis. Higher latency, perfect accuracy.</div>
</div>
<div>
<div>Eventual Consistency</div>
<div>Local cache + sync. Lower latency, allows brief overage.</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div>
<h4>Hybrid Approach: Best of Both Worlds</h4>
<pre><code class="language-python">class HybridRateLimiter:
    &quot;&quot;&quot;
    Combines local caching with Redis for optimal performance.

    - Fast path: Check local cache (sub-ms)
    - Slow path: Verify with Redis (1-5ms)
    - Sync: Periodically update local state
    &quot;&quot;&quot;

    def __init__(self, redis_client, limit, window_seconds):
        self.redis = redis_client
        self.limit = limit
        self.window_seconds = window_seconds
        self.local_cache = {}  # client_id -&gt; (count, last_sync)
        self.sync_interval = 0.1  # 100ms

    def allow(self, client_id: str) -&gt; bool:
        # Fast path: definitely over limit (local knowledge)
        if self._locally_over_limit(client_id):
            return False

        # Slow path: accurate check with Redis
        allowed = self._redis_check_and_increment(client_id)

        # Update local cache
        self._update_local_cache(client_id, allowed)

        return allowed

    def _locally_over_limit(self, client_id: str) -&gt; bool:
        &quot;&quot;&quot;Quick local check - may have false negatives.&quot;&quot;&quot;
        if client_id not in self.local_cache:
            return False
        count, last_sync = self.local_cache[client_id]
        # Allow some headroom for distributed counter lag
        return count &gt;= self.limit * 0.9
</code></pre>
</div>
<hr />
<h2 id="real-life-failure-story">Real-Life Failure Story</h2>
<h3 id="cloudflare-rate-limiting-incident">The Cloudflare Rate Limiting Incident (2020)</h3>
<div>
<h4>What Happened</h4>
<div>
<div>The Incident</div>
<div>
  A misconfigured rate limiter caused Cloudflare to reject legitimate traffic during a major DDoS attack. The rate limiting rules were too aggressive and didn't distinguish between attack traffic and legitimate users, causing a 27-minute outage affecting millions of websites.
</div>
</div>
<div>
<div>Root Cause</div>
<div>
<div>Rate limiting was applied at IP level only</div>
<div>Many legitimate users share IPs (corporate NAT, mobile carriers)</div>
<div>Attack traffic triggered limits that blocked legitimate users</div>
<div>No fallback mechanism or graceful degradation</div>
</div>
</div>
<div>
<div>How They Fixed It</div>
<div>
<div>1. Multi-dimensional rate limiting (IP + User-Agent + behavior patterns)</div>
<div>2. Implemented "fail open" mode for rate limiter failures</div>
<div>3. Added progressive rate limiting (warn, slow, then block)</div>
<div>4. Better monitoring with alerts on high rejection rates</div>
</div>
</div>
</div>
<hr />
<h2 id="implementation">Implementation</h2>
<h3 id="token-bucket-implementation">Token Bucket Algorithm</h3>
<pre><code class="language-python">import time
from threading import Lock
from dataclasses import dataclass
from typing import Tuple, Dict, Optional


@dataclass
class TokenBucketConfig:
    &quot;&quot;&quot;Configuration for a token bucket rate limiter.&quot;&quot;&quot;
    rate: float          # Tokens added per second
    capacity: int        # Maximum tokens (burst size)


class TokenBucket:
    &quot;&quot;&quot;
    Token bucket rate limiter.

    Tokens are added at a fixed rate up to a maximum capacity.
    Each request consumes one token. Requests are rejected when
    no tokens are available.

    Allows controlled bursts up to capacity, then enforces average rate.
    &quot;&quot;&quot;

    def __init__(self, rate: float, capacity: int):
        self.rate = rate              # Tokens per second
        self.capacity = capacity      # Maximum tokens
        self.tokens = capacity        # Current tokens
        self.last_update = time.time()
        self.lock = Lock()

    def allow(self, tokens_required: int = 1) -&gt; Tuple[bool, dict]:
        &quot;&quot;&quot;
        Check if request is allowed and consume tokens if so.

        Returns (allowed, metadata) where metadata includes
        remaining tokens and reset time.
        &quot;&quot;&quot;
        with self.lock:
            now = time.time()

            # Add tokens based on elapsed time
            elapsed = now - self.last_update
            self.tokens = min(
                self.capacity,
                self.tokens + elapsed * self.rate
            )
            self.last_update = now

            if self.tokens &gt;= tokens_required:
                self.tokens -= tokens_required
                return True, {
                    'remaining': int(self.tokens),
                    'limit': self.capacity,
                    'reset': now + (self.capacity - self.tokens) / self.rate
                }

            # Calculate when enough tokens will be available
            wait_time = (tokens_required - self.tokens) / self.rate

            return False, {
                'remaining': 0,
                'limit': self.capacity,
                'reset': now + wait_time,
                'retry_after': wait_time
            }


class TokenBucketRateLimiter:
    &quot;&quot;&quot;
    Rate limiter managing multiple token buckets by client ID.
    &quot;&quot;&quot;

    def __init__(self, rate: float, capacity: int):
        self.rate = rate
        self.capacity = capacity
        self.buckets: Dict[str, TokenBucket] = {}
        self.lock = Lock()

    def allow(self, client_id: str) -&gt; Tuple[bool, dict]:
        &quot;&quot;&quot;Check if request from client is allowed.&quot;&quot;&quot;
        with self.lock:
            if client_id not in self.buckets:
                self.buckets[client_id] = TokenBucket(
                    self.rate, self.capacity
                )

        return self.buckets[client_id].allow()

    def cleanup_expired(self, max_age_seconds: float = 3600):
        &quot;&quot;&quot;Remove buckets that haven't been used recently.&quot;&quot;&quot;
        now = time.time()
        with self.lock:
            expired = [
                cid for cid, bucket in self.buckets.items()
                if now - bucket.last_update &gt; max_age_seconds
            ]
            for cid in expired:
                del self.buckets[cid]
</code></pre>
<h3 id="sliding-window-counter-implementation">Sliding Window Counter</h3>
<pre><code class="language-python">import time
from threading import Lock
from typing import Tuple, Dict


class SlidingWindowCounter:
    &quot;&quot;&quot;
    Sliding window counter rate limiter.

    Combines the memory efficiency of fixed windows with the
    accuracy of sliding windows by weighting the previous window.

    Formula: count = prev_count * (1 - window_progress) + curr_count
    &quot;&quot;&quot;

    def __init__(self, limit: int, window_seconds: int):
        self.limit = limit
        self.window_seconds = window_seconds
        self.counters: Dict[str, Dict[int, int]] = {}
        self.lock = Lock()

    def allow(self, client_id: str) -&gt; Tuple[bool, dict]:
        &quot;&quot;&quot;Check if request from client is allowed.&quot;&quot;&quot;
        now = time.time()
        current_window = int(now // self.window_seconds)
        previous_window = current_window - 1

        # Progress through current window (0.0 to 1.0)
        window_progress = (now % self.window_seconds) / self.window_seconds

        with self.lock:
            if client_id not in self.counters:
                self.counters[client_id] = {}

            client = self.counters[client_id]

            # Get counts for current and previous windows
            prev_count = client.get(previous_window, 0)
            curr_count = client.get(current_window, 0)

            # Calculate weighted count
            weighted_count = prev_count * (1 - window_progress) + curr_count

            if weighted_count &gt;= self.limit:
                # Calculate when limit resets
                reset_time = (current_window + 1) * self.window_seconds

                return False, {
                    'remaining': 0,
                    'limit': self.limit,
                    'reset': reset_time,
                    'retry_after': reset_time - now
                }

            # Increment current window counter
            client[current_window] = curr_count + 1

            # Cleanup old windows
            old_windows = [w for w in client if w &lt; previous_window]
            for w in old_windows:
                del client[w]

            remaining = int(self.limit - weighted_count - 1)
            reset_time = (current_window + 1) * self.window_seconds

            return True, {
                'remaining': max(0, remaining),
                'limit': self.limit,
                'reset': reset_time
            }
</code></pre>
<h3 id="distributed-rate-limiter-with-redis">Distributed Rate Limiter with Redis</h3>
<pre><code class="language-python">import redis
import time
from typing import Tuple


class RedisRateLimiter:
    &quot;&quot;&quot;
    Distributed rate limiter using Redis.

    Uses Lua scripts for atomic operations to ensure
    consistency across multiple application instances.
    &quot;&quot;&quot;

    def __init__(self, redis_client: redis.Redis,
                 limit: int, window_seconds: int):
        self.redis = redis_client
        self.limit = limit
        self.window_seconds = window_seconds

    def allow(self, client_id: str) -&gt; Tuple[bool, dict]:
        &quot;&quot;&quot;
        Check if request is allowed using sliding window counter.

        Uses a Lua script for atomic check-and-increment.
        &quot;&quot;&quot;
        now = time.time()
        current_window = int(now // self.window_seconds)
        previous_window = current_window - 1
        window_progress = (now % self.window_seconds) / self.window_seconds

        # Keys for current and previous windows
        curr_key = f&quot;ratelimit:{client_id}:{current_window}&quot;
        prev_key = f&quot;ratelimit:{client_id}:{previous_window}&quot;

        # Lua script for atomic operation
        lua_script = &quot;&quot;&quot;
        local curr_key = KEYS[1]
        local prev_key = KEYS[2]
        local limit = tonumber(ARGV[1])
        local window_seconds = tonumber(ARGV[2])
        local window_progress = tonumber(ARGV[3])

        local prev_count = tonumber(redis.call('GET', prev_key) or '0')
        local curr_count = tonumber(redis.call('GET', curr_key) or '0')

        local weighted_count = prev_count * (1 - window_progress) + curr_count

        if weighted_count &gt;= limit then
            return {0, curr_count, prev_count}
        end

        local new_count = redis.call('INCR', curr_key)
        redis.call('EXPIRE', curr_key, window_seconds * 2)

        return {1, new_count, prev_count}
        &quot;&quot;&quot;

        result = self.redis.eval(
            lua_script, 2, curr_key, prev_key,
            self.limit, self.window_seconds, window_progress
        )

        allowed = bool(result[0])
        curr_count = result[1]
        prev_count = result[2]

        weighted_count = prev_count * (1 - window_progress) + curr_count
        remaining = max(0, int(self.limit - weighted_count))
        reset_time = (current_window + 1) * self.window_seconds

        metadata = {
            'remaining': remaining,
            'limit': self.limit,
            'reset': reset_time
        }

        if not allowed:
            metadata['retry_after'] = reset_time - now

        return allowed, metadata


class RedisTokenBucket:
    &quot;&quot;&quot;
    Distributed token bucket using Redis.
    &quot;&quot;&quot;

    def __init__(self, redis_client: redis.Redis,
                 rate: float, capacity: int):
        self.redis = redis_client
        self.rate = rate
        self.capacity = capacity

    def allow(self, client_id: str) -&gt; Tuple[bool, dict]:
        &quot;&quot;&quot;
        Check if request is allowed using token bucket algorithm.
        &quot;&quot;&quot;
        key = f&quot;tokenbucket:{client_id}&quot;
        now = time.time()

        lua_script = &quot;&quot;&quot;
        local key = KEYS[1]
        local rate = tonumber(ARGV[1])
        local capacity = tonumber(ARGV[2])
        local now = tonumber(ARGV[3])
        local requested = tonumber(ARGV[4])

        local data = redis.call('HMGET', key, 'tokens', 'last_update')
        local tokens = tonumber(data[1]) or capacity
        local last_update = tonumber(data[2]) or now

        -- Add tokens based on elapsed time
        local elapsed = now - last_update
        tokens = math.min(capacity, tokens + elapsed * rate)

        local allowed = 0
        if tokens &gt;= requested then
            tokens = tokens - requested
            allowed = 1
        end

        -- Update bucket state
        redis.call('HMSET', key, 'tokens', tokens, 'last_update', now)
        redis.call('EXPIRE', key, 3600)

        return {allowed, tokens}
        &quot;&quot;&quot;

        result = self.redis.eval(lua_script, 1, key, self.rate,
                                 self.capacity, now, 1)

        allowed = bool(result[0])
        remaining_tokens = result[1]

        metadata = {
            'remaining': int(remaining_tokens),
            'limit': self.capacity,
            'reset': now + (self.capacity - remaining_tokens) / self.rate
        }

        if not allowed:
            metadata['retry_after'] = (1 - remaining_tokens) / self.rate

        return allowed, metadata
</code></pre>
<h3 id="http-middleware">HTTP Middleware</h3>
<pre><code class="language-python">from functools import wraps
from flask import Flask, request, jsonify, Response


def rate_limit_middleware(limiter, get_client_id=None):
    &quot;&quot;&quot;
    Decorator for rate limiting Flask routes.

    Args:
        limiter: Rate limiter instance
        get_client_id: Function to extract client ID from request
    &quot;&quot;&quot;
    if get_client_id is None:
        def get_client_id(req):
            # Default: use API key or IP address
            return req.headers.get('X-API-Key') or req.remote_addr

    def decorator(f):
        @wraps(f)
        def wrapped(*args, **kwargs):
            client_id = get_client_id(request)
            allowed, metadata = limiter.allow(client_id)

            # Always include rate limit headers
            response_headers = {
                'X-RateLimit-Limit': str(metadata['limit']),
                'X-RateLimit-Remaining': str(metadata['remaining']),
                'X-RateLimit-Reset': str(int(metadata['reset']))
            }

            if not allowed:
                response = jsonify({
                    'error': 'Rate limit exceeded',
                    'retry_after': metadata.get('retry_after', 60)
                })
                response.status_code = 429
                response.headers['Retry-After'] = str(
                    int(metadata.get('retry_after', 60))
                )
                for key, value in response_headers.items():
                    response.headers[key] = value
                return response

            # Process the request
            response = f(*args, **kwargs)

            # Add rate limit headers to successful response
            if isinstance(response, Response):
                for key, value in response_headers.items():
                    response.headers[key] = value
            elif isinstance(response, tuple):
                resp_data, status = response[:2]
                response = jsonify(resp_data)
                response.status_code = status
                for key, value in response_headers.items():
                    response.headers[key] = value

            return response

        return wrapped
    return decorator


# Usage Example
app = Flask(__name__)
limiter = SlidingWindowCounter(limit=100, window_seconds=60)


@app.route('/api/resource')
@rate_limit_middleware(limiter)
def get_resource():
    return {'data': 'resource content'}


# Per-endpoint rate limiting
expensive_limiter = SlidingWindowCounter(limit=10, window_seconds=60)


@app.route('/api/expensive-operation')
@rate_limit_middleware(expensive_limiter)
def expensive_operation():
    return {'result': 'expensive computation'}
</code></pre>
<hr />
<h2 id="interview-questions">Interview Questions</h2>
<div>
<h3 id="q1-distributed-rate-limiting">Q1: How would you implement rate limiting in a distributed system?</h3>
<p><strong>Answer:</strong> Use a centralized store (Redis) with atomic operations to ensure consistency across multiple application instances.</p>
<p>Key considerations:</p>
<ol>
<li><strong>Atomic operations</strong>: Use Lua scripts in Redis to check and increment atomically</li>
<li><strong>Clock synchronization</strong>: Use server time from Redis, not application servers</li>
<li><strong>Failover strategy</strong>: Decide between &quot;fail open&quot; (allow on Redis failure) or &quot;fail closed&quot;</li>
<li><strong>Local caching</strong>: Cache rate limit state locally for performance, sync periodically</li>
</ol>
<pre><code class="language-python"># Hybrid approach: local + Redis
class HybridRateLimiter:
    def allow(self, client_id):
        # Fast path: check local cache
        if self.local_cache.definitely_over_limit(client_id):
            return False

        # Slow path: check Redis for accurate count
        return self.redis_limiter.allow(client_id)
</code></pre>
<h3 id="q2-token-bucket-vs-sliding-window">Q2: Token bucket vs sliding window - when to use each?</h3>
<p><strong>Answer:</strong></p>
<p><strong>Token Bucket:</strong></p>
<ul>
<li>Allows controlled bursts (users can make many requests quickly, then wait)</li>
<li>Good for APIs where burst behavior is acceptable</li>
<li>Memory efficient (just stores token count and timestamp)</li>
<li>Example: Twitter API allows bursts during viral tweets</li>
</ul>
<p><strong>Sliding Window Counter:</strong></p>
<ul>
<li>Stricter limiting, prevents bursts</li>
<li>Better for protecting resources with hard capacity limits</li>
<li>More accurate rate enforcement</li>
<li>Example: Payment APIs where you truly want X requests per minute max</li>
</ul>
<h3 id="q3-shared-ip-rate-limiting">Q3: How do you handle rate limiting for users behind shared IPs (NAT)?</h3>
<p><strong>Answer:</strong> Use multiple identification dimensions:</p>
<ol>
<li><strong>Primary</strong>: API key or user ID (authenticated requests)</li>
<li><strong>Secondary</strong>: IP address + User-Agent hash</li>
<li><strong>Behavioral</strong>: Request patterns, endpoints accessed</li>
</ol>
<pre><code class="language-python">def get_client_id(request):
    if request.headers.get('X-API-Key'):
        return f&quot;api:{request.headers['X-API-Key']}&quot;

    # Fallback: IP + User-Agent hash for anonymous users
    ua_hash = hashlib.md5(
        request.headers.get('User-Agent', '').encode()
    ).hexdigest()[:8]

    return f&quot;anon:{request.remote_addr}:{ua_hash}&quot;
</code></pre>
<p>Also consider higher limits for known shared IPs (corporate proxies, cloud providers).</p>
<h3 id="q4-rate-limiter-failure">Q4: What happens when your rate limiter fails?</h3>
<p><strong>Answer:</strong> Design for failure with a clear strategy:</p>
<p><strong>Fail Open (Allow):</strong></p>
<ul>
<li>Requests proceed when rate limiter is unavailable</li>
<li>Protects user experience</li>
<li>Risk: Potential overload during outages</li>
<li>Use for: Non-critical rate limits</li>
</ul>
<p><strong>Fail Closed (Block):</strong></p>
<ul>
<li>Reject requests when rate limiter fails</li>
<li>Protects backend systems</li>
<li>Risk: Availability impact</li>
<li>Use for: Critical protection (DDoS, expensive operations)</li>
</ul>
<p><strong>Best Practice:</strong> Implement circuit breaker pattern with fallback to local rate limiting.</p>
<h3 id="q5-pricing-tier-rate-limiting">Q5: How would you design rate limiting for different pricing tiers?</h3>
<p><strong>Answer:</strong></p>
<pre><code class="language-python">TIER_LIMITS = {
    'free': {'requests_per_minute': 60, 'burst': 10},
    'pro': {'requests_per_minute': 600, 'burst': 100},
    'enterprise': {'requests_per_minute': 6000, 'burst': 1000}
}

class TieredRateLimiter:
    def __init__(self):
        self.limiters = {}

    def get_limiter(self, user):
        tier = user.subscription_tier
        config = TIER_LIMITS[tier]

        if user.id not in self.limiters:
            self.limiters[user.id] = TokenBucket(
                rate=config['requests_per_minute'] / 60,
                capacity=config['burst']
            )

        return self.limiters[user.id]

    def allow(self, user):
        limiter = self.get_limiter(user)
        return limiter.allow()
</code></pre>
<p>Also consider: endpoint-specific limits, daily/monthly quotas, and overage billing.</p>
</div>
<hr />
<h2 id="common-mistakes">Common Mistakes</h2>
<div>
<h4>Rate Limiting Anti-Patterns</h4>
<div>
<div>
<div>Rate limiting only by IP address</div>
<div>Many users share IPs (corporate networks, mobile carriers). Use multiple identifiers or authenticated user IDs when possible.</div>
</div>
<div>
<div>Not returning rate limit headers</div>
<div>Clients need to know their limits to implement proper backoff. Always include X-RateLimit-* and Retry-After headers.</div>
</div>
<div>
<div>Hard-coded limits without configuration</div>
<div>Rate limits should be configurable per-endpoint, per-tier, and adjustable without deployment. Use configuration service.</div>
</div>
<div>
<div>No monitoring or alerting</div>
<div>Track rejection rates, top limited clients, and limiter latency. Alert on unusual patterns that might indicate attacks or misconfigurations.</div>
</div>
<div>
<div>Applying rate limits after expensive operations</div>
<div>Check rate limits BEFORE processing requests. Place rate limiting middleware at the earliest point in the request pipeline.</div>
</div>
<div>
<div>Same limits for all endpoints</div>
<div>Expensive operations (AI inference, reports) need stricter limits than cheap ones (health checks). Differentiate by endpoint cost.</div>
</div>
</div>
</div>
<hr />
<h2 id="quick-reference-card">Quick Reference Card</h2>
<div>
<h4>Rate Limiting Cheat Sheet</h4>
<div>
<div>
<div>Algorithm Selection</div>
<div>
<div><strong>Token Bucket:</strong> Allow bursts, smooth average</div>
<div><strong>Sliding Window:</strong> Strict limits, no bursts</div>
<div><strong>Fixed Window:</strong> Simple, edge burst problem</div>
<div><strong>Leaky Bucket:</strong> Smooth output rate</div>
</div>
</div>
<div>
<div>Standard Headers</div>
<div>
<div><strong>X-RateLimit-Limit:</strong> Maximum allowed</div>
<div><strong>X-RateLimit-Remaining:</strong> Remaining</div>
<div><strong>X-RateLimit-Reset:</strong> Reset timestamp</div>
<div><strong>Retry-After:</strong> Seconds to wait</div>
</div>
</div>
<div>
<div>Client Identification</div>
<div>
<div>1. API key (best)</div>
<div>2. User ID (authenticated)</div>
<div>3. IP + User-Agent (anonymous)</div>
<div>4. Behavioral fingerprinting</div>
</div>
</div>
<div>
<div>Failure Strategies</div>
<div>
<div><strong>Fail Open:</strong> Allow on failure (UX)</div>
<div><strong>Fail Closed:</strong> Block on failure (safety)</div>
<div><strong>Local Fallback:</strong> Use local cache</div>
<div><strong>Circuit Breaker:</strong> Graceful degradation</div>
</div>
</div>
</div>
</div>
<hr />
<h2 id="related-topics">Related Topics</h2>
<ul>
<li><a href="/topic/system-design/api-gateway">API Gateway</a> - Centralized rate limiting</li>
<li><a href="/topic/system-design/load-balancing">Load Balancing</a> - Distributing traffic</li>
<li><a href="/topic/system-design/caching">Caching</a> - Reducing backend load</li>
<li><a href="/topic/design-patterns/circuit-breaker">Circuit Breaker</a> - Failure handling</li>
</ul>
