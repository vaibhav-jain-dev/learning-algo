<h1 id="network-protocols">Network Protocols</h1>
<h2 id="overview">Overview</h2>
<p>Network protocols are the foundational contracts that govern how distributed systems exchange data. Understanding their internal mechanisms, failure modes, and performance characteristics is essential for designing systems that operate correctly under real-world conditions.</p>
<div>
<h4>Why Protocol Internals Matter in Interviews</h4>
<div>
<div>
<div>Surface-level answer:</div>
<div>"TCP is reliable, UDP is fast"</div>
</div>
<div>
<div>Interview-winning answer:</div>
<div>"TCP's congestion control uses AIMD, which causes throughput oscillation. For high-bandwidth transcontinental links, the BDP exceeds default buffer sizes, requiring tuning or protocols like QUIC."</div>
</div>
</div>
</div>
<hr />
<h2 id="tcp-transmission-control-protocol">TCP: Transmission Control Protocol</h2>
<h3 id="internal-mechanisms">Internal Mechanisms</h3>
<p>TCP provides reliable, ordered, connection-oriented byte streams. Understanding its internals reveals why it behaves the way it does.</p>
<div>
<h4>TCP Connection Lifecycle</h4>
<div>
<div>
<div>
<h5>Three-Way Handshake</h5>
<div>
<div><strong>SYN:</strong> Client sends ISN (Initial Sequence Number), randomly chosen to prevent session hijacking</div>
<div><strong>SYN-ACK:</strong> Server responds with its own ISN and acknowledges client's ISN+1</div>
<div><strong>ACK:</strong> Client acknowledges server's ISN+1 - connection established</div>
</div>
<div>
<strong>Latency cost:</strong> 1.5 RTT before first data byte can be sent
</div>
</div>
<div>
<h5>Four-Way Termination</h5>
<div>
<div><strong>FIN:</strong> Initiator signals no more data to send</div>
<div><strong>ACK:</strong> Receiver acknowledges</div>
<div><strong>FIN:</strong> Receiver signals its completion</div>
<div><strong>ACK:</strong> Initiator acknowledges - connection closed</div>
</div>
<div>
<strong>TIME_WAIT:</strong> 2*MSL (typically 60-120s) prevents old packets from corrupting new connections
</div>
</div>
</div>
</div>
</div>
<div>
<h4>Critical Assumption</h4>
<div>
TCP assumes the network is a shared resource requiring <strong>cooperative congestion control</strong>. This assumption breaks in adversarial scenarios (e.g., competing flows intentionally ignoring congestion signals) or when RTT fairness becomes an issue (flows with lower RTT get disproportionate bandwidth).
</div>
</div>
<h3 id="congestion-control-deep-dive">Congestion Control Deep Dive</h3>
<p>TCP's congestion control algorithms determine throughput and fairness across the internet.</p>
<div>
<h4>Congestion Control Algorithms</h4>
<div>
<div>
<h5>Traditional: Reno/NewReno (AIMD)</h5>
<div>
<div><strong>Slow Start:</strong> Exponential growth (cwnd doubles each RTT) until ssthresh</div>
<div><strong>Congestion Avoidance:</strong> Linear growth (cwnd += 1/cwnd per ACK)</div>
<div><strong>Fast Retransmit:</strong> 3 duplicate ACKs trigger retransmit without timeout</div>
<div><strong>Fast Recovery:</strong> Halve cwnd on loss, don't return to slow start</div>
</div>
<div>
<strong>Problem:</strong> Throughput = (MSS/RTT) * (1/sqrt(p)) - poor for high BDP networks
</div>
</div>
<div>
<h5>Modern: BBR (Bottleneck Bandwidth and RTT)</h5>
<div>
<div><strong>Model-based:</strong> Estimates bottleneck bandwidth and min RTT</div>
<div><strong>Pacing:</strong> Sends at estimated bandwidth rate, not bursty</div>
<div><strong>ProbeRTT:</strong> Periodically drains queue to measure true RTT</div>
<div><strong>No loss-based signals:</strong> Uses delay as primary congestion signal</div>
</div>
<div>
<strong>Benefit:</strong> 2-25x throughput improvement on lossy/high-BDP networks
</div>
</div>
</div>
</div>
<div>
<h4>Design Trade-off: Loss vs Delay Signals</h4>
<div>
<strong>Loss-based (Reno, CUBIC):</strong> Simple, works everywhere, but causes bufferbloat and sawtooth throughput patterns.<br><br>
<strong>Delay-based (Vegas, BBR):</strong> Can achieve near-optimal throughput, but vulnerable to competing loss-based flows that fill buffers and starve delay-sensitive flows. BBRv2 attempts to address fairness issues.
</div>
</div>
<h3 id="tcp-edge-cases-and-failure-modes">TCP Edge Cases and Failure Modes</h3>
<div>
<h4>Production Edge Cases</h4>
<div>
<div>
<strong>TIME_WAIT Exhaustion</strong>
<div>
  High-volume servers creating many short connections exhaust ephemeral ports. Each closed connection holds a port for 2*MSL. Solutions: connection pooling, SO_REUSEADDR, tcp_tw_reuse (Linux), or move to long-lived connections.
</div>
</div>
<div>
<strong>Head-of-Line Blocking</strong>
<div>
  A single lost packet blocks all subsequent data until retransmission completes. For multiplexed protocols (HTTP/2 over TCP), one slow stream blocks all streams. This motivated QUIC's move to UDP with per-stream reliability.
</div>
</div>
<div>
<strong>Receive Window Zero</strong>
<div>
  Slow receiver fills its buffer, advertises zero window. Sender enters persist mode, sending window probes. If application is deadlocked waiting for data it can't read, connection stalls indefinitely. Monitor for connections stuck in persist state.
</div>
</div>
<div>
<strong>SYN Flood Attacks</strong>
<div>
  Attackers send SYN packets with spoofed IPs, filling the SYN backlog. Server can't accept legitimate connections. Mitigations: SYN cookies (stateless until ACK), increased backlog, rate limiting, [[load-balancing]](/topics/system-design/load-balancing) with SYN proxy.
</div>
</div>
</div>
</div>
<h3 id="tcp-interview-questions-3-level-deep">TCP Interview Questions (3-Level Deep)</h3>
<div>
<h4>Level 1: Explain the TCP three-way handshake</h4>
<div>
<div>
<strong>Answer:</strong> SYN, SYN-ACK, ACK sequence establishes connection. Client sends SYN with random sequence number, server responds with SYN-ACK acknowledging and providing its sequence number, client ACKs to complete.
</div>
<div>
<h5>Level 2: Why is the ISN random? What happens if it's predictable?</h5>
<div>
<div>
<strong>Answer:</strong> Predictable ISNs enable TCP session hijacking. Attacker predicts the next ISN, sends spoofed packets that appear to be from legitimate client. Mirai botnet exploited this. Modern systems use RFC 6528 algorithm: ISN = Hash(local IP, local port, remote IP, remote port, secret key) + timestamp.
</div>
<div>
<h6>Level 3: How do SYN cookies work and what are their limitations?</h6>
<div>
<strong>Answer:</strong> SYN cookies encode connection state in the ISN itself: timestamp (5 bits), MSS index (3 bits), hash of connection tuple (24 bits). Server doesn't store state until ACK arrives with valid cookie.
  <br><br>
<strong>Limitations:</strong> (1) Only 8 MSS values possible, losing precision; (2) TCP options in SYN are lost (no window scaling, SACK, timestamps); (3) Cannot detect duplicate SYNs; (4) Slight CPU overhead for crypto hash per SYN. Modern variant: TCP Fast Open with cookies for repeat clients.
</div>
</div>
</div>
</div>
</div>
<h4>Level 1: What is TCP congestion control?</h4>
<div>
<div>
<strong>Answer:</strong> TCP adjusts sending rate based on network conditions using congestion window (cwnd). Starts slow, increases until detecting loss, then backs off. Prevents overwhelming the network.
</div>
<div>
<h5>Level 2: Why does TCP throughput collapse on high-latency lossy links?</h5>
<div>
<div>
<strong>Answer:</strong> TCP Reno throughput = MSS * sqrt(3/2) / (RTT * sqrt(p)). For a 100ms RTT link with 1% loss and 1500-byte MSS, max throughput is ~1.5 Mbps regardless of available bandwidth. Loss-based congestion control treats all loss as congestion, halving cwnd even for random wireless loss.
</div>
<div>
<h6>Level 3: How does BBR achieve better throughput and what are its fairness concerns?</h6>
<div>
<strong>Answer:</strong> BBR models the path: estimates bottleneck bandwidth (BtlBw) by tracking max delivery rate, estimates min RTT (RTprop) by tracking min RTT when not probing. Sends at BtlBw rate, targeting 2*BDP inflight.
  <br><br>
<strong>Fairness issues:</strong> (1) BBRv1 is unfair to Reno/CUBIC - can take 40%+ of bandwidth in competition; (2) BBR vs BBR flows with different RTTs favor lower-RTT flows; (3) ProbeRTT phase causes synchronized throughput drops. BBRv2 adds loss-based signals for fairness, but sacrifices some throughput gains.
</div>
</div>
</div>
</div>
</div>
<h4>Level 1: What is the TCP receive window?</h4>
<div>
<div>
<strong>Answer:</strong> Flow control mechanism where receiver advertises available buffer space. Sender limits unacknowledged data to window size, preventing receiver buffer overflow.
</div>
<div>
<h5>Level 2: What is the bandwidth-delay product problem and how is it solved?</h5>
<div>
<div>
<strong>Answer:</strong> BDP = bandwidth * RTT. For 1 Gbps link with 100ms RTT, BDP = 12.5 MB. Original TCP window field is 16 bits (max 64KB), limiting throughput to 64KB/100ms = 5 Mbps. Window scaling option (RFC 7323) provides 14-bit shift, allowing windows up to 1 GB.
</div>
<div>
<h6>Level 3: How do you diagnose and fix TCP buffer tuning issues in production?</h6>
<div>
<strong>Diagnosis:</strong> Check net.ipv4.tcp_rmem/tcp_wmem settings. Use ss -i to see cwnd, rtt, delivery_rate. If cwnd is small but no retransmits, receive window is limiting. If retransmits are high, it's network loss not buffer issues.
  <br><br>
<strong>Tuning:</strong> Set tcp_rmem/tcp_wmem max>= 2*BDP for expected paths. Enable tcp_window_scaling. For many connections, use tcp_moderate_rcvbuf for auto-tuning. <strong>Caution:</strong> Large buffers * many connections = memory exhaustion. Use tcp_mem limits.
</div>
</div>
</div>
</div>
</div>
</div>
<hr />
<h2 id="udp-user-datagram-protocol">UDP: User Datagram Protocol</h2>
<h3 id="internal-mechanisms-1">Internal Mechanisms</h3>
<p>UDP provides minimal transport: just multiplexing (ports) and optional checksums. Everything else is the application's responsibility.</p>
<div>
<h4>UDP Header Structure</h4>
<div>
<div>
<div>
<div>Source Port<br><span>16 bits</span></div>
<div>Dest Port<br><span>16 bits</span></div>
<div>Length<br><span>16 bits</span></div>
<div>Checksum<br><span>16 bits</span></div>
</div>
<div>Total: 8 bytes (vs TCP's 20-60 bytes)</div>
</div>
</div>
</div>
<div>
<h4>Critical Assumption</h4>
<div>
UDP assumes the <strong>application knows better than the transport layer</strong> what reliability semantics it needs. This is correct for real-time media (where old data is useless) but requires careful application design for anything needing reliability.
</div>
</div>
<h3 id="when-udp-outperforms-tcp">When UDP Outperforms TCP</h3>
<div>
<table>
<tr>
<th>Scenario</th>
<th>Why UDP Wins</th>
<th>Real-World Example</th>
</tr>
<tr>
<td><strong>Real-time media</strong></td>
<td>Retransmitting old audio/video frame is pointless - it's already past playback time</td>
<td>Zoom, Discord voice, Twitch ingest</td>
</tr>
<tr>
<td><strong>Small stateless queries</strong></td>
<td>Handshake overhead (1.5 RTT) exceeds query time. Single packet fits entire request.</td>
<td>DNS, NTP, DHCP</td>
</tr>
<tr>
<td><strong>Broadcast/Multicast</strong></td>
<td>TCP is point-to-point only. UDP supports one-to-many transmission.</td>
<td>IPTV, service discovery, mDNS</td>
</tr>
<tr>
<td><strong>High fan-out telemetry</strong></td>
<td>Thousands of connections = massive TCP state overhead. UDP is stateless.</td>
<td>StatsD, game server updates</td>
</tr>
<tr>
<td><strong>Custom reliability</strong></td>
<td>Application needs partial reliability or FEC, not TCP's all-or-nothing retransmit</td>
<td>QUIC, WebRTC, game engines</td>
</tr>
</table>
</div>
<h3 id="building-reliability-on-udp">Building Reliability on UDP</h3>
<div>
<h4>Application-Layer Reliability Mechanisms</h4>
<div>
<div>
<h5>Sequence Numbers + ACKs</h5>
<div>
  Application adds sequence number to each packet. Receiver tracks gaps and requests retransmits. Allows selective reliability - only retransmit what matters.
</div>
<div>
<strong>Used by:</strong> QUIC, game protocols
</div>
</div>
<div>
<h5>Forward Error Correction (FEC)</h5>
<div>
  Send redundant packets using Reed-Solomon or similar codes. Receiver can reconstruct lost packets without retransmission. Trades bandwidth for latency.
</div>
<div>
<strong>Used by:</strong> QUIC (optional), WebRTC, video codecs
</div>
</div>
<div>
<h5>Idempotent Operations</h5>
<div>
  Design requests so duplicates and reordering don't matter. Include request ID for deduplication. Response can be safely retried.
</div>
<div>
<strong>Used by:</strong> DNS, NTP, stateless RPCs
</div>
</div>
<div>
<h5>Deadline-Based Delivery</h5>
<div>
  Each packet has timestamp/deadline. Receiver drops packets past deadline rather than buffering. No retransmit of stale data.
</div>
<div>
<strong>Used by:</strong> Real-time audio/video, gaming
</div>
</div>
</div>
</div>
<h3 id="udp-interview-questions-3-level-deep">UDP Interview Questions (3-Level Deep)</h3>
<div>
<h4>Level 1: When would you use UDP over TCP?</h4>
<div>
<div>
<strong>Answer:</strong> Real-time applications (voice, video, gaming) where latency matters more than reliability, or small stateless queries (DNS) where connection overhead is wasteful.
</div>
<div>
<h5>Level 2: How would you implement reliable delivery on UDP?</h5>
<div>
<div>
<strong>Answer:</strong> Add sequence numbers, send ACKs or NACKs, implement retransmission with timeout. Can add selective acknowledgment (SACK) for efficiency. May add FEC for latency-sensitive paths. Implement congestion control to be network-friendly.
</div>
<div>
<h6>Level 3: Why did QUIC choose UDP? What obstacles does this create?</h6>
<div>
<strong>Why UDP:</strong> (1) Kernel TCP stacks ossified - can't deploy new algorithms without OS updates; (2) Middleboxes (NATs, firewalls) mangle unknown TCP options; (3) Per-stream reliability impossible in TCP; (4) Connection migration impossible with TCP's 4-tuple identity.
  <br><br>
<strong>Obstacles:</strong> (1) NAT rebinding - UDP NAT mappings timeout faster (30s) than TCP; QUIC needs keepalives; (2) Firewalls blocking UDP/443; (3) No hardware offload initially; (4) Some networks rate-limit UDP; (5) Reimplementing congestion control correctly is hard.
</div>
</div>
</div>
</div>
</div>
<h4>Level 1: What are the risks of using UDP?</h4>
<div>
<div>
<strong>Answer:</strong> No guaranteed delivery, packets can arrive out of order or duplicated, no congestion control means you can overwhelm the network.
</div>
<div>
<h5>Level 2: How are UDP amplification attacks performed and mitigated?</h5>
<div>
<div>
<strong>Attack:</strong> Attacker sends small requests with spoofed source IP (victim's IP) to servers that respond with large replies. DNS (70x), NTP (556x), Memcached (51000x) amplification factors. Victim flooded with traffic.
  <br><br>
<strong>Mitigations:</strong> BCP38 source address validation, rate limiting responses, disabling unnecessary UDP services, response rate limiting (RRL) for DNS.
</div>
<div>
<h6>Level 3: How do you design a UDP protocol that's amplification-resistant?</h6>
<div>
<strong>Design principles:</strong> (1) Response must not exceed request size until source IP verified; (2) Require proof-of-work or puzzle for expensive operations; (3) Use challenge-response before amplifying (like TCP SYN cookies); (4) Rate-limit by source IP at application layer.
  <br><br>
<strong>QUIC's approach:</strong> Server sends Retry packet with address validation token before sending large handshake data. Client must echo token, proving it can receive at claimed IP. Initial packet minimum size (1200 bytes) limits amplification to 3x.
</div>
</div>
</div>
</div>
</div>
</div>
<hr />
<h2 id="http2-binary-framing-and-multiplexing">HTTP/2: Binary Framing and Multiplexing</h2>
<h3 id="internal-mechanisms-2">Internal Mechanisms</h3>
<p>HTTP/2 fundamentally restructures HTTP communication while maintaining semantic compatibility with HTTP/1.1.</p>
<div>
<h4>HTTP/2 Frame Structure</h4>
<div>
<div>
<div>
<div>Length<br><span>24 bits</span></div>
<div>Type<br><span>8 bits</span></div>
<div>Flags<br><span>8 bits</span></div>
<div>R<br><span>1 bit</span></div>
<div>Stream ID<br><span>31 bits</span></div>
</div>
<div>
  Frame Payload (variable length)
</div>
</div>
<div>
<div>
<strong>DATA</strong><br>Request/response body
</div>
<div>
<strong>HEADERS</strong><br>HTTP headers (HPACK)
</div>
<div>
<strong>SETTINGS</strong><br>Connection params
</div>
<div>
<strong>WINDOW_UPDATE</strong><br>Flow control
</div>
<div>
<strong>RST_STREAM</strong><br>Cancel stream
</div>
</div>
</div>
</div>
<h3 id="multiplexing-and-stream-prioritization">Multiplexing and Stream Prioritization</h3>
<div>
<h4>HTTP/1.1 vs HTTP/2 Connection Usage</h4>
<div>
<div>
<h5>HTTP/1.1: Multiple Connections</h5>
<div>
<div>
<div>GET /page.html</div>
<div></div>
</div>
<div>
<div></div>
<div>GET /style.css</div>
<div></div>
</div>
<div>
<div></div>
<div>GET /app.js</div>
</div>
</div>
<div>6 parallel connections (browser limit), head-of-line blocking per connection</div>
</div>
<div>
<h5>HTTP/2: Single Multiplexed Connection</h5>
<div>
<div>
<div></div>
<div></div>
<div></div>
<div></div>
</div>
<div>
<div></div>
<div></div>
<div></div>
<div></div>
</div>
<div>
<div>Stream 1</div>
<div>Stream 3</div>
<div>Stream 5</div>
</div>
</div>
<div>Interleaved frames from multiple streams, single TCP connection</div>
</div>
</div>
</div>
<div>
<h4>Design Trade-off: Stream Prioritization</h4>
<div>
HTTP/2 supports priority through dependency trees and weights. However, <strong>prioritization is advisory</strong> - servers may ignore it. Many CDNs and servers have poor or no priority support. Chrome deprecated complex priority trees in favor of simple urgency/incremental hints (Priority header) due to inconsistent implementation.
  <br><br>
<strong>Implication:</strong> Don't rely on HTTP/2 priorities for correctness. Use them as optimization hints only.
</div>
</div>
<h3 id="hpack-header-compression">HPACK Header Compression</h3>
<div>
<h4>HPACK Compression Mechanism</h4>
<div>
<div>
<h5>Static Table</h5>
<div>
  61 predefined header name-value pairs. Index 2 = ":method: GET", Index 7 = ":scheme: https". Single byte reference.
</div>
</div>
<div>
<h5>Dynamic Table</h5>
<div>
  Connection-specific table built from headers seen. "Authorization: Bearer xyz" added on first use, referenced by index thereafter. FIFO eviction.
</div>
</div>
<div>
<h5>Huffman Coding</h5>
<div>
  Static Huffman table for literals. Optimized for HTTP header byte frequencies. ~30% savings on string values.
</div>
</div>
</div>
<div>
<strong>Security consideration:</strong> Dynamic table is stateful. CRIME/BREACH attacks exploit compression ratio as side channel to extract secrets. Never compress headers containing secrets with attacker-controlled data.
</div>
</div>
<h3 id="http2-interview-questions-3-level-deep">HTTP/2 Interview Questions (3-Level Deep)</h3>
<div>
<h4>Level 1: How does HTTP/2 improve over HTTP/1.1?</h4>
<div>
<div>
<strong>Answer:</strong> Multiplexing (multiple requests on one connection), binary framing (efficient parsing), header compression (HPACK), server push, and stream prioritization.
</div>
<div>
<h5>Level 2: Why does HTTP/2 still suffer from head-of-line blocking?</h5>
<div>
<div>
<strong>Answer:</strong> HTTP/2 eliminates HTTP-level HOL blocking via multiplexing, but runs over TCP. TCP delivers bytes in order - if packet N is lost, packets N+1 through M must wait for N's retransmission even if they belong to different HTTP/2 streams. With many multiplexed streams, loss probability increases, making this worse than HTTP/1.1's multiple connections.
</div>
<div>
<h6>Level 3: How does HTTP/3 solve this, and what new problems does it introduce?</h6>
<div>
<strong>Solution:</strong> HTTP/3 uses QUIC over UDP. Each QUIC stream has independent reliability - loss in stream A doesn't block stream B. Only the affected stream waits for retransmission.
  <br><br>
<strong>New problems:</strong> (1) UDP often blocked/rate-limited - needs TCP fallback; (2) No hardware offload initially - higher CPU; (3) User-space implementation means more syscalls; (4) NAT rebinding happens faster for UDP; (5) Kernel bypass (DPDK) needed for high performance, adding complexity; (6) QPACK header compression needs careful synchronization to avoid HOL blocking in the compression state.
</div>
</div>
</div>
</div>
</div>
<h4>Level 1: What is HTTP/2 Server Push?</h4>
<div>
<div>
<strong>Answer:</strong> Server can proactively send resources before client requests them. When serving HTML, push CSS/JS the client will need. Saves round trip.
</div>
<div>
<h5>Level 2: Why has Server Push largely failed in practice?</h5>
<div>
<div>
<strong>Answer:</strong> (1) Cache invalidation: pushed resources may already be in browser cache, wasting bandwidth; (2) Priority inversion: pushed low-priority resources compete with high-priority requests; (3) No client opt-out: client can RST_STREAM but data may already be sent; (4) Complexity: servers must track what to push; (5) CDN caching complications. Chrome removed Server Push support in 2022.
</div>
<div>
<h6>Level 3: What alternatives replaced Server Push?</h6>
<div>
<strong>103 Early Hints:</strong> Server sends 103 response with Link headers hinting resources to preload. Client decides whether to fetch - respects cache. Simpler, cache-aware, widely supported.
  <br><br>
<strong>Resource hints:</strong> Preload, prefetch, preconnect in HTML. Client-controlled, cache-aware.
  <br><br>
<strong>Service Workers:</strong> Can prefetch/cache resources intelligently based on app knowledge.
  <br><br>
<strong>Lesson:</strong> Server Push tried to optimize something the server doesn't have enough information about (client cache state). Better solutions give hints and let the client decide.
</div>
</div>
</div>
</div>
</div>
</div>
<hr />
<h2 id="http3-and-quic">HTTP/3 and QUIC</h2>
<h3 id="quic-internal-architecture">QUIC Internal Architecture</h3>
<p>QUIC (Quick UDP Internet Connections) reimagines transport for the modern internet, combining ideas from TCP, TLS, and HTTP/2.</p>
<div>
<h4>QUIC Protocol Stack</h4>
<div>
<div>
<strong>HTTP/3</strong>
<div>Binary framing, header compression (QPACK)</div>
</div>
<div>
<strong>QUIC Transport</strong>
<div>Streams, reliability, congestion control, encryption</div>
</div>
<div>
<strong>TLS 1.3</strong>
<div>Integrated into QUIC, not layered above</div>
</div>
<div>
<strong>UDP</strong>
<div>Minimal transport - ports and checksums only</div>
</div>
</div>
</div>
<h3 id="quic-key-innovations">QUIC Key Innovations</h3>
<div>
<div>
<div>
<h5>0-RTT Connection Establishment</h5>
<div>
  First connection: 1-RTT (vs TCP+TLS 3-RTT). Subsequent connections: 0-RTT using cached keys. Client sends encrypted data in first packet.
</div>
<div>
<strong>Risk:</strong> 0-RTT data can be replayed. Only safe for idempotent requests. Servers must track seen tickets or accept replay risk.
</div>
</div>
<div>
<h5>Connection Migration</h5>
<div>
  Connections identified by Connection ID, not 4-tuple. When IP changes (WiFi to cellular), connection survives. No re-handshake needed.
</div>
<div>
<strong>Implementation:</strong> Multiple Connection IDs per connection. Retire old IDs to prevent tracking. Path validation before migration.
</div>
</div>
<div>
<h5>Per-Stream Flow Control</h5>
<div>
  Independent flow control windows per stream AND connection-level. Slow consumer on one stream doesn't block others. More granular than TCP.
</div>
<div>
<strong>Complexity:</strong> More state to track. Deadlock possible if stream windows exhausted while connection window available.
</div>
</div>
<div>
<h5>Integrated Encryption</h5>
<div>
  All QUIC packets encrypted (except initial handshake). Even packet numbers and ACK frames encrypted. Prevents ossification by middleboxes.
</div>
<div>
<strong>Trade-off:</strong> Network operators can't inspect traffic. Debugging harder. No selective ACK visibility for network diagnostics.
</div>
</div>
</div>
</div>
<div>
<h4>Critical Assumption</h4>
<div>
QUIC assumes <strong>middlebox ossification is the enemy</strong>. By encrypting everything and using UDP, QUIC can evolve without waiting for middlebox firmware updates. However, this creates operational challenges - network operators lose visibility, and some networks block UDP entirely.
</div>
</div>
<h3 id="http3-interview-questions-3-level-deep">HTTP/3 Interview Questions (3-Level Deep)</h3>
<div>
<h4>Level 1: What is QUIC and why was it created?</h4>
<div>
<div>
<strong>Answer:</strong> QUIC is a transport protocol over UDP that provides reliable, multiplexed, encrypted connections. Created to reduce latency (fewer round trips), eliminate head-of-line blocking, and enable connection migration.
</div>
<div>
<h5>Level 2: How does QUIC achieve 0-RTT connection establishment?</h5>
<div>
<div>
<strong>Answer:</strong> On first connection, QUIC completes TLS 1.3 handshake in 1 RTT. Server provides session ticket and transport parameters. On reconnection, client uses cached ticket to derive early data keys and sends encrypted data in first packet (before server responds).
</div>
<div>
<h6>Level 3: What are the security implications of 0-RTT and how do you mitigate them?</h6>
<div>
<strong>Replay attacks:</strong> Attacker captures 0-RTT data and replays it. Server processes request twice. Catastrophic for non-idempotent operations (payments, votes).
  <br><br>
<strong>Mitigations:</strong> (1) Single-use session tickets - server tracks used tickets (requires shared state); (2) Strike registers - probabilistic tracking of used tickets; (3) Time-bounded tickets - limit replay window; (4) Application-layer idempotency keys; (5) Only allow safe methods (GET) in 0-RTT.
  <br><br>
<strong>Trade-off:</strong> Strict anti-replay requires server state sharing across replicas, negating some scalability benefits. Most deployments accept bounded replay window.
</div>
</div>
</div>
</div>
</div>
<h4>Level 1: Why does HTTP/3 use UDP instead of TCP?</h4>
<div>
<div>
<strong>Answer:</strong> UDP allows implementing custom reliability and congestion control in user-space, enabling per-stream reliability (no HOL blocking) and connection migration (IP changes don't break connections).
</div>
<div>
<h5>Level 2: What challenges does UDP-based transport face in enterprise networks?</h5>
<div>
<div>
<strong>Answer:</strong> (1) Firewalls often block UDP/443; (2) UDP rate-limited to prevent DDoS; (3) NAT mappings expire faster (30s vs TCP's minutes); (4) No deep packet inspection possible (encrypted); (5) QoS policies may not recognize QUIC; (6) Proxy infrastructure assumes TCP.
</div>
<div>
<h6>Level 3: How do browsers handle QUIC blocking, and what's the performance impact?</h6>
<div>
<strong>Happy Eyeballs for QUIC:</strong> Browser races QUIC and TCP connections. If QUIC fails/slow, TCP wins. Subsequent requests use winner. QUIC failures cached temporarily to avoid repeated probing overhead.
  <br><br>
<strong>Performance impact:</strong> Racing adds ~1 connection worth of overhead. On networks blocking QUIC, adds RTT delay before TCP fallback. Chrome reports ~5-7% of connections fall back to TCP.
  <br><br>
<strong>Alt-Svc learning:</strong> Server advertises QUIC support via Alt-Svc header. Browser caches this, tries QUIC on next visit. If blocked, falls back and clears cache. Results in good long-term behavior but suboptimal first visits.
</div>
</div>
</div>
</div>
</div>
</div>
<hr />
<h2 id="websockets">WebSockets</h2>
<h3 id="protocol-internals">Protocol Internals</h3>
<p>WebSocket provides full-duplex communication over a single TCP connection, upgrading from HTTP.</p>
<div>
<h4>WebSocket Handshake and Frame Format</h4>
<div>
<div>
<h5>HTTP Upgrade Handshake</h5>
<div>
<div>Client Request:</div>
  GET /chat HTTP/1.1<br>
  Upgrade: websocket<br>
  Connection: Upgrade<br>
  Sec-WebSocket-Key: dGhlIHNhbXBsZQ==<br>
  Sec-WebSocket-Version: 13<br><br>
<div>Server Response:</div>
  HTTP/1.1 101 Switching Protocols<br>
  Upgrade: websocket<br>
  Connection: Upgrade<br>
  Sec-WebSocket-Accept: s3pPLMBiTxaQ9k...
</div>
<div>
  Accept = Base64(SHA1(Key + GUID))
</div>
</div>
<div>
<h5>Frame Structure</h5>
<div>
<div>
<div>FIN</div>
<div>RSV</div>
<div colspan="4">Opcode</div>
<div>M</div>
<div>Len</div>
</div>
<div>
<strong>Opcodes:</strong> 0x1=text, 0x2=binary, 0x8=close, 0x9=ping, 0xA=pong<br>
<strong>Mask:</strong> Client frames MUST be masked (XOR with 32-bit key)<br>
<strong>Length:</strong> 7 bits, or 16/64 bits if 126/127
</div>
</div>
</div>
</div>
</div>
<div>
<h4>Critical Assumption</h4>
<div>
WebSocket assumes <strong>infrastructure supports long-lived connections</strong>. This fails with: (1) Load balancers with short idle timeouts; (2) Proxies that buffer/batch; (3) Firewalls that drop idle connections; (4) [[auto-scaling]](/topics/system-design/auto-scaling) that terminates instances with active connections.
</div>
</div>
<h3 id="scaling-websocket-connections">Scaling WebSocket Connections</h3>
<div>
<h4>WebSocket Scaling Challenges</h4>
<div>
<div>
<h5>Connection State</h5>
<div>
  Each connection consumes memory (~5-50KB per connection). 100K connections = 500MB-5GB just for connection state. File descriptors also limited (ulimit).
</div>
</div>
<div>
<h5>Sticky Sessions</h5>
<div>
  WebSocket requires routing to same server. Traditional round-robin load balancing breaks. Need IP hash, cookie-based, or connection ID routing.
</div>
</div>
<div>
<h5>Pub/Sub Backend</h5>
<div>
  Broadcasting to users on different servers requires [[message-queues]](/topics/system-design/message-queues) backend. Redis Pub/Sub, Kafka, or dedicated solutions like Socket.IO adapters.
</div>
</div>
<div>
<h5>Graceful Shutdown</h5>
<div>
  Deploy requires draining connections. Send close frame, wait for client reconnect to new server. Blue-green deployments need connection migration strategy.
</div>
</div>
</div>
</div>
<h3 id="websocket-interview-questions-3-level-deep">WebSocket Interview Questions (3-Level Deep)</h3>
<div>
<h4>Level 1: When would you use WebSocket vs HTTP polling?</h4>
<div>
<div>
<strong>Answer:</strong> WebSocket for real-time bidirectional communication (chat, gaming, live updates). HTTP polling for infrequent updates where simplicity matters. WebSocket eliminates polling overhead and provides lower latency.
</div>
<div>
<h5>Level 2: How do you handle WebSocket reconnection and message ordering?</h5>
<div>
<div>
<strong>Reconnection:</strong> Exponential backoff with jitter to avoid thundering herd. Client stores last message ID, server resends missed messages on reconnect. Handle connection state machine: CONNECTING, OPEN, CLOSING, CLOSED.
  <br><br>
<strong>Ordering:</strong> Include sequence numbers. Client buffers out-of-order messages. Request retransmit for gaps. Or use server-side ordering with Lamport timestamps for distributed scenarios.
</div>
<div>
<h6>Level 3: Design a WebSocket system handling 1M concurrent connections across multiple servers</h6>
<div>
<strong>Connection tier:</strong> Dedicated WebSocket servers (10 servers, 100K connections each). Use epoll/kqueue for efficient I/O multiplexing. Tune kernel: file descriptors, memory, TCP buffers.
  <br><br>
<strong>Routing:</strong> Consistent hashing on user ID for sticky routing. Connection registry in Redis: user_id -> server_id. For geographic distribution, route to nearest region.
  <br><br>
<strong>Pub/Sub:</strong> Redis Cluster or Kafka for cross-server messaging. Each server subscribes to relevant channels. Fan-out on each server to local connections.
  <br><br>
<strong>Presence:</strong> Heartbeat every 30s. Server tracks last_seen. Distributed presence aggregation for "who's online" queries.
  <br><br>
<strong>Deployment:</strong> Rolling deploys with connection draining. Send reconnect hints before shutdown. Client connects to new server with resume token.
</div>
</div>
</div>
</div>
</div>
<h4>Level 1: What's the purpose of WebSocket frame masking?</h4>
<div>
<div>
<strong>Answer:</strong> Client-to-server frames are XORed with a 32-bit mask. Prevents cache poisoning attacks where malicious JavaScript could craft frames that look like HTTP responses to proxies.
</div>
<div>
<h5>Level 2: Explain the cache poisoning attack that masking prevents</h5>
<div>
<div>
<strong>Attack:</strong> Attacker's JS opens WebSocket to attacker.com. Sends crafted frame that looks like "GET /jquery.js HTTP/1.1" followed by fake response. Transparent proxy caches the fake response. Subsequent requests for jquery.js get poisoned content.
  <br><br>
<strong>Masking solution:</strong> Client generates random 32-bit mask per frame. Proxy can't predict mask, so can't construct frames that decode to valid HTTP.
</div>
<div>
<h6>Level 3: Why is masking only required client-to-server, and what are the performance implications?</h6>
<div>
<strong>Direction:</strong> Attack requires attacker-controlled client. Server-to-client doesn't have this threat model - server is trusted. If server is compromised, they have worse attacks than cache poisoning.
  <br><br>
<strong>Performance:</strong> Masking requires XOR of every byte - ~3-5% CPU overhead for client. Server must unmask, similar overhead. For high-throughput scenarios (binary data streaming), this matters. Some propose "permessage-deflate" extension which compresses before masking, reducing bytes to mask.
  <br><br>
<strong>Alternative:</strong> If proxy isn't present (direct TLS connection), masking is security theater. But WebSocket spec requires it unconditionally for simplicity - can't reliably detect proxy presence.
</div>
</div>
</div>
</div>
</div>
</div>
<hr />
<h2 id="grpc">gRPC</h2>
<h3 id="protocol-internals-1">Protocol Internals</h3>
<p>gRPC is a high-performance RPC framework using HTTP/2 transport and Protocol Buffers serialization.</p>
<div>
<h4>gRPC Protocol Layers</h4>
<div>
<div>
<strong>Application Code</strong>
<div>Generated stubs, business logic</div>
</div>
<div>
<strong>gRPC Core</strong>
<div>Call handling, interceptors, deadlines, cancellation</div>
</div>
<div>
<strong>Protocol Buffers</strong>
<div>Binary serialization, schema evolution</div>
</div>
<div>
<strong>HTTP/2</strong>
<div>Multiplexing, flow control, headers</div>
</div>
<div>
<strong>TLS</strong>
<div>Encryption, authentication</div>
</div>
</div>
</div>
<h3 id="grpc-streaming-patterns">gRPC Streaming Patterns</h3>
<div>
<h4>Four Communication Patterns</h4>
<div>
<div>
<h5>Unary RPC</h5>
<div>
  rpc GetUser(GetUserRequest)<br>&nbsp;&nbsp;returns (User);
</div>
<div>
  Single request, single response. Like HTTP request/response. Most common pattern.
</div>
</div>
<div>
<h5>Server Streaming</h5>
<div>
  rpc ListUsers(ListRequest)<br>&nbsp;&nbsp;returns (stream User);
</div>
<div>
  Single request, multiple responses. Good for large result sets, real-time updates.
</div>
</div>
<div>
<h5>Client Streaming</h5>
<div>
  rpc UploadLogs(stream LogEntry)<br>&nbsp;&nbsp;returns (UploadResult);
</div>
<div>
  Multiple requests, single response. Good for uploads, aggregation.
</div>
</div>
<div>
<h5>Bidirectional Streaming</h5>
<div>
  rpc Chat(stream Message)<br>&nbsp;&nbsp;returns (stream Message);
</div>
<div>
  Both stream independently. Chat, real-time collaboration, game state sync.
</div>
</div>
</div>
</div>
<h3 id="protocol-buffers-schema-evolution">Protocol Buffers Schema Evolution</h3>
<div>
<h4>Safe vs Breaking Changes</h4>
<div>
<div>
<h5>Safe Changes</h5>
<ul>
<li>Add new fields (with new field numbers)</li>
<li>Remove fields (mark as reserved)</li>
<li>Rename fields (wire format uses numbers)</li>
<li>Change int32 to int64 (wire compatible)</li>
<li>Add new enum values</li>
<li>Add new RPC methods</li>
</ul>
</div>
<div>
<h5>Breaking Changes</h5>
<ul>
<li>Change field numbers</li>
<li>Change field types incompatibly</li>
<li>Remove enum values (without reserved)</li>
<li>Change message name in wire format</li>
<li>Remove RPC methods</li>
<li>Change streaming mode of RPC</li>
</ul>
</div>
</div>
<div>
<strong>Best practice:</strong> Use reserved to prevent reusing deleted field numbers. Unknown fields are preserved for forward compatibility. Always version your APIs.
</div>
</div>
<div>
<h4>Design Trade-off: gRPC vs REST</h4>
<div>
<strong>gRPC advantages:</strong> ~10x smaller payloads, ~10x faster serialization, strong typing, streaming, generated clients.
  <br><br>
<strong>REST advantages:</strong> Browser-native, human-readable, easier debugging, universal tooling, cacheable by default, no generated code needed.
  <br><br>
<strong>Recommendation:</strong> gRPC for service-to-service, REST for public APIs. Consider gRPC-Web or gRPC-gateway for browser clients needing gRPC backend.
</div>
</div>
<h3 id="grpc-interview-questions-3-level-deep">gRPC Interview Questions (3-Level Deep)</h3>
<div>
<h4>Level 1: What are the benefits of gRPC over REST?</h4>
<div>
<div>
<strong>Answer:</strong> Binary Protocol Buffers (smaller, faster), HTTP/2 multiplexing, built-in streaming, strong typing with code generation, deadlines and cancellation.
</div>
<div>
<h5>Level 2: How do gRPC deadlines and cancellation propagate across services?</h5>
<div>
<div>
<strong>Deadlines:</strong> Client sets deadline (absolute time). Each hop reduces remaining time. If deadline expires, call returns DEADLINE_EXCEEDED. Deadline propagates automatically through call chain - service A calls B calls C, C's deadline reflects original minus transit time.
  <br><br>
<strong>Cancellation:</strong> When client cancels, RST_STREAM sent. Server receives cancellation, can cancel downstream calls. Prevents wasted work.
</div>
<div>
<h6>Level 3: What happens when deadline propagation conflicts with retry policies?</h6>
<div>
<strong>Conflict:</strong> Original deadline is 5s. First attempt takes 3s and fails. Retry budget allows retry, but only 2s remaining. Should we retry with likely timeout, or fail immediately?
  <br><br>
<strong>Solutions:</strong> (1) Hedging instead of retry - start parallel request before timeout; (2) Per-attempt deadline separate from overall deadline; (3) Deadline budget accounting - only retry if budget allows meaningful attempt; (4) Circuit breaker - if service consistently timing out, fail fast.
  <br><br>
<strong>gRPC behavior:</strong> Service config can specify maxAttempts, retryableStatusCodes, and hedgingPolicy. Library handles budget tracking. But: deadlines are end-to-end, retries consume budget. Design for total latency including retries.
</div>
</div>
</div>
</div>
</div>
<h4>Level 1: How does gRPC handle errors?</h4>
<div>
<div>
<strong>Answer:</strong> gRPC uses status codes (OK, INVALID_ARGUMENT, NOT_FOUND, etc.) with optional error messages and details. Richer than HTTP status codes with semantic meaning.
</div>
<div>
<h5>Level 2: How do you return structured error details with field-level validation errors?</h5>
<div>
<div>
<strong>Status + Details:</strong> gRPC Status includes repeated Any details field. Pack structured error messages (BadRequest, DebugInfo, etc.) from google.rpc.error_details.proto. Client unpacks and handles specific error types.
  <br><br>
<strong>Example:</strong> INVALID_ARGUMENT status with BadRequest detail containing FieldViolation for each invalid field (field name, description).
</div>
<div>
<h6>Level 3: How do you handle partial failures in streaming RPCs?</h6>
<div>
<strong>Challenge:</strong> Client streams 100 records. 95 succeed, 5 fail validation. Stream must complete to get final status. How to report partial success?
  <br><br>
<strong>Patterns:</strong> (1) Include status in each response message - client tracks failures as they stream; (2) Bidirectional stream - server immediately responds with per-item status; (3) Final status with details array listing failed items; (4) Atomic semantics - all or nothing with transaction.
  <br><br>
<strong>Trade-offs:</strong> Per-message status adds overhead but enables early failure detection. Batched final status is simpler but client doesn't know until end. Atomic is safest but limits throughput. Choose based on failure probability and recovery needs.
</div>
</div>
</div>
</div>
</div>
</div>
<hr />
<h2 id="connection-pooling">Connection Pooling</h2>
<h3 id="why-connection-pooling-matters">Why Connection Pooling Matters</h3>
<p>Connection establishment is expensive: TCP handshake (1 RTT), TLS handshake (1-2 RTT), protocol negotiation. Pooling amortizes this cost.</p>
<div>
<h4>Connection Lifecycle Costs</h4>
<div>
<div>
<div>TCP Handshake</div>
<div></div>
<div>1 RTT</div>
</div>
<div>
<div>TLS Handshake</div>
<div></div>
<div>1-2 RTT</div>
</div>
<div>
<div>Request/Response</div>
<div></div>
<div>1 RTT</div>
</div>
</div>
<div>
<span>New connection:</span> <strong>3-4 RTT</strong> &nbsp;|&nbsp;
<span>Pooled connection:</span> <strong>1 RTT</strong>
</div>
</div>
<h3 id="pool-configuration-parameters">Pool Configuration Parameters</h3>
<div>
<h4>Critical Pool Settings</h4>
<div>
<div>
<h5>Pool Size</h5>
<div>
<strong>Min:</strong> Baseline connections kept open (avoids cold start)<br>
<strong>Max:</strong> Upper limit to prevent resource exhaustion<br>
<strong>Guidance:</strong> Max = (core_count * 2) + effective_spindle_count for DBs. For services: measure and tune.
</div>
</div>
<div>
<h5>Timeouts</h5>
<div>
<strong>Connection timeout:</strong> Time to establish new connection<br>
<strong>Idle timeout:</strong> Close connections unused for N seconds<br>
<strong>Max lifetime:</strong> Close after N seconds regardless of use
</div>
</div>
<div>
<h5>Health Checks</h5>
<div>
<strong>Validation query:</strong> SELECT 1 before use (adds latency)<br>
<strong>Background validation:</strong> Async health check thread<br>
<strong>Eviction policy:</strong> Remove stale/failed connections
</div>
</div>
<div>
<h5>Queue Behavior</h5>
<div>
<strong>When full:</strong> Block, fail fast, or create overflow?<br>
<strong>Queue timeout:</strong> Max time waiting for connection<br>
<strong>Fair queuing:</strong> FIFO prevents starvation
</div>
</div>
</div>
</div>
<h3 id="connection-pool-anti-patterns">Connection Pool Anti-patterns</h3>
<div>
<h4>Common Mistakes</h4>
<div>
<div>
<strong>Pool per request</strong>
<div>
  Creating new pool for each request defeats the purpose. Pool should be singleton/application-scoped. Often caused by wrong dependency injection scope.
</div>
</div>
<div>
<strong>Connection leaks</strong>
<div>
  Not returning connections to pool (missing close/release). Pool exhausts, new requests block. Use try-with-resources or equivalent. Monitor pool metrics.
</div>
</div>
<div>
<strong>Ignoring server-side limits</strong>
<div>
  100 app servers with 50-connection pools = 5000 connections to database. Database might support only 1000. Coordinate pool sizes with backend capacity.
</div>
</div>
<div>
<strong>Stale connection usage</strong>
<div>
  Connection appears healthy but server closed it (idle timeout). First query fails. Solution: test-on-borrow, shorter idle timeout than server's, or maxLifetime.
</div>
</div>
</div>
</div>
<h3 id="connection-pooling-interview-questions-3-level-deep">Connection Pooling Interview Questions (3-Level Deep)</h3>
<div>
<h4>Level 1: Why use connection pooling?</h4>
<div>
<div>
<strong>Answer:</strong> Connection establishment is expensive (TCP + TLS handshakes). Pooling reuses connections, reducing latency and resource usage. Typical 3-4x latency improvement for short requests.
</div>
<div>
<h5>Level 2: How do you size a connection pool correctly?</h5>
<div>
<div>
<strong>Formula:</strong> pool_size = throughput * avg_latency. If you need 1000 req/s and average latency is 10ms, you need 10 connections (plus buffer for variance).
  <br><br>
<strong>For databases:</strong> connections = (core_count * 2) + effective_spindle_count. Most apps use pools far larger than optimal, causing contention.
</div>
<div>
<h6>Level 3: How do you handle connection pool exhaustion during traffic spikes?</h6>
<div>
<strong>Detection:</strong> Monitor pool wait time, active connections, queue depth. Alert when approaching limits.
  <br><br>
<strong>Short-term:</strong> Queue with timeout - fail fast rather than block indefinitely. Return 503 with Retry-After to enable client backoff.
  <br><br>
<strong>Strategies:</strong> (1) Adaptive pool sizing - grow under load, shrink during calm; (2) [[rate-limiting]](/topics/system-design/rate-limiting) to prevent overload; (3) [[circuit-breaker]](/topics/system-design/circuit-breaker) to fail fast when backend overwhelmed; (4) Request prioritization - shed low-priority traffic first.
  <br><br>
<strong>Prevention:</strong> [[load-testing]](/topics/system-design/load-testing) to find limits. Capacity planning with headroom. Autoscaling triggers before exhaustion.
</div>
</div>
</div>
</div>
</div>
<h4>Level 1: What is the connection pool idle timeout?</h4>
<div>
<div>
<strong>Answer:</strong> Connections unused for longer than idle timeout are closed, freeing resources. Balances keeping warm connections vs resource usage.
</div>
<div>
<h5>Level 2: Why might connections become invalid even before idle timeout?</h5>
<div>
<div>
<strong>Server-side timeout:</strong> Server's idle timeout shorter than client's pool timeout. Client thinks connection valid, server already closed.
  <br><br>
<strong>Network issues:</strong> Firewall drops idle connections. NAT rebinding. Network partition.
  <br><br>
<strong>Server restart:</strong> Server process restarted, existing connections become half-open.
</div>
<div>
<h6>Level 3: Design a connection validation strategy that balances reliability vs latency</h6>
<div>
<strong>Naive approach:</strong> Validate every borrow (SELECT 1). Adds latency to every request.
  <br><br>
<strong>Better approaches:</strong> (1) Validate if idle> N seconds - recent connections likely valid; (2) Background thread validates all connections periodically - moves latency off critical path; (3) Set maxLifetime slightly below server timeout - preemptive closure; (4) TCP keepalive - OS-level connection validation.
  <br><br>
<strong>Hybrid strategy:</strong> Background validation every 30s. On borrow, validate only if idle> 10s. maxLifetime = server_timeout - 30s. This catches most stale connections with minimal latency impact. Accept occasional first-request failure with automatic retry.
</div>
</div>
</div>
</div>
</div>
</div>
<hr />
<h2 id="protocol-selection-decision-tree">Protocol Selection Decision Tree</h2>
<div>
<h4>Choosing the Right Protocol</h4>
<div>
<div>
<h5>Is it browser-to-server?</h5>
<div>
<div>
<strong>Real-time bidirectional:</strong> WebSocket or SSE<br>
<strong>Standard request/response:</strong> REST over HTTP/2 or HTTP/3<br>
<strong>Need gRPC backend:</strong> gRPC-Web with Envoy proxy
</div>
<div>
  Browsers don't support arbitrary TCP/UDP. Limited to HTTP, WebSocket, WebRTC.
</div>
</div>
</div>
<div>
<h5>Is it service-to-service (internal)?</h5>
<div>
<div>
<strong>High throughput, low latency:</strong> gRPC<br>
<strong>Streaming:</strong> gRPC streaming<br>
<strong>Simple integration:</strong> REST<br>
<strong>Async processing:</strong> [[Message queues]](/topics/system-design/message-queues)
</div>
<div>
  Internal APIs can use binary protocols. Prioritize performance and type safety.
</div>
</div>
</div>
<div>
<h5>Is latency critical (real-time)?</h5>
<div>
<div>
<strong>Audio/video:</strong> UDP with custom reliability (or WebRTC)<br>
<strong>Gaming:</strong> UDP with app-level protocol<br>
<strong>Financial data:</strong> Multicast UDP or dedicated lines
</div>
<div>
  TCP's reliability adds latency. Accept some loss for lower latency.
</div>
</div>
</div>
<div>
<h5>Is it public API?</h5>
<div>
<div>
<strong>Standard:</strong> REST with OpenAPI<br>
<strong>GraphQL for flexibility:</strong> Single endpoint, client-specified queries<br>
<strong>High-performance clients:</strong> Offer both REST and gRPC
</div>
<div>
  Public APIs prioritize usability, documentation, tooling support.
</div>
</div>
</div>
</div>
</div>
<hr />
<h2 id="related-topics">Related Topics</h2>
<div>
<div>
<div>
<h5>Infrastructure</h5>
<ul>
<li>[[Load Balancing]](/topics/system-design/load-balancing)</li>
<li>[[API Gateway]](/topics/system-design/api-gateway)</li>
<li>[[Service Mesh]](/topics/system-design/service-mesh)</li>
<li>[[CDN]](/topics/system-design/cdn)</li>
</ul>
</div>
<div>
<h5>Reliability</h5>
<ul>
<li>[[Circuit Breaker]](/topics/system-design/circuit-breaker)</li>
<li>[[Rate Limiting]](/topics/system-design/rate-limiting)</li>
<li>[[Retry Strategies]](/topics/system-design/retry-strategies)</li>
<li>[[Timeouts]](/topics/system-design/timeouts)</li>
</ul>
</div>
<div>
<h5>Communication</h5>
<ul>
<li>[[Message Queues]](/topics/system-design/message-queues)</li>
<li>[[Event-Driven Architecture]](/topics/system-design/event-driven)</li>
<li>[[API Design]](/topics/system-design/api-design)</li>
<li>[[Serialization]](/topics/system-design/serialization)</li>
</ul>
</div>
</div>
</div>
