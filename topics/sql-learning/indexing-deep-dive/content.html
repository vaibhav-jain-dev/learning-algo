<h1 id="database-indexing-deep-dive">Database Indexing Deep Dive</h1>
<h2 id="overview">Overview</h2>
<p>Database indexing is the <span>single most important optimization technique</span> for query performance. An index is a separate data structure that maintains a sorted reference to rows in a table, enabling the database to locate data without scanning every row. Understanding index internals—B-tree structure, selectivity, covering indexes, and query planner behavior—is essential for designing performant database schemas.</p>
<div>
<h4>Core Performance Equation</h4>
<div>
    Query Time = (Disk I/O x Pages Read) + (CPU x Rows Processed) + (Network x Result Size)
</div>
<p>Indexes minimize disk I/O by reducing pages read from O(n) to O(log n)</p>
</div>
<p><strong>Critical Assumption</strong>: Indexes assume that <span>selective queries</span> (returning small percentage of rows) are the common case. For queries returning most rows, a full table scan is often faster than index lookups.</p>
<p><strong>Key Trade-off</strong>: Read performance vs. Write overhead. Every index must be updated on INSERT/UPDATE/DELETE operations. More indexes means faster reads but slower writes.</p>
<hr />
<h2 id="section-1-b-tree-index-architecture">Section 1: B-Tree Index Architecture</h2>
<h3 id="deep-mechanics">Deep Mechanics</h3>
<p>The <span>B-Tree (Balanced Tree)</span> is the default and most versatile index type in virtually all relational databases. It maintains sorted data in a tree structure where all leaf nodes are at the same depth, guaranteeing O(log n) lookup time regardless of data distribution.</p>
<div>
<h4>B-Tree Properties</h4>
<div>
<div>
<strong>Self-Balancing</strong>
<p>Insertions and deletions automatically rebalance the tree. No manual maintenance required for balance. Height remains O(log n) regardless of insertion order.</p>
</div>
<div>
<strong>High Fan-Out</strong>
<p>Each node contains many keys (typically 100-500). A 3-level B-tree with fan-out 100 can index 100³ = 1 million rows. Most queries require only 3-4 disk reads.</p>
</div>
<div>
<strong>Sequential Leaf Access</strong>
<p>Leaf nodes are linked in sorted order. Range queries traverse leaves sequentially without returning to internal nodes. Enables efficient ORDER BY and BETWEEN operations.</p>
</div>
</div>
</div>
<h3 id="b-tree-structure-visualization">B-Tree Structure Visualization</h3>
<div>
<h4>B-Tree Index Internal Structure</h4>
<div>
    <!-- Root Node -->
<div>
<div>ROOT NODE</div>
<div>[ 50 ]</div>
</div>
<pre><code>&lt;!-- Arrows down --&gt;
</code></pre>
<div>
<span>↙</span>
<span>↘</span>
</div>
<pre><code>&lt;!-- Internal Nodes --&gt;
</code></pre>
<div>
<div>
<div>INTERNAL</div>
<div>[ 20 | 35 ]</div>
</div>
<div>
<div>INTERNAL</div>
<div>[ 70 | 85 ]</div>
</div>
</div>
<pre><code>&lt;!-- Arrows to leaves --&gt;
</code></pre>
<div>
<span>↙</span><span>↓</span><span>↘</span>
<span>↙</span><span>↓</span><span>↘</span>
</div>
<pre><code>&lt;!-- Leaf Nodes with horizontal links --&gt;
</code></pre>
<div>
<div>
<div>LEAF</div>
<div>10,15,18</div>
<div>→RowIDs</div>
</div>
<span>⟷</span>
<div>
<div>LEAF</div>
<div>22,28,33</div>
<div>→RowIDs</div>
</div>
<span>⟷</span>
<div>
<div>LEAF</div>
<div>37,42,48</div>
<div>→RowIDs</div>
</div>
<span>⟷</span>
<div>
<div>LEAF</div>
<div>55,62,68</div>
<div>→RowIDs</div>
</div>
<span>⟷</span>
<div>
<div>LEAF</div>
<div>73,78,82</div>
<div>→RowIDs</div>
</div>
<span>⟷</span>
<div>
<div>LEAF</div>
<div>88,92,97</div>
<div>→RowIDs</div>
</div>
</div>
</div>
<div>
<strong>Key Insight:</strong>
<span> Leaf nodes contain actual row pointers and are doubly-linked for efficient range scans. Finding value 62: Root(50) → Right → Internal(70,85) → Left → Leaf(55,62,68) = 3 disk reads.</span>
</div>
</div>
<h3 id="b-tree-vs-btree">B-Tree vs B+Tree</h3>
<div>
<h4>B-Tree Variants Comparison</h4>
<div>
<div>
<div>
<strong>B-Tree (Original)</strong>
<ul>
<li>Data stored in all nodes</li>
<li>Faster single-key lookup (may find in internal node)</li>
<li>Less efficient range scans</li>
<li>Lower fan-out (data takes space)</li>
</ul>
</div>
</div>
<div>
<div>
<strong>B+Tree (Used in Databases)</strong>
<ul>
<li>Data only in leaf nodes</li>
<li>Internal nodes store only keys</li>
<li>Excellent range scans (linked leaves)</li>
<li>Higher fan-out, shallower tree</li>
</ul>
</div>
</div>
</div>
<div>
<strong>Interview Note:</strong>
<p>When interviews mention "B-Tree indexes," they typically mean B+Tree. PostgreSQL, MySQL InnoDB, Oracle, and SQL Server all use B+Tree variants. The distinction matters for understanding why range queries are efficient.</p>
</div>
</div>
<h3 id="b-tree-interview-questions-3-levels-deep">B-Tree Interview Questions (3 Levels Deep)</h3>
<div>
<h4>Level 1: Why do databases use B-Trees instead of binary search trees or hash tables for indexes?</h4>
<p><strong>Answer:</strong> B-Trees are optimized for <span>disk-based storage</span>. Binary trees have O(log₂ n) height, meaning many disk seeks for large datasets. B-Trees have high fan-out (hundreds of keys per node), reducing height to typically 3-4 levels even for billions of rows. Unlike hash tables, B-Trees maintain sorted order, enabling efficient range queries (BETWEEN, ORDER BY) and prefix searches (LIKE 'abc%'). Each B-Tree node is sized to match disk page size (typically 8KB), maximizing data read per I/O operation.</p>
<div>
<h5>Level 2: How does B-Tree node splitting work during insertion, and what is its impact on write performance?</h5>
<p><strong>Answer:</strong> When inserting into a full node, the B-Tree performs a <span>node split</span>: (1) Allocate new node, (2) Move half the keys to new node, (3) Promote middle key to parent, (4) Update parent pointers. If parent is also full, splitting cascades upward—potentially creating a new root level. Write amplification factor is typically 2-3x: each logical insert may write multiple pages. To mitigate: (a) Use <span>fill factor</span> below 100% (e.g., 90%) to leave room for inserts, (b) Batch inserts in sorted order when possible, (c) Use append-only structures (LSM-trees) for write-heavy workloads. PostgreSQL FILLFACTOR, MySQL innodb_page_size affect this behavior.</p>
<div>
<h6>Level 3: Explain B-Tree index fragmentation. How do you detect it and what are the trade-offs of REINDEX vs VACUUM in PostgreSQL?</h6>
<p><strong>Answer:</strong> <span>Index fragmentation</span> occurs when: (1) <strong>Internal fragmentation</strong>: pages are partially filled due to splits/deletes, wasting space; (2) <strong>External fragmentation</strong>: logical order differs from physical order on disk, causing random I/O during scans. Detection: In PostgreSQL, query <code>pgstattuple('index_name')</code> to see dead tuples and fragmentation ratio. <code>pg_stat_user_indexes</code> shows index bloat. <strong>VACUUM</strong>: Marks dead tuples as reusable but doesn't compact; low overhead, online operation. <strong>REINDEX</strong>: Rebuilds entire index from scratch; creates new compact structure but requires exclusive lock (blocking writes). <strong>REINDEX CONCURRENTLY</strong> (PostgreSQL 12+): Builds new index alongside old, swaps atomically; requires 2x space temporarily but doesn't block writes. Trade-off: VACUUM is maintenance, REINDEX is repair. For highly bloated indexes (>30% dead space), REINDEX CONCURRENTLY is preferred. Monitor with [[query-optimization]](/topic/sql-learning/query-optimization) techniques.</p>
</div>
</div>
</div>
<hr />
<h2 id="section-2-hash-indexes">Section 2: Hash Indexes</h2>
<h3 id="deep-mechanics-1">Deep Mechanics</h3>
<p><span>Hash indexes</span> use a hash function to map keys directly to bucket locations, providing O(1) average-case lookup time. However, they have significant limitations compared to B-Trees.</p>
<div>
<h4>Hash Index Characteristics</h4>
<div>
<div>
<strong>O(1) Equality Lookup</strong>
<p>Single hash computation + bucket access. Fastest possible for exact match queries like WHERE id = 12345.</p>
</div>
<div>
<strong>No Range Support</strong>
<p>Hash destroys ordering. Cannot support <,>, BETWEEN, ORDER BY, or LIKE 'prefix%' queries.</p>
</div>
<div>
<strong>Collision Handling</strong>
<p>Multiple keys may hash to same bucket. Uses chaining or open addressing. Degrades with poor hash function.</p>
</div>
<div>
<strong>Resize Operations</strong>
<p>When load factor exceeds threshold, entire hash table must be rebuilt. Expensive operation that can cause latency spikes.</p>
</div>
</div>
</div>
<h3 id="hash-index-structure">Hash Index Structure</h3>
<div>
<h4>Hash Index Lookup Process</h4>
<div>
    <!-- Input Key -->
<div>
<div>INPUT KEY</div>
<div>'user@email.com'</div>
</div>
<pre><code>&lt;!-- Arrow --&gt;
</code></pre>
<div>→</div>
<pre><code>&lt;!-- Hash Function --&gt;
</code></pre>
<div>
<div>HASH FUNCTION</div>
<div>hash(key) % buckets</div>
</div>
<pre><code>&lt;!-- Arrow --&gt;
</code></pre>
<div>→</div>
<pre><code>&lt;!-- Bucket Number --&gt;
</code></pre>
<div>
<div>BUCKET #</div>
<div>42</div>
</div>
</div>
  <!-- Bucket Array -->
<div>
<div>
<div>0</div>
<div>1</div>
<div>...</div>
<div>41</div>
<div>42</div>
<div>43</div>
<div>...</div>
<div>n</div>
</div>
<div>Bucket Array (Direct Access)</div>
</div>
  <!-- Bucket Contents -->
<div>
<div>
<div>Bucket 42 Contents:</div>
<div>
  'user@email.com' → Row 1847<br>
  'test@mail.com' → Row 3921<br>
<span>(collision chain)</span>
</div>
</div>
</div>
</div>
<h3 id="when-to-use-hash-indexes">When to Use Hash Indexes</h3>
<pre><code>```sql
-- Hash indexes are useful for exact equality on high-cardinality columns
-- PostgreSQL example:
CREATE INDEX idx_users_email_hash ON users USING HASH (email);

-- Good use case: exact match lookups
SELECT * FROM users WHERE email = 'john@example.com';  -- O(1)

-- Hash index CANNOT help with:
SELECT * FROM users WHERE email LIKE 'john%';     -- Range/prefix
SELECT * FROM users WHERE email &gt; 'a@example.com'; -- Range
SELECT * FROM users ORDER BY email;               -- Sorting
```
</code></pre>
<div>
<h4>Hash Index Limitations in Production</h4>
<div>
<div>
<strong>PostgreSQL < 10</strong>
<p>Hash indexes were not crash-safe (not WAL-logged). NEVER use hash indexes on PostgreSQL versions before 10. PostgreSQL 10+ fixed this, but B-Tree is still preferred in most cases.</p>
</div>
<div>
<strong>MySQL InnoDB</strong>
<p>InnoDB does not support hash indexes directly. The "Adaptive Hash Index" is an internal optimization that InnoDB creates automatically for frequently accessed B-Tree pages.</p>
</div>
<div>
<strong>Memory Tables Only</strong>
<p>In MySQL MEMORY engine, hash indexes are default and very fast. But data is lost on restart. Use for session tables or caches only.</p>
</div>
</div>
</div>
<h3 id="hash-index-interview-questions-3-levels-deep">Hash Index Interview Questions (3 Levels Deep)</h3>
<div>
<h4>Level 1: When would you choose a hash index over a B-Tree index?</h4>
<p><strong>Answer:</strong> Hash indexes excel at <span>pure equality lookups</span> on high-cardinality columns where you never need range queries, sorting, or prefix matching. Classic examples: lookup by UUID, session token validation, API key authentication. The O(1) vs O(log n) difference matters at extreme scale—millions of lookups per second. However, B-Trees are so well-optimized that the practical difference is often negligible, while B-Trees provide much more flexibility.</p>
<div>
<h5>Level 2: How does hash collision affect index performance, and how do databases handle it?</h5>
<p><strong>Answer:</strong> <span>Hash collisions</span> occur when different keys map to the same bucket. Databases use two strategies: (1) <strong>Chaining</strong>: Each bucket contains a linked list of colliding entries—lookup becomes O(k) where k is chain length. (2) <strong>Open addressing</strong>: Probe to next available slot using linear/quadratic probing or double hashing. PostgreSQL uses chaining with overflow pages. Performance degrades when load factor (entries/buckets) exceeds ~0.7. Worst case with poor hash function or adversarial input: all keys hash to same bucket, degenerating to O(n). Mitigations: cryptographic hash functions, load factor monitoring, automatic resizing.</p>
<div>
<h6>Level 3: Explain how extensible hashing works and why PostgreSQL adopted it for hash indexes in version 10.</h6>
<p><strong>Answer:</strong> <span>Extensible hashing</span> allows the hash table to grow incrementally without rehashing all entries. It uses a directory that maps hash prefixes to buckets. When a bucket overflows, only that bucket splits—the directory doubles but only one bucket is redistributed. This avoids the traditional hash table problem where resize rehashes everything. PostgreSQL 10 reimplemented hash indexes using extensible hashing with: (1) WAL logging for crash safety, (2) Incremental bucket splits avoiding long locks, (3) Better concurrency with page-level locking. Key insight: directory depth increases logarithmically with data size. A hash index with 2^20 buckets needs only 20 bits of directory, regardless of total entries. Trade-off: directory lookup adds indirection, but enables online growth. This makes hash indexes viable for OLTP workloads where tables grow continuously.</p>
</div>
</div>
</div>
<pre><code>---
</code></pre>
<h2 id="section-3-composite-multi-column-indexes">Section 3: Composite (Multi-Column) Indexes</h2>
<h3 id="deep-mechanics-2">Deep Mechanics</h3>
<p>A <span>composite index</span> indexes multiple columns together, stored as concatenated keys in the B-Tree. The <span>leftmost prefix rule</span> determines which queries can use the index.</p>
<div>
<h4>Composite Index Key Structure</h4>
<div>
<div>INDEX ON (country, city, postal_code)</div>
<div>
<div>Key: "USA|New York|10001" → RowID: 1847</div>
<div>Key: "USA|New York|10002" → RowID: 2391</div>
<div>Key: "USA|Los Angeles|90001" → RowID: 847</div>
<div>Key: "UK|London|EC1A" → RowID: 5123</div>
</div>
</div>
<p>Keys are sorted lexicographically by concatenated values. Think of it like a phone book sorted by (LastName, FirstName).</p>
</div>
<h3 id="leftmost-prefix-rule-visualization">Leftmost Prefix Rule Visualization</h3>
<div>
<h4>Composite Index: (A, B, C)</h4>
<div>
  <!-- Can Use Index -->
<div>
<h5>
<span>✓</span>
  CAN Use Index
</h5>
<div>
<div>
<code>WHERE A = 1</code>
<div>Uses first column</div>
</div>
<div>
<code>WHERE A = 1 AND B = 2</code>
<div>Uses first two columns</div>
</div>
<div>
<code>WHERE A = 1 AND B = 2 AND C = 3</code>
<div>Uses all columns (best)</div>
</div>
<div>
<code>WHERE A = 1 AND B> 5</code>
<div>Range on B after equality on A</div>
</div>
<div>
<code>WHERE A = 1 ORDER BY B</code>
<div>Index provides sort order</div>
</div>
</div>
</div>
  <!-- Cannot Use Index -->
<div>
<h5>
<span>✗</span>
  CANNOT Use Index Efficiently
</h5>
<div>
<div>
<code>WHERE B = 2</code>
<div>Skips first column A</div>
</div>
<div>
<code>WHERE C = 3</code>
<div>Skips first two columns</div>
</div>
<div>
<code>WHERE B = 2 AND C = 3</code>
<div>Still skips A</div>
</div>
<div>
<code>WHERE A> 1 AND B = 2</code>
<div>Range on A breaks B usage</div>
</div>
<div>
<code>WHERE A = 1 OR B = 2</code>
<div>OR typically prevents index use</div>
</div>
</div>
</div>
</div>
<div>
<strong>Critical Rule:</strong>
<span> Range conditions (>, <, BETWEEN, LIKE 'prefix%') can only use the index up to and including that column. Subsequent columns in the index cannot be used for filtering—only for sorting within matched rows.</span>
</div>
</div>
<h3 id="column-order-strategy">Column Order Strategy</h3>
<pre><code>```sql
-- WRONG: Low selectivity column first
CREATE INDEX idx_orders_status_user ON orders(status, user_id);
-- status has few distinct values (pending, completed, cancelled)
-- Matches millions of rows before filtering by user_id

-- RIGHT: High selectivity column first
CREATE INDEX idx_orders_user_status ON orders(user_id, status);
-- user_id is highly selective (one user among millions)
-- Quickly narrows to user's orders, then filters by status

-- Check column cardinality:
SELECT
COUNT(DISTINCT user_id) as user_cardinality,    -- High (millions)
COUNT(DISTINCT status) as status_cardinality    -- Low (3-5)
FROM orders;
```
</code></pre>
<h3 id="composite-index-interview-questions-3-levels-deep">Composite Index Interview Questions (3 Levels Deep)</h3>
<div>
<h4>Level 1: Why does column order matter in a composite index?</h4>
<p><strong>Answer:</strong> Composite indexes store keys as concatenated values, sorted lexicographically. The index can only be used starting from the <span>leftmost column</span>. An index on (A, B, C) is like a phone book sorted by (LastName, FirstName)—you can't efficiently look up by FirstName alone because entries aren't grouped that way. Column order determines which query patterns the index supports: put equality columns first, range columns after, and the most selective (highest cardinality) columns earliest for best performance.</p>
<div>
<h5>Level 2: Given queries WHERE status = 'active' AND created_at> '2024-01-01' AND user_id = 123, what's the optimal composite index?</h5>
<p><strong>Answer:</strong> The optimal index is <code>CREATE INDEX idx ON orders(user_id, status, created_at)</code>. Strategy: (1) <span>Equality conditions first</span>: user_id and status both use equality (=). (2) <span>Most selective equality first</span>: user_id has higher cardinality than status, so it goes first—filters millions down to hundreds immediately. (3) <span>Range condition last</span>: created_at uses range (>), so it must come after all equality columns. This index: finds user_id=123 entries (very few), within those finds status='active' (even fewer), then scans created_at range. If created_at were earlier in the index, it would break the ability to filter on status. See [[query-optimization]](/topic/sql-learning/query-optimization) for EXPLAIN analysis.</p>
<div>
<h6>Level 3: How do you design indexes when your application has multiple query patterns that conflict (e.g., some filter by A then B, others by B then A)?</h6>
<p><strong>Answer:</strong> This is an <span>index merging vs. multiple indexes</span> trade-off. Options: (1) <strong>Multiple single-column indexes</strong>: Database can use bitmap index scan to combine them. Works for OR conditions but less efficient than composite for AND. (2) <strong>Multiple composite indexes</strong>: Create (A, B) and (B, A). Doubles storage and write overhead but optimal for both patterns. (3) <strong>Prioritize by frequency</strong>: Analyze query logs with pg_stat_statements or slow query log. Index for the 80% case; accept slower performance for edge cases. (4) <strong>Covering indexes with INCLUDE</strong>: If queries differ only in SELECT columns, one covering index may serve both. (5) <strong>Index skip scan</strong> (PostgreSQL 13+): Can sometimes use composite index (A, B) for queries on B alone by scanning each A value—less efficient but better than full scan. Decision matrix: measure storage cost (pg_relation_size), write amplification (inserts/sec × index count), and read latency for each query pattern. Related: [[connection-pooling]](/topic/system-design/connection-pooling) considerations for write-heavy workloads.</p>
</div>
</div>
</div>
<pre><code>---
</code></pre>
<h2 id="section-4-covering-indexes-and-index-only-scans">Section 4: Covering Indexes and Index-Only Scans</h2>
<h3 id="deep-mechanics-3">Deep Mechanics</h3>
<p>A <span>covering index</span> contains all columns needed by a query, eliminating the need to access the heap (table data). This enables an <span>index-only scan</span>—the fastest possible query execution path.</p>
<div>
<h4>Why Covering Indexes Matter</h4>
<div>
<div>
<div>
<strong>Regular Index Scan (2 I/O operations)</strong>
<div>
  1. Read index → Find row pointer<br>
  2. Read table page → Get column values<br>
<span>Random I/O to heap = expensive</span>
</div>
</div>
<div>
<strong>Index-Only Scan (1 I/O operation)</strong>
<div>
  1. Read index → Get all column values directly<br>
<span>No heap access = much faster</span>
</div>
</div>
</div>
</div>
</div>
<h3 id="index-only-scan-flow">Index-Only Scan Flow</h3>
<div>
<h4>Index-Only Scan vs Regular Index Scan</h4>
<div>
  <!-- Regular Index Scan -->
<div>
<h5>Regular Index Scan</h5>
<div>
<div>
<div>Query</div>
<div>SELECT name, email</div>
</div>
<div>↓</div>
<div>
<div>INDEX</div>
<div>idx(id) → RowID</div>
</div>
<div>↓ Heap Lookup</div>
<div>
<div>TABLE (Heap)</div>
<div>Read name, email</div>
</div>
<div>↓</div>
<div>
<div>Result</div>
</div>
</div>
</div>
  <!-- Index-Only Scan -->
<div>
<h5>Index-Only Scan</h5>
<div>
<div>
<div>Query</div>
<div>SELECT name, email</div>
</div>
<div>↓ Direct</div>
<div>
<div>COVERING INDEX</div>
<div>idx(id) INCLUDE(name, email)</div>
</div>
<div>↓ No Heap Access!</div>
<div>
<div>Result</div>
</div>
<div>
<span>50-90% faster!</span>
</div>
</div>
</div>
</div>
</div>
<h3 id="creating-covering-indexes">Creating Covering Indexes</h3>
<pre><code>      ```sql
      -- PostgreSQL 11+ syntax with INCLUDE
      CREATE INDEX idx_orders_covering ON orders(status)
      INCLUDE (order_number, amount, created_at);

      -- Query satisfied entirely from index:
      SELECT order_number, amount, created_at
      FROM orders
      WHERE status = 'pending';
      -- EXPLAIN shows: Index Only Scan

      -- Alternative: All columns in the index key (older approach)
      CREATE INDEX idx_orders_all ON orders(status, order_number, amount, created_at);
      -- Works but: larger index, unnecessary sorting overhead

      -- When INCLUDE is better:
      -- 1. INCLUDE columns don't need to be searchable/sortable
      -- 2. Smaller index keys = more keys per page = shallower tree
      -- 3. No overhead maintaining sort order for INCLUDE columns
      ```
</code></pre>
<h3 id="visibility-map-and-index-only-scans">Visibility Map and Index-Only Scans</h3>
<div>
<h4>PostgreSQL Visibility Map Requirement</h4>
<p>
Index-only scans require checking the <span>visibility map</span> to confirm rows are visible to the transaction. If a page is not "all-visible" (recently modified), PostgreSQL must access the heap to check tuple visibility.
</p>
<div>
<strong>Implication:</strong>
<span> Tables with heavy UPDATE/DELETE activity may not benefit from covering indexes until VACUUM marks pages as all-visible. Monitor with: </span>
<code>SELECT relname, n_tup_upd, n_tup_del FROM pg_stat_user_tables;</code>
</div>
</div>
<h3 id="covering-index-interview-questions-3-levels-deep">Covering Index Interview Questions (3 Levels Deep)</h3>
<div>
<h4>Level 1: What is a covering index and when should you use one?</h4>
<p><strong>Answer:</strong> A covering index contains all columns needed by a query—both the filter columns (in WHERE) and the projected columns (in SELECT). This enables an <span>index-only scan</span> where the database reads only the index without accessing the main table. Use covering indexes for frequently executed queries where heap access is the bottleneck. They're especially valuable for: aggregation queries (COUNT, SUM on indexed columns), high-selectivity lookups returning few columns, and read-heavy workloads where increased index size is acceptable.</p>
<div>
<h5>Level 2: Why does PostgreSQL use INCLUDE columns instead of just adding all columns to the index key?</h5>
<p><strong>Answer:</strong> The INCLUDE clause adds columns to the <span>leaf level only</span>, not to internal nodes. Benefits: (1) <strong>Smaller internal nodes</strong>: Higher fan-out means shallower tree and fewer I/O operations to reach leaves. (2) <strong>No sort overhead</strong>: INCLUDE columns aren't sorted, reducing insert/update cost. (3) <strong>No uniqueness impact</strong>: INCLUDE columns don't affect UNIQUE constraints—useful for adding payload to unique indexes. (4) <strong>Wider applicability</strong>: Can include columns with non-indexable types. Trade-off: INCLUDE columns can't be used for WHERE, ORDER BY, or as the basis for index seek—only for avoiding heap lookup.</p>
<div>
<h6>Level 3: Your EXPLAIN shows "Index Only Scan" but "Heap Fetches: 50000" is high. Diagnose and fix this.</h6>
<p><strong>Answer:</strong> High heap fetches during index-only scan indicates the <span>visibility map</span> is stale—many pages aren't marked "all-visible." Diagnosis: (1) Check <code>pg_stat_user_tables.n_dead_tup</code>—high dead tuple count suggests pending VACUUM. (2) Check <code>pg_visibility</code> extension: <code>SELECT * FROM pg_visibility_map_summary('tablename')</code> shows all-visible vs total pages. (3) Check autovacuum settings—table may be vacuumed infrequently. Fixes: (1) Run <code>VACUUM tablename</code> to mark all-visible pages—heap fetches will drop dramatically. (2) Tune autovacuum: lower <code>autovacuum_vacuum_scale_factor</code> for this table (e.g., 0.01 instead of default 0.2) so VACUUM runs more frequently. (3) For very high-update tables, consider <code>VACUUM (INDEX_CLEANUP OFF)</code> for faster visibility map updates. (4) If updates concentrate on specific rows, those pages will never be all-visible—evaluate if covering index is appropriate. Monitor: <code>pg_stat_user_indexes.idx_tup_fetch</code> (heap fetches) vs <code>idx_tup_read</code> (index entries read).</p>
</div>
</div>
</div>
<hr />
<h2 id="section-5-index-selectivity">Section 5: Index Selectivity</h2>
<h3 id="deep-mechanics-4">Deep Mechanics</h3>
<p><span>Index selectivity</span> measures how effectively an index filters rows. It's calculated as the ratio of distinct values to total rows: <code>selectivity = cardinality / total_rows</code>. A selectivity of 1.0 means every value is unique (perfect for indexing); near 0 means many duplicates (poor for indexing).</p>
<div>
<h4>Selectivity Formula</h4>
<div>
  Selectivity = COUNT(DISTINCT column) / COUNT(*)<br><br>
<span>Higher selectivity = Better index candidate</span>
</div>
</div>
<h3 id="selectivity-examples">Selectivity Examples</h3>
<div>
<h4>Selectivity Spectrum</h4>
<div>
  <!-- Poor selectivity -->
<div>
<div>POOR SELECTIVITY</div>
<div>~0.00001</div>
<div>status, is_active</div>
<div>3-5 distinct values<br>in millions of rows</div>
<div>Index often ignored by planner</div>
</div>
  <!-- Medium selectivity -->
<div>
<div>MEDIUM SELECTIVITY</div>
<div>~0.001</div>
<div>category, country</div>
<div>100-1000 distinct values<br>in millions of rows</div>
<div>Useful with additional filters</div>
</div>
  <!-- Good selectivity -->
<div>
<div>GOOD SELECTIVITY</div>
<div>~0.1</div>
<div>user_id, date</div>
<div>10% of table is distinct</div>
<div>Index likely to be used</div>
</div>
  <!-- Perfect selectivity -->
<div>
<div>PERFECT SELECTIVITY</div>
<div>1.0</div>
<div>email, uuid, pk</div>
<div>Every value unique</div>
<div>Always use index (if queried)</div>
</div>
</div>
</div>
<h3 id="calculating-selectivity">Calculating Selectivity</h3>
<pre><code>              ```sql
              -- Calculate selectivity for all columns in a table
              SELECT
              column_name,
              n_distinct,
              CASE
              WHEN n_distinct &lt; 0 THEN ABS(n_distinct)  -- Negative means fraction
              ELSE n_distinct / (SELECT reltuples FROM pg_class WHERE relname = 'orders')
              END as selectivity
              FROM pg_stats
              WHERE tablename = 'orders'
              ORDER BY selectivity DESC;

              -- Manual calculation
              SELECT
              'email' as column_name,
              COUNT(DISTINCT email)::float / COUNT(*)::float as selectivity
              FROM users;
              -- Result: 0.9999 (nearly unique)

              SELECT
              'status' as column_name,
              COUNT(DISTINCT status)::float / COUNT(*)::float as selectivity
              FROM orders;
              -- Result: 0.000003 (only 3 values in 1M rows)
              ```
</code></pre>
<h3 id="why-low-selectivity-indexes-are-problematic">Why Low-Selectivity Indexes Are Problematic</h3>
<div>
<h4>The Math Behind Selectivity Decisions</h4>
<div>
<p><strong>Table: 1,000,000 rows, 8KB pages, 100 rows/page = 10,000 pages</strong></p>
<div>
<div>
<strong>High Selectivity Query (1 row)</strong>
<ul>
<li>Index lookup: ~3-4 page reads</li>
<li>Heap fetch: 1 page read</li>
<li><strong>Total: ~5 page reads</strong></li>
</ul>
</div>
<div>
<strong>Low Selectivity Query (300,000 rows)</strong>
<ul>
<li>Index scan: read 30% of index</li>
<li>Random heap fetches: ~300,000 random reads!</li>
<li>Sequential scan: 10,000 sequential reads</li>
<li><strong>Sequential scan is 30x faster!</strong></li>
</ul>
</div>
</div>
<p>
<strong>Rule of thumb:</strong> If query returns more than ~10-15% of table rows, sequential scan is usually faster than index scan due to random I/O overhead.
</p>
</div>
</div>
<h3 id="selectivity-interview-questions-3-levels-deep">Selectivity Interview Questions (3 Levels Deep)</h3>
<div>
<h4>Level 1: What is index selectivity and why does it matter for query performance?</h4>
<p><strong>Answer:</strong> <span>Index selectivity</span> measures how well an index discriminates between rows—calculated as distinct values divided by total rows. High selectivity (close to 1.0) means the index quickly narrows down to few rows, making index scans efficient. Low selectivity (close to 0) means many rows match each distinct value, causing excessive random I/O when fetching rows from the heap. The query planner uses selectivity statistics to decide whether to use an index or perform a sequential scan.</p>
<div>
<h5>Level 2: Your query on a boolean column (is_active) has an index but EXPLAIN shows sequential scan. Why and how do you fix it?</h5>
<p><strong>Answer:</strong> Boolean columns have <span>selectivity of 0.5 at best</span> (two values). If 50% of rows are active, an index scan would need random I/O to half the table—slower than sequential scan. Fixes: (1) <strong>Partial index</strong>: <code>CREATE INDEX idx_active ON users(id) WHERE is_active = true</code>—only indexes the minority of rows you actually query. (2) <strong>Composite index</strong>: Combine with a selective column: <code>(user_type, is_active)</code> where user_type has high cardinality. (3) <strong>Covering index</strong>: If query selects few columns, add them to index to enable index-only scan—avoids heap random I/O entirely. (4) <strong>Rethink the query</strong>: If you always filter by is_active=true, consider archiving inactive rows to separate table. Related: [[query-optimization]](/topic/sql-learning/query-optimization).</p>
<div>
<h6>Level 3: How do statistics collection and histogram buckets affect the query planner's selectivity estimates, and how do you diagnose incorrect estimates?</h6>
<p><strong>Answer:</strong> PostgreSQL stores statistics in <code>pg_statistic</code> (accessed via <code>pg_stats</code> view). Key components: (1) <strong>n_distinct</strong>: Estimated distinct values. Positive = absolute count; negative = fraction of rows. (2) <strong>most_common_vals/most_common_freqs</strong>: Top N values and their frequencies for skewed distributions. (3) <strong>histogram_bounds</strong>: Bucket boundaries for non-MCV values. Statistics affect selectivity estimates: For <code>WHERE status = 'pending'</code>, planner checks if 'pending' is in MCV list; if yes, uses stored frequency; otherwise estimates from histogram. <strong>Diagnosing bad estimates</strong>: Run <code>EXPLAIN ANALYZE</code>—compare "estimated rows" vs "actual rows". If off by 10x+, statistics are stale or <code>default_statistics_target</code> is too low. Fixes: (1) <code>ANALYZE tablename</code> to refresh stats. (2) Increase <code>ALTER TABLE t ALTER COLUMN c SET STATISTICS 1000</code> for columns with many distinct values or skewed distributions. (3) For correlated columns where combined selectivity differs from independent assumption, consider extended statistics (PostgreSQL 10+): <code>CREATE STATISTICS stat_name (dependencies) ON col1, col2 FROM table</code>. Monitor with <code>pg_stat_user_tables.last_analyze</code>.</p>
</div>
</div>
</div>
<hr />
<h2 id="section-6-query-planner-and-optimizer">Section 6: Query Planner and Optimizer</h2>
<h3 id="deep-mechanics-5">Deep Mechanics</h3>
<p>The <span>query planner</span> (optimizer) analyzes SQL queries and generates an execution plan that minimizes estimated cost. It evaluates multiple strategies (index scan vs. sequential scan, join algorithms, join order) and selects the plan with lowest cost based on statistics.</p>
<div>
<h4>Query Planner Cost Model</h4>
<div>
<div>
<strong>seq_page_cost = 1.0</strong>
<p>Cost of reading one page sequentially. Baseline cost unit. Sequential reads are efficient (OS readahead).</p>
</div>
<div>
<strong>random_page_cost = 4.0</strong>
<p>Cost of random page read. 4x sequential by default. On SSD, reduce to 1.1-1.5 (random ~= sequential).</p>
</div>
<div>
<strong>cpu_tuple_cost = 0.01</strong>
<p>Cost of processing one row (evaluating conditions, projecting columns). Much cheaper than I/O.</p>
</div>
<div>
<strong>cpu_index_tuple_cost = 0.005</strong>
<p>Cost of processing one index entry. Lower than heap tuple because index entries are smaller.</p>
</div>
</div>
</div>
<h3 id="understanding-explain-output">Understanding EXPLAIN Output</h3>
<div>
<h4>Anatomy of EXPLAIN ANALYZE</h4>
<pre><code>                ```sql
                EXPLAIN (ANALYZE, BUFFERS, FORMAT TEXT)
                SELECT u.name, COUNT(o.id)
                FROM users u
                LEFT JOIN orders o ON u.id = o.user_id
                WHERE u.created_at &gt; '2024-01-01'
                GROUP BY u.id, u.name;
                ```
</code></pre>
<div>
<div><span>HashAggregate</span> (cost=1234.56..1240.00 rows=100 width=40) <span>(actual time=15.2..15.8 rows=95 loops=1)</span></div>
<div>Group Key: u.id, u.name</div>
<div>Buffers: shared hit=45 read=12</div>
<div>-> <span>Hash Left Join</span> (cost=200.00..1200.00 rows=5000 width=32) <span>(actual time=2.1..12.5 rows=4850 loops=1)</span></div>
<div>Hash Cond: (u.id = o.user_id)</div>
<div>Buffers: shared hit=40 read=10</div>
<div>-> <span>Index Scan</span> using idx_users_created on users u (cost=0.42..50.00 rows=100 width=24) <span>(actual time=0.02..0.5 rows=95 loops=1)</span></div>
<div>Index Cond: (created_at> '2024-01-01')</div>
<div>Buffers: shared hit=5</div>
<div>-> <span>Hash</span> (cost=150.00..150.00 rows=5000 width=12) <span>(actual time=1.8..1.8 rows=4850 loops=1)</span></div>
<div>Buckets: 8192 Batches: 1 Memory Usage: 250kB</div>
<div>-> <span>Seq Scan</span> on orders o (cost=0.00..150.00 rows=5000 width=12) <span>(actual time=0.01..1.2 rows=4850 loops=1)</span></div>
<div>Buffers: shared hit=35 read=10</div>
<div>Planning Time: 0.25 ms</div>
<div>Execution Time: 16.1 ms</div>
</div>
<div>
<div>
<strong>Good Signs</strong>
<ul>
<li>Index Scan on filtered columns</li>
<li>Estimated rows ≈ actual rows</li>
<li>shared hit>> shared read</li>
<li>Hash Join for equality joins</li>
</ul>
</div>
<div>
<strong>Warning Signs</strong>
<ul>
<li>Seq Scan on large filtered tables</li>
<li>Nested Loop with high row counts</li>
<li>rows=X estimated vs rows=Y actual (10x+ diff)</li>
<li>Sort with external merge (spilled to disk)</li>
</ul>
</div>
</div>
</div>
<h3 id="scan-types-hierarchy">Scan Types Hierarchy</h3>
<div>
<h4>Scan Types: Best to Worst (for selective queries)</h4>
<div>
<div>
<div>Index Only Scan</div>
<div>
  All data from index. No heap access. Fastest possible. Requires covering index + visibility map.
</div>
</div>
<div>
<div>Index Scan</div>
<div>
  Find rows via index, fetch from heap. Good for selective queries. Random I/O to heap.
</div>
</div>
<div>
<div>Bitmap Index Scan</div>
<div>
  Build bitmap of matching rows, then fetch. Reduces random I/O. Good for OR conditions, medium selectivity.
</div>
</div>
<div>
<div>Sequential Scan</div>
<div>
  Read entire table. Fast for small tables or low selectivity. Benefits from OS readahead.
</div>
</div>
</div>
</div>
<h3 id="query-planner-interview-questions-3-levels-deep">Query Planner Interview Questions (3 Levels Deep)</h3>
<div>
<h4>Level 1: How does the query planner decide between using an index scan vs. sequential scan?</h4>
<p><strong>Answer:</strong> The planner <span>estimates the total cost</span> of each approach using statistics. For index scan: cost = (index pages read × random_page_cost) + (heap pages read × random_page_cost) + (rows processed × cpu_tuple_cost). For sequential scan: cost = (table pages × seq_page_cost) + (all rows × cpu_tuple_cost). If the query returns few rows (high selectivity), index scan wins because it reads fewer pages. If query returns many rows, sequential scan wins because sequential I/O is cheaper than random I/O, and reading everything once beats random heap fetches.</p>
<div>
<h5>Level 2: The planner chose sequential scan even though an index exists. How do you diagnose and potentially fix this?</h5>
<p><strong>Answer:</strong> Diagnosis steps: (1) <strong>Check statistics</strong>: Run <code>ANALYZE tablename</code>—stale stats cause bad estimates. (2) <strong>Check selectivity</strong>: If query matches>10-15% of rows, seq scan may be correct choice. (3) <strong>Check cost parameters</strong>: On SSD, <code>random_page_cost</code> should be 1.1-1.5, not 4.0. (4) <strong>Check EXPLAIN estimates vs. actuals</strong>: Large discrepancy indicates statistics problem. (5) <strong>Check for function on column</strong>: <code>WHERE LOWER(email) = 'x'</code> can't use index on email—need expression index. Fixes: (a) Update statistics, (b) Create more selective composite index, (c) Use <span>partial index</span> if query always filters on constant, (d) Adjust <code>random_page_cost</code> for SSD. Forcing index: <code>SET enable_seqscan = off</code> for testing only—never in production.</p>
<div>
<h6>Level 3: Explain how PostgreSQL handles join ordering for multi-table queries, and what happens when the planner's estimate is wildly wrong?</h6>
<p><strong>Answer:</strong> PostgreSQL uses <span>dynamic programming</span> for join ordering up to <code>geqo_threshold</code> (default 12) tables—evaluating all permutations for optimal plan. Beyond that, it uses GEQO (Genetic Query Optimizer) for heuristic search. Join order matters because it determines intermediate result sizes: joining 1000-row table first, then filtering, is cheaper than starting with 1M-row table. When estimates are wrong, cascade effects occur: underestimated intermediate results → nested loop chosen (expecting few rows) → actually millions of iterations → query takes hours. <strong>Diagnosing</strong>: <code>EXPLAIN ANALYZE</code> shows "estimated rows=100" vs "actual rows=500000". <strong>Fixes</strong>: (1) <code>ANALYZE</code> to refresh stats. (2) Increase <code>default_statistics_target</code> for columns with complex distributions. (3) Create <span>extended statistics</span> for correlated columns: <code>CREATE STATISTICS stat1 (dependencies) ON col1, col2 FROM table</code>. (4) For known-problematic queries, use CTEs with <code>MATERIALIZED</code> hint to force intermediate result calculation, creating optimization fence. (5) Consider pg_hint_plan extension for explicit join hints in production-critical queries. Monitor with [[query-optimization]](/topic/sql-learning/query-optimization) and auto_explain for catching regression.</p>
</div>
</div>
</div>
<hr />
<h2 id="section-7-index-types-comparison">Section 7: Index Types Comparison</h2>
<div>
<h4>Complete Index Types Reference</h4>
<div>
<table>
  <thead>
<tr>
<th>Index Type</th>
<th>Best For</th>
<th>Operations</th>
<th>Example</th>
</tr>
  </thead>
  <tbody>
<tr>
<td><strong>B-Tree</strong></td>
<td>General purpose, most queries</td>
<td>=, <,>, <=,>=, BETWEEN, IN, LIKE 'prefix%', ORDER BY</td>
<td><code>CREATE INDEX idx ON t(col)</code></td>
</tr>
<tr>
<td><strong>Hash</strong></td>
<td>Equality only, high cardinality</td>
<td>= only</td>
<td><code>CREATE INDEX idx ON t USING HASH(col)</code></td>
</tr>
<tr>
<td><strong>GIN</strong></td>
<td>Arrays, JSONB, full-text</td>
<td>@>, <@, &&, @@ (containment, overlap, text search)</td>
<td><code>CREATE INDEX idx ON t USING GIN(tags)</code></td>
</tr>
<tr>
<td><strong>GiST</strong></td>
<td>Geometric, range types, full-text</td>
<td><<,>>, &<, &>, @>, <@, &&, ~= (spatial operations)</td>
<td><code>CREATE INDEX idx ON t USING GIST(location)</code></td>
</tr>
<tr>
<td><strong>BRIN</strong></td>
<td>Large, naturally ordered data (time-series)</td>
<td><,>, BETWEEN on correlated data</td>
<td><code>CREATE INDEX idx ON t USING BRIN(created_at)</code></td>
</tr>
<tr>
<td><strong>SP-GiST</strong></td>
<td>Non-balanced structures (IP addresses, phone numbers)</td>
<td>Similar to GiST, prefix-based</td>
<td><code>CREATE INDEX idx ON t USING SPGIST(ip_addr)</code></td>
</tr>
  </tbody>
</table>
</div>
</div>
<hr />
<h2 id="section-8-index-maintenance-and-monitoring">Section 8: Index Maintenance and Monitoring</h2>
<h3 id="monitoring-index-health">Monitoring Index Health</h3>
<pre><code>              ```sql
              -- Index size and usage statistics
              SELECT
              schemaname,
              relname as table_name,
              indexrelname as index_name,
              pg_size_pretty(pg_relation_size(indexrelid)) as index_size,
              idx_scan as number_of_scans,
              idx_tup_read as tuples_read,
              idx_tup_fetch as tuples_fetched
              FROM pg_stat_user_indexes
              ORDER BY pg_relation_size(indexrelid) DESC
              LIMIT 20;

              -- Find unused indexes (candidates for removal)
              SELECT
              schemaname || '.' || relname as table,
              indexrelname as index,
              pg_size_pretty(pg_relation_size(indexrelid)) as size,
              idx_scan as times_used
              FROM pg_stat_user_indexes
              WHERE idx_scan = 0
              AND indexrelname NOT LIKE '%_pkey'
              AND indexrelname NOT LIKE '%_unique'
              ORDER BY pg_relation_size(indexrelid) DESC;

              -- Index bloat estimation
              SELECT
              nspname || '.' || relname as table,
              pg_size_pretty(pg_relation_size(relid)) as table_size,
              pg_size_pretty(pg_total_relation_size(relid) - pg_relation_size(relid)) as total_index_size,
              round(100 * pg_total_relation_size(relid) / NULLIF(pg_relation_size(relid), 0)) as index_ratio_pct
              FROM pg_stat_user_tables
              JOIN pg_class ON pg_stat_user_tables.relid = pg_class.oid
              WHERE schemaname = 'public'
              ORDER BY pg_total_relation_size(relid) DESC;
              ```
</code></pre>
<h3 id="index-rebuild-strategies">Index Rebuild Strategies</h3>
<div>
<h4>When to Rebuild Indexes</h4>
<div>
<div>
<strong>Index Bloat> 30%</strong>
<p>Dead tuples accumulate from UPDATE/DELETE. Check with pgstattuple extension. REINDEX reclaims space.</p>
</div>
<div>
<strong>After Large Data Changes</strong>
<p>Bulk imports, mass updates, or deletions can leave indexes unoptimized. REINDEX + ANALYZE refreshes structure and statistics.</p>
</div>
<div>
<strong>PostgreSQL Major Upgrade</strong>
<p>New versions may have improved index formats. REINDEX takes advantage of optimizations.</p>
</div>
</div>
</div>
<pre><code>              ```sql
              -- Standard REINDEX (locks table for writes)
              REINDEX INDEX idx_users_email;

              -- Concurrent REINDEX (PostgreSQL 12+) - no lock, but uses more resources
              REINDEX INDEX CONCURRENTLY idx_users_email;

              -- Rebuild all indexes on a table
              REINDEX TABLE users;

              -- Alternative: CREATE INDEX CONCURRENTLY + DROP old index
              -- More control, can rename to maintain same name
              CREATE INDEX CONCURRENTLY idx_users_email_new ON users(email);
              DROP INDEX idx_users_email;
              ALTER INDEX idx_users_email_new RENAME TO idx_users_email;
              ```
</code></pre>
<hr />
<h2 id="best-practices-summary">Best Practices Summary</h2>
<div>
<h4>Index Design Checklist</h4>
<div>
<div>
<h5>DO</h5>
<ul>
<li>Index columns in WHERE, JOIN, ORDER BY</li>
<li>Put equality columns before range columns</li>
<li>Put high-selectivity columns first</li>
<li>Use INCLUDE for covering indexes</li>
<li>Create partial indexes for filtered queries</li>
<li>Monitor index usage with pg_stat_user_indexes</li>
<li>ANALYZE after large data changes</li>
<li>Test with EXPLAIN ANALYZE</li>
</ul>
</div>
<div>
<h5>AVOID</h5>
<ul>
<li>Indexing low-selectivity columns alone</li>
<li>Too many indexes on write-heavy tables</li>
<li>Functions on indexed columns in WHERE</li>
<li>Unused indexes (check pg_stat_user_indexes)</li>
<li>Over-indexing (maintain index sprawl)</li>
<li>Ignoring index bloat in OLTP systems</li>
<li>Forcing index hints in production</li>
<li>Indexing without analyzing query patterns</li>
</ul>
</div>
</div>
</div>
<hr />
<h2 id="related-topics">Related Topics</h2>
<pre><code>              - [[query-optimization]](/topic/sql-learning/query-optimization) - EXPLAIN analysis and query rewriting
              - [[joins-mastery]](/topic/sql-learning/joins-mastery) - Join algorithms and optimization
              - [[sql-fundamentals]](/topic/sql-learning/sql-fundamentals) - SQL basics and syntax
              - [[database-sharding]](/topic/system-design/database-sharding) - Horizontal scaling strategies
              - [[database-replication]](/topic/system-design/database-replication) - Read replicas for query distribution
              - [[caching]](/topic/system-design/caching) - Reducing database load with caching layers
              - [[connection-pooling]](/topic/system-design/connection-pooling) - Managing database connections efficiently
</code></pre>
