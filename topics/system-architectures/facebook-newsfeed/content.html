<h1 id="design-facebook-news-feed">Design Facebook News Feed</h1>
<style>
/* Flow Diagram Styles */
.flow-diagram {
    display: flex;
    flex-direction: column;
    align-items: center;
    gap: 8px;
    padding: 20px;
    background: linear-gradient(135deg, #f8fafc 0%, #f1f5f9 100%);
    border-radius: 12px;
    margin: 16px 0;
    overflow-x: auto;
}
.flow-diagram.horizontal {
    flex-direction: row;
    flex-wrap: wrap;
    justify-content: center;
}
.flow-box {
    padding: 12px 20px;
    border-radius: 8px;
    text-align: center;
    font-weight: 500;
    min-width: 120px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}
.flow-box.primary {
    background: linear-gradient(135deg, #3b82f6 0%, #1d4ed8 100%);
    color: white;
    border: none;
}
.flow-box.success {
    background: linear-gradient(135deg, #22c55e 0%, #16a34a 100%);
    color: white;
    border: none;
}
.flow-box.warning {
    background: linear-gradient(135deg, #f59e0b 0%, #d97706 100%);
    color: white;
    border: none;
}
.flow-box.danger {
    background: linear-gradient(135deg, #ef4444 0%, #dc2626 100%);
    color: white;
    border: none;
}
.flow-box.purple {
    background: linear-gradient(135deg, #8b5cf6 0%, #7c3aed 100%);
    color: white;
    border: none;
}
.flow-box.light {
    background: white;
    border: 2px solid #e2e8f0;
    color: #1e293b;
}
.flow-arrow {
    font-size: 20px;
    color: #64748b;
    font-weight: bold;
}
.flow-arrow.vertical {
    transform: rotate(90deg);
}
@media (max-width: 768px) {
    .flow-diagram {
        padding: 12px;
    }
    .flow-box {
        padding: 8px 12px;
        min-width: 80px;
        font-size: 12px;
    }
    .flow-diagram.horizontal {
        flex-direction: column;
    }
}
</style>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<div>
<ul>
<li><a href="#problem-statement">Problem Statement</a>
<ul>
<li><a href="#core-requirements">Core Requirements</a></li>
<li><a href="#critical-assumptions">Critical Assumptions</a></li>
</ul>
</li>
<li><a href="#high-level-architecture">High-Level Architecture</a>
<ul>
<li><a href="#feed-system-component-flow">Feed System Component Flow</a></li>
</ul>
</li>
<li><a href="#fan-out-strategies">Fan-out Strategies</a>
<ul>
<li><a href="#fan-out-on-write">Fan-out-on-Write (Push Model)</a></li>
<li><a href="#fan-out-on-read">Fan-out-on-Read (Pull Model)</a></li>
<li><a href="#hybrid-model">Hybrid Model</a></li>
</ul>
</li>
<li><a href="#ranking-algorithms">Ranking Algorithms</a>
<ul>
<li><a href="#evolution-of-ranking">Evolution of Feed Ranking</a></li>
<li><a href="#edgerank">EdgeRank: The Foundation</a></li>
<li><a href="#ml-ranking">Modern ML-Based Ranking</a></li>
</ul>
</li>
<li><a href="#caching-strategies">Caching Strategies</a>
<ul>
<li><a href="#multi-layer-cache">Multi-Layer Cache Architecture</a></li>
<li><a href="#feed-cache-implementation">Feed Cache Implementation</a></li>
<li><a href="#cache-stampede">Cache Stampede Prevention</a></li>
<li><a href="#tao-cache">TAO: Facebook's Graph-Aware Cache</a></li>
</ul>
</li>
<li><a href="#social-graph-traversal">Social Graph Traversal</a>
<ul>
<li><a href="#graph-storage-patterns">Graph Storage Patterns</a></li>
<li><a href="#friend-of-friend">Efficient Friend-of-Friend Queries</a></li>
<li><a href="#graph-partitioning">Graph Partitioning for Scale</a></li>
</ul>
</li>
<li><a href="#edge-cases-failure-modes">Edge Cases &amp; Failure Modes</a>
<ul>
<li><a href="#celebrity-spike">Celebrity Posting Spike</a></li>
<li><a href="#cache-stampede-scenarios">Cache Stampede Scenarios</a></li>
<li><a href="#graph-inconsistencies">Social Graph Inconsistencies</a></li>
<li><a href="#ranking-failures">Ranking Model Failures</a></li>
<li><a href="#datacenter-failover">Data Center Failover</a></li>
</ul>
</li>
<li><a href="#scaling-strategies">Scaling Strategies</a>
<ul>
<li><a href="#horizontal-scaling">Horizontal Scaling Patterns</a></li>
<li><a href="#data-partitioning">Data Partitioning Strategy</a></li>
<li><a href="#geographic-distribution">Geographic Distribution</a></li>
<li><a href="#capacity-planning">Capacity Planning</a></li>
</ul>
</li>
<li><a href="#interview-deep-dive">Interview Deep Dive</a>
<ul>
<li><a href="#fanout-questions">Fan-out Strategy Questions</a></li>
<li><a href="#ranking-questions">Ranking Algorithm Questions</a></li>
<li><a href="#caching-questions">Caching Strategy Questions</a></li>
<li><a href="#graph-questions">Social Graph Questions</a></li>
</ul>
</li>
<li><a href="#cross-references">Cross-Referenced Concepts</a></li>
<li><a href="#design-summary">Design Decision Summary</a></li>
</ul>
</div>
<hr />
<h2 id="problem-statement-problem-statement">Problem Statement {#problem-statement}</h2>
<p>Design a personalized news feed system that aggregates, ranks, and delivers relevant content from a user's social connections in near real-time at massive scale.</p>
<div>
<h3 id="core-requirements-core-requirements">Core Requirements {#core-requirements}</h3>
<table>
<thead>
<tr>
<th>Requirement</th>
<th>Target</th>
<th>Implication</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Feed Generation</strong></td>
<td>&lt; 200ms p99 latency</td>
<td>Pre-computation required at scale</td>
</tr>
<tr>
<td><strong>Freshness</strong></td>
<td>&lt; 30s for close friends</td>
<td>Tiered consistency model</td>
</tr>
<tr>
<td><strong>Personalization</strong></td>
<td>Engagement-optimized ranking</td>
<td>ML pipeline with 10K+ features</td>
</tr>
<tr>
<td><strong>Scale</strong></td>
<td>2B+ DAU, 100K+ QPS</td>
<td>Distributed architecture essential</td>
</tr>
<tr>
<td><strong>Availability</strong></td>
<td>99.99% uptime</td>
<td>Graceful degradation patterns</td>
</tr>
</tbody>
</table>
</div>
<div>
<h3 id="critical-assumptions-critical-assumptions">Critical Assumptions {#critical-assumptions}</h3>
<p><strong>Assumption 1: Read-Heavy Workload</strong></p>
<ul>
<li>Typical ratio: 100:1 reads to writes</li>
<li>Users scroll feed 10-50x more often than they post</li>
<li><strong>Design Impact</strong>: Optimize for read path; accept write amplification</li>
</ul>
<p><strong>Assumption 2: Power Law Distribution</strong></p>
<ul>
<li>1% of users (celebrities) have 99% of followers</li>
<li>Most users have &lt; 500 friends</li>
<li><strong>Design Impact</strong>: Hybrid fan-out strategy mandatory</li>
</ul>
<p><strong>Assumption 3: Eventual Consistency Acceptable</strong></p>
<ul>
<li>Users tolerate 30-60 second delays for others' content</li>
<li>Only own posts require immediate visibility</li>
<li><strong>Design Impact</strong>: Aggressive caching with tiered TTLs</li>
</ul>
<p><strong>Assumption 4: Engagement Over Completeness</strong></p>
<ul>
<li>Users prefer relevant subset over complete chronological list</li>
<li>Missing low-relevance posts is acceptable</li>
<li><strong>Design Impact</strong>: Aggressive filtering and ranking</li>
</ul>
</div>
<hr />
<h2 id="high-level-architecture-high-level-architecture">High-Level Architecture {#high-level-architecture}</h2>
<div>
<h3 id="feed-system-component-flow-feed-system-component-flow">Feed System Component Flow {#feed-system-component-flow}</h3>
<div class="flow-diagram">
    <div class="flow-box primary">
        <strong>User Posts Content</strong><br>
        <span>API Gateway</span>
    </div>
    <div class="flow-arrow">&#8595;</div>
    <div class="flow-box success">
        <strong>Ingestion Layer</strong><br>
        <span>Validate & Persist</span>
    </div>
    <div class="flow-arrow">&#8595;</div>
    <div class="flow-box warning">
        <strong>Fan-out Layer</strong><br>
        <span>Push to Followers</span>
    </div>
    <div class="flow-arrow">&#8595;</div>
    <div class="flow-box purple">
        <strong>Aggregation Layer</strong><br>
        <span>Merge Sources</span>
    </div>
    <div class="flow-arrow">&#8595;</div>
    <div class="flow-box danger">
        <strong>Ranking Layer</strong><br>
        <span>ML Scoring</span>
    </div>
    <div class="flow-arrow">&#8595;</div>
    <div class="flow-box primary">
        <strong>Serving Layer</strong><br>
        <span>Cache & Deliver</span>
    </div>
</div>
<div>
<div>
<h4>Layer 1: Ingestion Layer</h4>
<p><strong>Components</strong>: API Gateway, Post Service, Event Bus</p>
<p><strong>Responsibilities</strong>:</p>
<ul>
<li>Validate and persist new posts</li>
<li>Emit events for downstream processing</li>
<li>Handle media upload coordination</li>
</ul>
<p><strong>Key Metrics</strong>: Write latency p99 &lt; 500ms, durability 99.999%</p>
</div>
<div>
<h4>Layer 2: Fan-out Layer</h4>
<p><strong>Components</strong>: Fan-out Service, Social Graph Service, User Segmentation</p>
<p><strong>Responsibilities</strong>:</p>
<ul>
<li>Determine fan-out strategy per post</li>
<li>Execute async writes to follower feeds</li>
<li>Handle celebrity vs normal user bifurcation</li>
</ul>
<p><strong>Key Metrics</strong>: Fan-out completion &lt; 5s for 99% of posts</p>
</div>
<div>
<h4>Layer 3: Aggregation Layer</h4>
<p><strong>Components</strong>: Feed Aggregator, Content Sources, Merge Service</p>
<p><strong>Responsibilities</strong>:</p>
<ul>
<li>Collect posts from push-based feeds</li>
<li>Pull celebrity posts on demand</li>
<li>Merge ads, suggested content, friend posts</li>
</ul>
<p><strong>Key Metrics</strong>: Aggregation latency &lt; 50ms</p>
</div>
<div>
<h4>Layer 4: Ranking Layer</h4>
<p><strong>Components</strong>: Feature Store, ML Inference, Ranking Models</p>
<p><strong>Responsibilities</strong>:</p>
<ul>
<li>Extract real-time features</li>
<li>Score candidates using ML models</li>
<li>Apply business rules and filters</li>
</ul>
<p><strong>Key Metrics</strong>: Ranking latency &lt; 100ms, model freshness &lt; 1 hour</p>
</div>
<div>
<h4>Layer 5: Serving Layer</h4>
<p><strong>Components</strong>: Feed Cache, CDN, Real-time Updates</p>
<p><strong>Responsibilities</strong>:</p>
<ul>
<li>Cache pre-computed feeds</li>
<li>Serve with sub-50ms latency</li>
<li>Push updates via WebSocket/SSE</li>
</ul>
<p><strong>Key Metrics</strong>: Cache hit rate &gt; 99%, serving latency p99 &lt; 100ms</p>
</div>
</div>
</div>
<hr />
<h2 id="fan-out-strategies-the-core-design-decision-fan-out-strategies">Fan-out Strategies: The Core Design Decision {#fan-out-strategies}</h2>
<div>
<h3 id="fan-out-on-write-push-model-fan-out-on-write">Fan-out-on-Write (Push Model) {#fan-out-on-write}</h3>
<div>
<p><strong>Mechanism</strong>: When a user posts, immediately write the post ID to every follower's pre-computed feed list.</p>
<div class="flow-diagram horizontal">
    <div class="flow-box success">
        <strong>User Posts</strong>
    </div>
    <div class="flow-arrow">&#8594;</div>
    <div class="flow-box light">
        <strong>Get Followers</strong><br>
        <span>Social Graph</span>
    </div>
    <div class="flow-arrow">&#8594;</div>
    <div class="flow-box warning">
        <strong>Queue Jobs</strong><br>
        <span>Async</span>
    </div>
    <div class="flow-arrow">&#8594;</div>
    <div class="flow-box primary">
        <strong>Write to Feeds</strong><br>
        <span>Redis ZADD</span>
    </div>
</div>
<pre><code class="language-python">class FanOutOnWriteService:
    &quot;&quot;&quot;
    Push model: Write amplification in exchange for O(1) reads.

    CRITICAL INSIGHT: This is NOT about copying post content.
    We only fan out post IDs (8 bytes each). Content is fetched
    separately and cached.
    &quot;&quot;&quot;

    def on_post_created(self, post: Post):
        # Step 1: Persist the post (source of truth)
        self.post_store.save(post)

        # Step 2: Get follower list (from social graph cache)
        followers = self.social_graph.get_followers(
            post.author_id,
            max_count=self.FANOUT_LIMIT  # Typically 10K-50K
        )

        # Step 3: Async fan-out to avoid blocking write
        for batch in self._batch(followers, size=1000):
            self.queue.enqueue(FanOutJob(
                post_id=post.id,
                follower_ids=batch,
                post_timestamp=post.created_at
            ))

    def execute_fanout(self, job: FanOutJob):
        &quot;&quot;&quot;
        Executed by background workers.

        DATA STRUCTURE: Each user's feed is a sorted set in Redis.
        Score = timestamp, Member = post_id
        &quot;&quot;&quot;
        pipeline = self.redis.pipeline()

        for follower_id in job.follower_ids:
            feed_key = f&quot;feed:{follower_id}&quot;

            # ZADD with timestamp as score
            pipeline.zadd(feed_key, {job.post_id: job.post_timestamp})

            # Trim to keep only recent N posts (memory management)
            pipeline.zremrangebyrank(feed_key, 0, -self.MAX_FEED_SIZE)

        pipeline.execute()
</code></pre>
<p><strong>Trade-off Analysis</strong>:</p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Impact</th>
<th>Mitigation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Write Amplification</td>
<td>O(followers) writes per post</td>
<td>Async processing, batching</td>
</tr>
<tr>
<td>Storage Cost</td>
<td>8 bytes x followers x posts</td>
<td>TTL-based expiration, trimming</td>
</tr>
<tr>
<td>Latency on Write</td>
<td>Can take seconds for popular users</td>
<td>Queue-based async processing</td>
</tr>
<tr>
<td>Read Performance</td>
<td>O(1) - just fetch pre-computed list</td>
<td>This is the benefit</td>
</tr>
<tr>
<td>Consistency</td>
<td>Followers see post at different times</td>
<td>Accept eventual consistency</td>
</tr>
</tbody>
</table>
</div>
<h3 id="fan-out-on-read-pull-model-fan-out-on-read">Fan-out-on-Read (Pull Model) {#fan-out-on-read}</h3>
<div>
<p><strong>Mechanism</strong>: When a user requests their feed, dynamically fetch posts from all accounts they follow and merge/rank on the fly.</p>
<div class="flow-diagram horizontal">
    <div class="flow-box primary">
        <strong>User Requests Feed</strong>
    </div>
    <div class="flow-arrow">&#8594;</div>
    <div class="flow-box light">
        <strong>Get Following</strong><br>
        <span>Social Graph</span>
    </div>
    <div class="flow-arrow">&#8594;</div>
    <div class="flow-box warning">
        <strong>Parallel Fetch</strong><br>
        <span>Posts from Each</span>
    </div>
    <div class="flow-arrow">&#8594;</div>
    <div class="flow-box success">
        <strong>Merge & Rank</strong>
    </div>
</div>
<pre><code class="language-python">class FanOutOnReadService:
    &quot;&quot;&quot;
    Pull model: Read amplification in exchange for O(1) writes.

    CRITICAL INSIGHT: This model is actually viable at scale
    with aggressive caching. Twitter used this for years.
    &quot;&quot;&quot;

    def get_feed(self, user_id: str, limit: int = 20) -&gt; List[Post]:
        # Step 1: Get accounts this user follows
        following = self.social_graph.get_following(user_id)

        # Step 2: Fetch recent posts from each (with caching)
        candidate_posts = []

        # OPTIMIZATION: Parallel fetch with connection pooling
        with ThreadPoolExecutor(max_workers=50) as executor:
            futures = {
                executor.submit(
                    self._get_user_posts_cached,
                    followed_id,
                    since=time.time() - 7 * 86400  # Last 7 days
                ): followed_id
                for followed_id in following
            }

            for future in as_completed(futures, timeout=0.2):
                try:
                    posts = future.result()
                    candidate_posts.extend(posts)
                except TimeoutError:
                    # Skip slow sources - graceful degradation
                    pass

        # Step 3: Merge and rank
        ranked = self.ranker.rank(user_id, candidate_posts)

        return ranked[:limit]

    def _get_user_posts_cached(self, user_id: str, since: float) -&gt; List[Post]:
        &quot;&quot;&quot;
        CACHING STRATEGY: User's posts are user-agnostic.
        One cache entry serves ALL followers of that user.

        This is why pull model scales better than intuition suggests.
        &quot;&quot;&quot;
        cache_key = f&quot;posts:{user_id}:recent&quot;

        cached = self.cache.get(cache_key)
        if cached:
            return [p for p in cached if p.created_at &gt; since]

        posts = self.post_store.get_by_author(user_id, limit=100)
        self.cache.set(cache_key, posts, ttl=60)  # 1 minute TTL

        return [p for p in posts if p.created_at &gt; since]
</code></pre>
<p><strong>Trade-off Analysis</strong>:</p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Impact</th>
<th>Mitigation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Read Amplification</td>
<td>O(following) fetches per request</td>
<td>Parallel fetch, aggressive caching</td>
</tr>
<tr>
<td>Read Latency</td>
<td>Higher baseline (50-200ms)</td>
<td>Pre-computation for active users</td>
</tr>
<tr>
<td>Storage Cost</td>
<td>Minimal - no feed duplication</td>
<td>This is the benefit</td>
</tr>
<tr>
<td>Write Performance</td>
<td>O(1) - just persist the post</td>
<td>This is the benefit</td>
</tr>
<tr>
<td>Cache Efficiency</td>
<td>High - one entry serves many readers</td>
<td>Celebrity posts cached once, read millions of times</td>
</tr>
</tbody>
</table>
</div>
<h3 id="hybrid-model-facebooktwitter-approach-hybrid-model">Hybrid Model (Facebook/Twitter Approach) {#hybrid-model}</h3>
<div>
<p><strong>Mechanism</strong>: Use push for normal users, pull for celebrities. The threshold is tunable based on write capacity.</p>
<div class="flow-diagram">
    <div class="flow-box purple">
        <strong>New Post Created</strong>
    </div>
    <div class="flow-arrow">&#8595;</div>
    <div class="flow-box light">
        <strong>Check Follower Count</strong>
    </div>
    <div>
        <div>
            <span>< 10K followers</span>
            <div class="flow-arrow">&#8595;</div>
            <div class="flow-box success">
                <strong>PUSH</strong><br>
                <span>Full Fan-out</span>
            </div>
        </div>
        <div>
            <span>>= 10K followers</span>
            <div class="flow-arrow">&#8595;</div>
            <div class="flow-box warning">
                <strong>PULL</strong><br>
                <span>Index Only</span>
            </div>
        </div>
    </div>
</div>
<pre><code class="language-python">class HybridFanOutService:
    &quot;&quot;&quot;
    Hybrid model: Best of both worlds.

    KEY INSIGHT: The threshold isn't arbitrary. It's derived from:
    - Write capacity (IOPS available for fan-out)
    - Celebrity post frequency
    - Acceptable fan-out completion time
    &quot;&quot;&quot;

    # Threshold tuning formula:
    # max_followers = (write_capacity * acceptable_delay) / posts_per_second
    # Example: (100K IOPS * 5 seconds) / 100 posts/sec = 5000 followers
    CELEBRITY_THRESHOLD = 10_000

    def on_post_created(self, post: Post):
        author = self.user_store.get(post.author_id)
        follower_count = self.social_graph.get_follower_count(post.author_id)

        if follower_count &lt; self.CELEBRITY_THRESHOLD:
            # Normal user: full fan-out
            self._push_to_followers(post, limit=None)
        else:
            # Celebrity: limited fan-out + indexing for pull
            self._index_celebrity_post(post)

            # Optional: push to &quot;super fans&quot; (high engagement followers)
            super_fans = self.engagement_service.get_top_fans(
                post.author_id,
                limit=1000
            )
            self._push_to_followers(post, follower_ids=super_fans)

    def get_feed(self, user_id: str, limit: int = 20) -&gt; List[Post]:
        # Step 1: Get pre-computed feed (from push)
        push_feed = self._get_cached_feed(user_id)

        # Step 2: Get celebrities this user follows
        celebrities_followed = self.social_graph.get_following(
            user_id,
            filter=lambda u: u.follower_count &gt;= self.CELEBRITY_THRESHOLD
        )

        # Step 3: Pull celebrity posts (heavily cached)
        celebrity_posts = []
        for celeb_id in celebrities_followed:
            posts = self._get_celebrity_posts_cached(celeb_id)
            celebrity_posts.extend(posts)

        # Step 4: Merge and rank
        all_candidates = push_feed + celebrity_posts
        ranked = self.ranker.rank(user_id, all_candidates)

        return ranked[:limit]

    def _get_celebrity_posts_cached(self, celeb_id: str) -&gt; List[Post]:
        &quot;&quot;&quot;
        CRITICAL OPTIMIZATION: Celebrity posts have user-agnostic cache.

        Cache key does NOT include viewer_id because:
        - Post content is same for all viewers
        - Personalization happens in ranking layer
        - One cache entry serves millions of requests

        This is why pull works for celebrities despite huge follower counts.
        &quot;&quot;&quot;
        cache_key = f&quot;celeb_posts:{celeb_id}&quot;

        cached = self.cache.get(cache_key)
        if cached:
            return cached

        posts = self.post_store.get_by_author(celeb_id, limit=50)

        # Longer TTL for celebrities (content changes less frequently per-user)
        self.cache.set(cache_key, posts, ttl=300)  # 5 minutes

        return posts
</code></pre>
<p><strong>Threshold Determination Deep Dive</strong>:</p>
<pre><code class="language-python">def calculate_optimal_threshold(
    write_capacity_iops: int,
    target_fanout_time_seconds: float,
    posts_per_second: float,
    safety_margin: float = 0.7
) -&gt; int:
    &quot;&quot;&quot;
    Derive celebrity threshold from system capacity.

    Formula: threshold = (capacity * time * margin) / post_rate

    Example calculation:
    - Write capacity: 100,000 IOPS
    - Target fan-out time: 5 seconds
    - Posts per second: 1,000
    - Safety margin: 70%

    threshold = (100,000 * 5 * 0.7) / 1,000 = 350 followers

    But this seems too low! The trick is:
    - Not all posts fan out simultaneously
    - Fan-out is batched (1000 writes per batch = 1 IOPS)
    - Most users are inactive at any given time

    Adjusted: threshold = (100,000 * 5 * 0.7 * 1000) / 1,000 = 350,000

    In practice, companies use 1,000 - 50,000 based on:
    - Twitter: ~1,000 (conservative, prioritizes write speed)
    - Instagram: ~10,000 (balanced)
    - Facebook: Dynamic based on user activity patterns
    &quot;&quot;&quot;
    raw_threshold = (write_capacity_iops * target_fanout_time_seconds * safety_margin) / posts_per_second

    # Adjust for batching efficiency
    batch_multiplier = 1000  # Writes per batch

    return int(raw_threshold * batch_multiplier)
</code></pre>
</div>
</div>
<hr />
<h2 id="ranking-algorithms-ranking-algorithms">Ranking Algorithms {#ranking-algorithms}</h2>
<div>
<h3 id="evolution-of-feed-ranking-evolution-of-ranking">Evolution of Feed Ranking {#evolution-of-ranking}</h3>
<div>
<div class="flow-diagram horizontal">
    <div class="flow-box light">
        <strong>Phase 1</strong><br>
        <span>Chronological</span><br>
        <span>Pre-2009</span>
    </div>
    <div class="flow-arrow">&#8594;</div>
    <div class="flow-box warning">
        <strong>Phase 2</strong><br>
        <span>EdgeRank</span><br>
        <span>2009-2013</span>
    </div>
    <div class="flow-arrow">&#8594;</div>
    <div class="flow-box success">
        <strong>Phase 3</strong><br>
        <span>ML Models</span><br>
        <span>2013-Present</span>
    </div>
</div>
<p><strong>Phase 1: Chronological (Pre-2009)</strong></p>
<ul>
<li>Simply sort by timestamp</li>
<li>Works for small networks</li>
<li>Breaks down with high follow counts</li>
</ul>
<p><strong>Phase 2: EdgeRank (2009-2013)</strong></p>
<ul>
<li>Affinity x Weight x Decay formula</li>
<li>Hand-tuned coefficients</li>
<li>Limited personalization</li>
</ul>
<p><strong>Phase 3: Machine Learning (2013-Present)</strong></p>
<ul>
<li>Neural networks with 10,000+ features</li>
<li>Real-time feature computation</li>
<li>Continuous model updates</li>
</ul>
</div>
<h3 id="edgerank-the-foundation-edgerank">EdgeRank: The Foundation {#edgerank}</h3>
<div>
<pre><code class="language-python">class EdgeRankScorer:
    &quot;&quot;&quot;
    Facebook's original ranking algorithm (simplified).

    Score = SUM(affinity * weight * decay) for all edges

    An &quot;edge&quot; is any connection between user and content:
    - Friend posted it
    - Friend liked it
    - Friend commented on it
    - User tagged in it
    &quot;&quot;&quot;

    # Content type weights (hand-tuned based on engagement data)
    CONTENT_WEIGHTS = {
        'video': 1.5,      # Highest engagement
        'photo': 1.2,
        'link': 1.0,
        'status': 0.8,
        'share': 0.7,      # Lower originality
    }

    # Interaction type weights
    INTERACTION_WEIGHTS = {
        'comment': 1.0,    # Highest signal
        'share': 0.8,
        'reaction': 0.5,
        'click': 0.3,
        'view': 0.1,       # Weakest signal
    }

    def calculate_score(self, viewer_id: str, post: Post) -&gt; float:
        &quot;&quot;&quot;
        Calculate EdgeRank score for a single post.

        INSIGHT: This runs for potentially thousands of posts
        per feed request. Must be &lt; 1ms per post.
        &quot;&quot;&quot;
        total_score = 0.0

        # Get all edges connecting viewer to this post
        edges = self._get_edges(viewer_id, post)

        for edge in edges:
            affinity = self._calculate_affinity(viewer_id, edge.source_user_id)
            weight = self._calculate_weight(edge.edge_type, post.content_type)
            decay = self._calculate_decay(edge.timestamp)

            total_score += affinity * weight * decay

        return total_score

    def _calculate_affinity(self, viewer_id: str, author_id: str) -&gt; float:
        &quot;&quot;&quot;
        Affinity score: How close is the viewer to this person?

        Based on historical interaction patterns:
        - How often does viewer interact with author's content?
        - How often do they message?
        - Profile visits, tags, mutual friends

        STORAGE: Pre-computed in a feature store, updated hourly.
        Key: (viewer_id, author_id) -&gt; affinity_score
        &quot;&quot;&quot;
        cache_key = f&quot;affinity:{viewer_id}:{author_id}&quot;

        affinity = self.feature_store.get(cache_key)
        if affinity is None:
            # Fallback: compute from recent interactions
            affinity = self._compute_affinity_realtime(viewer_id, author_id)

        return affinity

    def _calculate_decay(self, timestamp: datetime) -&gt; float:
        &quot;&quot;&quot;
        Time decay: Older content is less relevant.

        Formula: 1 / (1 + hours_old ^ decay_factor)

        TUNING: decay_factor determines how quickly relevance drops
        - 1.0: Gentle decay (1 day old = 0.04 score)
        - 1.5: Moderate decay (1 day old = 0.008 score)
        - 2.0: Aggressive decay (1 day old = 0.002 score)
        &quot;&quot;&quot;
        hours_old = (datetime.now() - timestamp).total_seconds() / 3600
        decay_factor = 1.5

        return 1.0 / (1.0 + hours_old ** decay_factor)
</code></pre>
<p><strong>EdgeRank Limitations</strong>:</p>
<table>
<thead>
<tr>
<th>Limitation</th>
<th>Problem</th>
<th>ML Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hand-tuned weights</td>
<td>Suboptimal, doesn't adapt</td>
<td>Learned weights from data</td>
</tr>
<tr>
<td>Linear combination</td>
<td>Can't capture interactions</td>
<td>Neural networks capture non-linear patterns</td>
</tr>
<tr>
<td>Same weights for all users</td>
<td>No personalization</td>
<td>Per-user or segment models</td>
</tr>
<tr>
<td>Limited features</td>
<td>Missing important signals</td>
<td>10,000+ features</td>
</tr>
<tr>
<td>No feedback loop</td>
<td>Stale over time</td>
<td>Online learning</td>
</tr>
</tbody>
</table>
</div>
<h3 id="modern-ml-based-ranking-ml-ranking">Modern ML-Based Ranking {#ml-ranking}</h3>
<div>
<div class="flow-diagram">
    <div class="flow-box light">
        <strong>Stage 1: Candidate Generation</strong><br>
        <span>1000s &#8594; 500 candidates</span><br>
        <span>Rules + Lightweight Models</span>
    </div>
    <div class="flow-arrow">&#8595;</div>
    <div class="flow-box warning">
        <strong>Stage 2: First-Pass Ranking</strong><br>
        <span>500 &#8594; 50 candidates</span><br>
        <span>Two-Tower Neural Network</span>
    </div>
    <div class="flow-arrow">&#8595;</div>
    <div class="flow-box success">
        <strong>Stage 3: Final Ranking</strong><br>
        <span>50 &#8594; 20 candidates</span><br>
        <span>Heavy Model (10K+ features)</span>
    </div>
    <div class="flow-arrow">&#8595;</div>
    <div class="flow-box primary">
        <strong>Post-Processing</strong><br>
        <span>Diversity, Dedup, Business Rules</span>
    </div>
</div>
<pre><code class="language-python">class MLRankingPipeline:
    &quot;&quot;&quot;
    Production ranking pipeline with multiple stages.

    ARCHITECTURE: Multi-stage funnel for efficiency

    Stage 1: Candidate Generation (1000s -&gt; 500)
    - Simple rules + lightweight models
    - Sub-millisecond per candidate

    Stage 2: First-Pass Ranking (500 -&gt; 50)
    - Lightweight neural network
    - ~1ms per candidate

    Stage 3: Final Ranking (50 -&gt; 20)
    - Heavy model with all features
    - ~10ms per candidate

    This funnel is CRITICAL for latency at scale.
    &quot;&quot;&quot;

    def rank(self, viewer_id: str, candidates: List[Post], limit: int = 20) -&gt; List[Post]:
        # Stage 1: Lightweight filtering
        filtered = self._candidate_generation(viewer_id, candidates)

        # Stage 2: First-pass ranking
        first_pass = self._first_pass_ranking(viewer_id, filtered)

        # Stage 3: Final ranking with full features
        final = self._final_ranking(viewer_id, first_pass[:50])

        # Post-processing: diversity, deduplication
        processed = self._post_process(final)

        return processed[:limit]

    def _candidate_generation(self, viewer_id: str, candidates: List[Post]) -&gt; List[Post]:
        &quot;&quot;&quot;
        Stage 1: Reduce candidate set using cheap signals.

        Rules-based filtering:
        - Remove posts viewer has seen
        - Remove posts from blocked users
        - Remove posts below quality threshold
        - Time-based filtering (e.g., &lt; 7 days old)
        &quot;&quot;&quot;
        filtered = []

        seen_posts = self.seen_service.get_seen_posts(viewer_id)
        blocked_users = self.privacy_service.get_blocked_users(viewer_id)

        for post in candidates:
            if post.id in seen_posts:
                continue
            if post.author_id in blocked_users:
                continue
            if post.quality_score &lt; self.MIN_QUALITY_THRESHOLD:
                continue
            if post.age_hours &gt; 168:  # 7 days
                continue

            filtered.append(post)

        return filtered[:500]  # Cap for next stage

    def _first_pass_ranking(self, viewer_id: str, candidates: List[Post]) -&gt; List[Post]:
        &quot;&quot;&quot;
        Stage 2: Lightweight neural network scoring.

        Model: Two-tower architecture
        - User tower: embeds viewer preferences
        - Item tower: embeds post characteristics
        - Score = dot product of embeddings

        Latency budget: 10ms total for 500 candidates
        &quot;&quot;&quot;
        # Fetch pre-computed user embedding
        user_embedding = self.feature_store.get_user_embedding(viewer_id)

        # Batch compute post embeddings (vectorized)
        post_embeddings = self.item_tower.embed_batch([p.id for p in candidates])

        # Dot product scoring (extremely fast with numpy)
        scores = np.dot(post_embeddings, user_embedding)

        # Sort by score
        scored = list(zip(candidates, scores))
        scored.sort(key=lambda x: x[1], reverse=True)

        return [post for post, score in scored]

    def _final_ranking(self, viewer_id: str, candidates: List[Post]) -&gt; List[Post]:
        &quot;&quot;&quot;
        Stage 3: Full-featured ranking model.

        Feature categories:
        1. User features (who is viewing)
        2. Author features (who posted)
        3. Post features (what content)
        4. Context features (when/where viewing)
        5. Interaction features (viewer-author history)
        6. Network features (mutual friends, groups)

        Total: 10,000+ features
        Model: Deep neural network (transformer-based)
        Latency budget: 50ms total for 50 candidates
        &quot;&quot;&quot;
        features_batch = []

        for post in candidates:
            features = self._extract_all_features(viewer_id, post)
            features_batch.append(features)

        # Batch inference
        scores = self.ranking_model.predict_batch(features_batch)

        scored = list(zip(candidates, scores))
        scored.sort(key=lambda x: x[1], reverse=True)

        return [post for post, score in scored]

    def _extract_all_features(self, viewer_id: str, post: Post) -&gt; Dict:
        &quot;&quot;&quot;
        Feature extraction for final ranking.

        CRITICAL: Features must be pre-computed or very fast to compute.
        Database queries here would kill latency.
        &quot;&quot;&quot;
        return {
            # User features (from feature store)
            'viewer_age_bucket': self.feature_store.get(f'user:{viewer_id}:age_bucket'),
            'viewer_activity_level': self.feature_store.get(f'user:{viewer_id}:activity'),
            'viewer_content_preferences': self.feature_store.get(f'user:{viewer_id}:prefs'),

            # Author features
            'author_follower_count_log': math.log1p(post.author.follower_count),
            'author_post_frequency': self.feature_store.get(f'user:{post.author_id}:post_freq'),
            'author_avg_engagement': self.feature_store.get(f'user:{post.author_id}:avg_eng'),

            # Post features
            'post_type': post.content_type,
            'post_length': len(post.content),
            'has_media': post.has_media,
            'has_link': post.has_link,
            'post_hour': post.created_at.hour,
            'post_day_of_week': post.created_at.weekday(),

            # Real-time engagement (computed in streaming pipeline)
            'likes_velocity': self.realtime_store.get(f'post:{post.id}:like_velocity'),
            'comments_velocity': self.realtime_store.get(f'post:{post.id}:comment_velocity'),
            'current_likes': post.like_count,
            'current_comments': post.comment_count,

            # Interaction features
            'viewer_author_affinity': self.feature_store.get(f'affinity:{viewer_id}:{post.author_id}'),
            'viewer_author_interactions_30d': self.feature_store.get(f'interactions:{viewer_id}:{post.author_id}:30d'),
            'mutual_friends_count': self.feature_store.get(f'mutual:{viewer_id}:{post.author_id}'),

            # Context features
            'viewer_platform': self.context.platform,
            'viewer_time_of_day': datetime.now().hour,
            'viewer_session_depth': self.context.posts_seen_in_session,
        }
</code></pre>
<p><strong>Multi-Objective Ranking</strong>:</p>
<pre><code class="language-python">class MultiObjectiveRanker:
    &quot;&quot;&quot;
    Real ranking optimizes multiple objectives simultaneously.

    Objectives:
    1. Engagement (clicks, likes, comments, shares)
    2. Time spent (dwell time on content)
    3. User satisfaction (from surveys, not leaving app)
    4. Content diversity (avoid filter bubbles)
    5. Business metrics (ad revenue, not covered here)

    CHALLENGE: These objectives can conflict!
    - Clickbait maximizes clicks but hurts satisfaction
    - Controversial content maximizes engagement but harms platform
    &quot;&quot;&quot;

    def compute_final_score(self, viewer_id: str, post: Post, predictions: Dict) -&gt; float:
        &quot;&quot;&quot;
        Combine multiple predicted outcomes into single score.

        Formula: score = sum(weight_i * prediction_i)

        Weights are tuned via:
        - A/B testing different combinations
        - Optimizing for long-term retention
        - Manual adjustment for policy reasons
        &quot;&quot;&quot;
        # Model predictions for different engagement types
        p_click = predictions['probability_click']
        p_like = predictions['probability_like']
        p_comment = predictions['probability_comment']
        p_share = predictions['probability_share']
        p_hide = predictions['probability_hide']  # Negative signal

        # Dwell time prediction
        expected_dwell_time = predictions['expected_dwell_time_seconds']

        # Weighted combination
        engagement_score = (
            0.3 * p_click +
            0.2 * p_like +
            0.3 * p_comment +  # Comments weighted high (strong signal)
            0.2 * p_share
        )

        # Normalize dwell time (cap at 60 seconds)
        dwell_score = min(expected_dwell_time / 60.0, 1.0)

        # Penalty for predicted negative actions
        negative_penalty = 0.5 * p_hide

        # Final score
        final_score = (
            0.6 * engagement_score +
            0.3 * dwell_score -
            0.1 * negative_penalty
        )

        return final_score
</code></pre>
</div>
</div>
<hr />
<h2 id="caching-strategies-caching-strategies">Caching Strategies {#caching-strategies}</h2>
<div>
<h3 id="multi-layer-cache-architecture-multi-layer-cache">Multi-Layer Cache Architecture {#multi-layer-cache}</h3>
<div>
<div class="flow-diagram">
    <div class="flow-box primary">
        <strong>L1: Client/CDN Edge</strong><br>
        <span>< 10ms | 1KB/user</span>
    </div>
    <div class="flow-arrow">&#8595;</div>
    <div class="flow-box success">
        <strong>L2: Regional Cache</strong><br>
        <span>< 20ms | 10KB/user</span>
    </div>
    <div class="flow-arrow">&#8595;</div>
    <div class="flow-box warning">
        <strong>L3: Central Cache</strong><br>
        <span>< 50ms | 100KB/user</span>
    </div>
    <div class="flow-arrow">&#8595;</div>
    <div class="flow-box danger">
        <strong>L4: Database</strong><br>
        <span>< 200ms | Source of Truth</span>
    </div>
</div>
<p><strong>Cache Hierarchy</strong> (See <a href="/topics/system-design/caching">[caching]</a> for fundamentals):</p>
<table>
<thead>
<tr>
<th>Layer</th>
<th>Location</th>
<th>Latency</th>
<th>Size</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td>L1</td>
<td>Client/CDN Edge</td>
<td>&lt; 10ms</td>
<td>1KB/user</td>
<td>Prefetched next page</td>
</tr>
<tr>
<td>L2</td>
<td>Regional Cache</td>
<td>&lt; 20ms</td>
<td>10KB/user</td>
<td>User's ranked feed</td>
</tr>
<tr>
<td>L3</td>
<td>Central Cache</td>
<td>&lt; 50ms</td>
<td>100KB/user</td>
<td>Post content, user data</td>
</tr>
<tr>
<td>L4</td>
<td>Database</td>
<td>&lt; 200ms</td>
<td>Unlimited</td>
<td>Source of truth</td>
</tr>
</tbody>
</table>
</div>
<h3 id="feed-cache-implementation-feed-cache-implementation">Feed Cache Implementation {#feed-cache-implementation}</h3>
<div>
<pre><code class="language-python">class FeedCacheService:
    &quot;&quot;&quot;
    Multi-tier feed caching with intelligent invalidation.

    INSIGHT: Different cache tiers serve different purposes:
    - Feed ID list: Changes frequently, short TTL
    - Post content: Changes rarely, long TTL
    - User metadata: Medium frequency, medium TTL

    Separating these allows independent scaling and invalidation.
    &quot;&quot;&quot;

    def __init__(self):
        # Regional cache (close to users)
        self.feed_cache = Redis(cluster='regional')

        # Central cache (shared across regions)
        self.content_cache = Redis(cluster='central')

        # Local in-memory cache (per-server)
        self.local_cache = TTLCache(maxsize=10000, ttl=10)

    def get_feed(self, user_id: str, page: int = 0, page_size: int = 20) -&gt; List[Post]:
        &quot;&quot;&quot;
        Get user's feed with multi-layer caching.

        Cache strategy:
        1. Check if we have pre-computed ranked feed (post IDs)
        2. Hydrate post IDs with content (separate cache)
        3. Apply real-time filters (seen, blocked, deleted)
        &quot;&quot;&quot;

        # Step 1: Get ranked post IDs
        feed_ids = self._get_feed_ids_cached(user_id, page, page_size)

        if not feed_ids:
            # Cache miss: compute feed on demand
            feed_ids = self._compute_and_cache_feed(user_id)

        # Step 2: Hydrate with post content (batch fetch)
        posts = self._hydrate_posts(feed_ids)

        # Step 3: Apply real-time filters
        posts = self._apply_realtime_filters(user_id, posts)

        return posts

    def _get_feed_ids_cached(self, user_id: str, page: int, page_size: int) -&gt; List[str]:
        &quot;&quot;&quot;
        Fetch pre-computed feed from cache.

        DATA STRUCTURE: Sorted set in Redis
        - Score: ranking score (not timestamp!)
        - Member: post_id

        Using ZREVRANGE for pagination:
        - Page 0: positions 0-19
        - Page 1: positions 20-39
        - etc.
        &quot;&quot;&quot;
        feed_key = f&quot;feed:{user_id}:ranked&quot;

        start = page * page_size
        end = start + page_size - 1

        return self.feed_cache.zrevrange(feed_key, start, end)

    def _hydrate_posts(self, post_ids: List[str]) -&gt; List[Post]:
        &quot;&quot;&quot;
        Fetch post content with batched cache lookup.

        OPTIMIZATION: Use MGET for batch retrieval
        - 20 individual GETs: ~20ms
        - 1 MGET with 20 keys: ~2ms

        Cache key design for content:
        - key: post:{post_id}
        - value: serialized Post object
        - TTL: 1 hour (content rarely changes)
        &quot;&quot;&quot;
        # Try local cache first (sub-millisecond)
        posts = {}
        missing_ids = []

        for post_id in post_ids:
            local_key = f&quot;post:{post_id}&quot;
            if local_key in self.local_cache:
                posts[post_id] = self.local_cache[local_key]
            else:
                missing_ids.append(post_id)

        # Batch fetch missing from Redis
        if missing_ids:
            cache_keys = [f&quot;post:{pid}&quot; for pid in missing_ids]
            cached_values = self.content_cache.mget(cache_keys)

            db_missing = []
            for post_id, cached in zip(missing_ids, cached_values):
                if cached:
                    post = Post.deserialize(cached)
                    posts[post_id] = post
                    self.local_cache[f&quot;post:{post_id}&quot;] = post
                else:
                    db_missing.append(post_id)

            # Fetch remaining from database
            if db_missing:
                db_posts = self.post_store.get_batch(db_missing)
                for post in db_posts:
                    posts[post.id] = post
                    # Populate cache
                    self.content_cache.setex(
                        f&quot;post:{post.id}&quot;,
                        3600,  # 1 hour TTL
                        post.serialize()
                    )
                    self.local_cache[f&quot;post:{post.id}&quot;] = post

        # Return in original order
        return [posts[pid] for pid in post_ids if pid in posts]

    def invalidate_feed(self, user_id: str, reason: str):
        &quot;&quot;&quot;
        Invalidate a user's cached feed.

        WHEN TO INVALIDATE:
        - User follows/unfollows someone
        - User changes privacy settings
        - User marks content as not interested
        - New post from high-affinity connection

        WHEN NOT TO INVALIDATE (use append instead):
        - New post from normal connection
        - Engagement updates (likes, comments)
        - These can be appended or handled at read time
        &quot;&quot;&quot;
        feed_key = f&quot;feed:{user_id}:ranked&quot;

        # Delete the cached feed
        self.feed_cache.delete(feed_key)

        # Track invalidation for monitoring
        self.metrics.increment(
            'feed_cache_invalidation',
            tags={'reason': reason}
        )
</code></pre>
</div>
<h3 id="cache-stampede-prevention-cache-stampede">Cache Stampede Prevention {#cache-stampede}</h3>
<div>
<div class="flow-diagram horizontal">
    <div class="flow-box danger">
        <strong>Cache Expires</strong>
    </div>
    <div class="flow-arrow">&#8594;</div>
    <div class="flow-box light">
        <strong>1000s of Requests</strong>
    </div>
    <div class="flow-arrow">&#8594;</div>
    <div class="flow-box warning">
        <strong>All Hit DB</strong>
    </div>
    <div class="flow-arrow">&#8594;</div>
    <div class="flow-box danger">
        <strong>DB Overwhelmed</strong>
    </div>
</div>
<pre><code class="language-python">class StampedePreventionCache:
    &quot;&quot;&quot;
    Prevent thundering herd on cache expiration.

    PROBLEM: When a popular cache entry expires:
    - 1000s of requests hit simultaneously
    - All see cache miss
    - All compute expensive operation
    - Database overwhelmed

    SOLUTIONS implemented here:
    1. Locking (only one request computes)
    2. Probabilistic early refresh
    3. Stale-while-revalidate
    &quot;&quot;&quot;

    def get_with_lock(self, key: str, compute_fn: Callable, ttl: int) -&gt; Any:
        &quot;&quot;&quot;
        Solution 1: Distributed locking.

        Only one request computes on cache miss.
        Others wait or return stale data.
        &quot;&quot;&quot;
        # Try to get from cache
        cached = self.cache.get(key)
        if cached:
            return cached

        # Try to acquire lock
        lock_key = f&quot;lock:{key}&quot;
        lock_acquired = self.cache.set(lock_key, &quot;1&quot;, nx=True, ex=10)

        if lock_acquired:
            try:
                # We have the lock - compute and cache
                value = compute_fn()
                self.cache.setex(key, ttl, value)
                return value
            finally:
                self.cache.delete(lock_key)
        else:
            # Another request is computing - wait briefly
            for _ in range(10):
                time.sleep(0.05)  # 50ms
                cached = self.cache.get(key)
                if cached:
                    return cached

            # Timeout - compute anyway (fallback)
            return compute_fn()

    def get_with_early_refresh(self, key: str, compute_fn: Callable, ttl: int) -&gt; Any:
        &quot;&quot;&quot;
        Solution 2: Probabilistic early refresh.

        Before cache expires, some requests randomly refresh it.
        Spreads recomputation load, prevents thundering herd.

        Formula: refresh_probability = (ttl_remaining / early_window) * base_probability
        &quot;&quot;&quot;
        cached = self.cache.get(key)
        ttl_remaining = self.cache.ttl(key)

        if cached:
            # Probabilistic early refresh
            early_window = 60  # Start considering refresh 60s before expiry

            if ttl_remaining &lt; early_window:
                # Probability increases as we approach expiration
                refresh_prob = (1 - ttl_remaining / early_window) * 0.1

                if random.random() &lt; refresh_prob:
                    # Refresh in background
                    self._refresh_async(key, compute_fn, ttl)

            return cached

        # Cache miss - compute and cache
        value = compute_fn()
        self.cache.setex(key, ttl, value)
        return value

    def get_stale_while_revalidate(self, key: str, compute_fn: Callable,
                                    ttl: int, stale_ttl: int) -&gt; Any:
        &quot;&quot;&quot;
        Solution 3: Stale-while-revalidate pattern.

        Keep two TTLs:
        - Fresh TTL: Data is fresh, serve directly
        - Stale TTL: Data is stale but usable, serve while refreshing

        This is what CDNs do with stale-while-revalidate header.
        &quot;&quot;&quot;
        cache_key = f&quot;data:{key}&quot;
        meta_key = f&quot;meta:{key}&quot;

        cached = self.cache.get(cache_key)
        meta = self.cache.get(meta_key)  # Contains fresh_until timestamp

        if cached:
            if meta and meta['fresh_until'] &gt; time.time():
                # Still fresh
                return cached
            else:
                # Stale but usable - refresh async
                self._refresh_async(key, compute_fn, ttl)
                return cached

        # No cached data - must compute
        value = compute_fn()
        self.cache.setex(cache_key, ttl + stale_ttl, value)
        self.cache.setex(meta_key, ttl + stale_ttl, {
            'fresh_until': time.time() + ttl
        })
        return value
</code></pre>
</div>
<h3 id="tao-facebooks-graph-aware-cache-tao-cache">TAO: Facebook's Graph-Aware Cache {#tao-cache}</h3>
<div>
<div class="flow-diagram">
    <div>
        <div class="flow-box purple">
            <strong>Leader Cache</strong><br>
            <span>Handles Writes</span>
        </div>
        <div class="flow-box light">
            <strong>Follower Cache 1</strong><br>
            <span>Read Replica</span>
        </div>
        <div class="flow-box light">
            <strong>Follower Cache 2</strong><br>
            <span>Read Replica</span>
        </div>
    </div>
    <div class="flow-arrow">&#8595;</div>
    <div class="flow-box warning">
        <strong>MySQL Cluster</strong><br>
        <span>Source of Truth</span>
    </div>
</div>
<pre><code class="language-python">class TAOCache:
    &quot;&quot;&quot;
    Simplified TAO (The Associations and Objects) implementation.

    TAO is Facebook's distributed data store optimized for social graph.

    KEY INSIGHTS:
    1. Objects (nodes) and Associations (edges) are first-class citizens
    2. Write-through caching with async replication
    3. Read-after-write consistency via leader forwarding
    4. Extremely high cache hit rate (99.8%+)

    Why TAO exists:
    - Social graphs have predictable access patterns
    - Most queries are: &quot;get friends of X&quot;, &quot;get posts by X&quot;
    - Generic databases aren't optimized for these patterns
    &quot;&quot;&quot;

    def __init__(self):
        # Leader cache: handles writes, ensures consistency
        self.leader_cache = Redis(cluster='leader')

        # Follower caches: handle reads, replicated from leader
        self.follower_caches = [
            Redis(cluster=f'follower_{i}')
            for i in range(5)
        ]

        # Persistent storage
        self.db = MySQL(cluster='social_graph')

    # ===== Object Operations =====

    def get_object(self, object_id: str, object_type: str) -&gt; Dict:
        &quot;&quot;&quot;
        Get an object (user, post, page, etc.)

        Read path:
        1. Check local follower cache
        2. If miss, fetch from leader (for consistency)
        3. If leader miss, fetch from DB
        &quot;&quot;&quot;
        cache_key = f&quot;obj:{object_type}:{object_id}&quot;

        # Try follower cache (local, fast)
        follower = self._get_local_follower()
        cached = follower.get(cache_key)
        if cached:
            return json.loads(cached)

        # Try leader cache
        cached = self.leader_cache.get(cache_key)
        if cached:
            # Populate follower cache
            follower.setex(cache_key, 3600, cached)
            return json.loads(cached)

        # Fetch from DB
        obj = self.db.query(
            f&quot;SELECT * FROM {object_type}s WHERE id = %s&quot;,
            object_id
        )

        if obj:
            # Write through to leader
            self.leader_cache.setex(cache_key, 3600, json.dumps(obj))
            # Async replicate to followers
            self._replicate_to_followers(cache_key, obj)

        return obj

    def update_object(self, object_id: str, object_type: str, updates: Dict):
        &quot;&quot;&quot;
        Update an object with write-through caching.

        Write path:
        1. Write to DB (source of truth)
        2. Write to leader cache
        3. Async invalidate follower caches

        CONSISTENCY: Read-after-write guaranteed by reading from leader
        for recently-written data.
        &quot;&quot;&quot;
        cache_key = f&quot;obj:{object_type}:{object_id}&quot;

        # Write to DB first
        self.db.update(object_type + 's', object_id, updates)

        # Get updated object
        updated = self.db.get(object_type + 's', object_id)

        # Write to leader cache
        self.leader_cache.setex(cache_key, 3600, json.dumps(updated))

        # Async invalidate followers (they'll fetch from leader on next read)
        self._invalidate_followers_async(cache_key)

        return updated

    # ===== Association Operations =====

    def get_associations(self, source_id: str, assoc_type: str,
                         limit: int = 100) -&gt; List[Dict]:
        &quot;&quot;&quot;
        Get associations (edges) from an object.

        Example: get_associations(user_id, 'friend') -&gt; list of friends

        DATA STRUCTURE: Sorted set for each (source_id, assoc_type)
        - Score: timestamp or sort order
        - Member: destination_id

        This is THE core operation for social graphs.
        Called billions of times per day.
        &quot;&quot;&quot;
        cache_key = f&quot;assoc:{source_id}:{assoc_type}&quot;

        # Try cache
        cached = self._get_from_any_cache(cache_key)
        if cached:
            return json.loads(cached)[:limit]

        # Fetch from DB
        associations = self.db.query(
            &quot;&quot;&quot;
            SELECT destination_id, data, timestamp
            FROM associations
            WHERE source_id = %s AND type = %s
            ORDER BY timestamp DESC
            LIMIT %s
            &quot;&quot;&quot;,
            source_id, assoc_type, limit
        )

        # Cache the result
        self.leader_cache.setex(cache_key, 3600, json.dumps(associations))

        return associations

    def add_association(self, source_id: str, assoc_type: str,
                        dest_id: str, data: Dict = None):
        &quot;&quot;&quot;
        Add an association (edge).

        Example: add_association(user_a, 'friend', user_b)

        BIDIRECTIONAL: For symmetric relationships (friends),
        we add both directions. Caller handles this.
        &quot;&quot;&quot;
        cache_key = f&quot;assoc:{source_id}:{assoc_type}&quot;

        # Write to DB
        self.db.insert('associations', {
            'source_id': source_id,
            'type': assoc_type,
            'destination_id': dest_id,
            'data': json.dumps(data) if data else None,
            'timestamp': time.time()
        })

        # Invalidate cache (will be repopulated on next read)
        self.leader_cache.delete(cache_key)
        self._invalidate_followers_async(cache_key)

        # For count caches, increment atomically
        count_key = f&quot;assoc_count:{source_id}:{assoc_type}&quot;
        self.leader_cache.incr(count_key)

    def get_association_count(self, source_id: str, assoc_type: str) -&gt; int:
        &quot;&quot;&quot;
        Get count of associations (e.g., friend count, follower count).

        Counts are cached separately from the actual list because:
        - Counts are requested more often
        - Counts don't need the full list data
        - Counts can be updated atomically
        &quot;&quot;&quot;
        count_key = f&quot;assoc_count:{source_id}:{assoc_type}&quot;

        cached = self.leader_cache.get(count_key)
        if cached:
            return int(cached)

        # Compute from DB
        count = self.db.query(
            &quot;SELECT COUNT(*) FROM associations WHERE source_id = %s AND type = %s&quot;,
            source_id, assoc_type
        )[0][0]

        self.leader_cache.setex(count_key, 3600, str(count))
        return count
</code></pre>
<p><strong>TAO Performance Characteristics</strong>:</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Value</th>
<th>How Achieved</th>
</tr>
</thead>
<tbody>
<tr>
<td>Cache hit rate</td>
<td>99.8%</td>
<td>Graph locality, pre-fetching</td>
</tr>
<tr>
<td>Read latency</td>
<td>&lt; 1ms</td>
<td>In-memory cache, optimized data structures</td>
</tr>
<tr>
<td>Write latency</td>
<td>&lt; 10ms</td>
<td>Write-through, async replication</td>
</tr>
<tr>
<td>Consistency</td>
<td>Read-after-write</td>
<td>Leader forwarding for recent writes</td>
</tr>
<tr>
<td>Availability</td>
<td>99.99%</td>
<td>Multi-datacenter replication</td>
</tr>
</tbody>
</table>
</div>
</div>
<hr />
<h2 id="social-graph-traversal-social-graph-traversal">Social Graph Traversal {#social-graph-traversal}</h2>
<div>
<h3 id="graph-storage-patterns-graph-storage-patterns">Graph Storage Patterns {#graph-storage-patterns}</h3>
<div>
<p><strong>Storage Options</strong> (See <a href="/topics/databases/graph-databases">[graph-databases]</a> for more):</p>
<table>
<thead>
<tr>
<th>Approach</th>
<th>Pros</th>
<th>Cons</th>
<th>Use When</th>
</tr>
</thead>
<tbody>
<tr>
<td>Adjacency List in RDBMS</td>
<td>Simple, ACID</td>
<td>Slow multi-hop</td>
<td>&lt; 10M edges</td>
</tr>
<tr>
<td>Redis Sorted Sets</td>
<td>Fast reads, easy scaling</td>
<td>Memory cost</td>
<td>Hot data, &lt; 1B edges</td>
</tr>
<tr>
<td>Graph DB (Neo4j)</td>
<td>Complex traversals</td>
<td>Operational complexity</td>
<td>Recommendation systems</td>
</tr>
<tr>
<td>Custom (TAO)</td>
<td>Optimized for social</td>
<td>Build cost</td>
<td>Facebook scale</td>
</tr>
</tbody>
</table>
</div>
<h3 id="efficient-friend-of-friend-queries-friend-of-friend">Efficient Friend-of-Friend Queries {#friend-of-friend}</h3>
<div>
<pre><code class="language-python">class SocialGraphService:
    &quot;&quot;&quot;
    Social graph operations optimized for feed generation.

    KEY INSIGHT: Most feed operations need only 1-hop traversals.
    Friend-of-friend (2-hop) is expensive but rarely needed for feeds.
    &quot;&quot;&quot;

    def __init__(self):
        self.tao = TAOCache()
        self.follower_count_cache = Redis(cluster='counts')

    def get_friends(self, user_id: str) -&gt; List[str]:
        &quot;&quot;&quot;
        1-hop: Get direct friends.

        This is the most common operation. Must be &lt; 5ms.

        Implementation: Single cache lookup (TAO association query)
        &quot;&quot;&quot;
        return self.tao.get_associations(user_id, 'friend')

    def get_friends_of_friends(self, user_id: str, limit: int = 1000) -&gt; List[str]:
        &quot;&quot;&quot;
        2-hop: Get friends of friends (excluding direct friends).

        Used for: People You May Know, Suggested follows
        NOT used for: Feed generation (too expensive)

        OPTIMIZATION: This is expensive, so we:
        1. Limit first-hop to top 50 friends (by interaction)
        2. Limit second-hop to 50 per friend
        3. Batch all second-hop queries
        4. Deduplicate at the end
        &quot;&quot;&quot;
        # Get direct friends (limited)
        direct_friends = set(self.get_friends(user_id)[:50])

        # Batch fetch friends-of-friends
        fof_set = set()

        # Batch query all friends' friends lists
        with ThreadPoolExecutor(max_workers=20) as executor:
            futures = {
                executor.submit(self.get_friends, friend_id): friend_id
                for friend_id in direct_friends
            }

            for future in as_completed(futures):
                try:
                    friend_friends = future.result()[:50]
                    fof_set.update(friend_friends)
                except Exception:
                    pass  # Skip failures

        # Remove self and direct friends
        fof_set.discard(user_id)
        fof_set -= direct_friends

        return list(fof_set)[:limit]

    def get_mutual_friends(self, user_a: str, user_b: str) -&gt; List[str]:
        &quot;&quot;&quot;
        Get mutual friends between two users.

        Used for: Displaying &quot;N mutual friends&quot;, ranking suggestions

        OPTIMIZATION: Use set intersection at cache layer

        For Redis: SINTER friends:{user_a} friends:{user_b}
        Single network round trip, O(min(n,m)) complexity
        &quot;&quot;&quot;
        friends_a = set(self.get_friends(user_a))
        friends_b = set(self.get_friends(user_b))

        return list(friends_a &amp; friends_b)

    def get_mutual_friends_count(self, user_a: str, user_b: str) -&gt; int:
        &quot;&quot;&quot;
        Get count of mutual friends (without fetching all).

        OPTIMIZATION: Pre-compute for common pairs.

        For high-interaction pairs, cache the count:
        mutual_count:{sorted(user_a, user_b)} -&gt; count
        &quot;&quot;&quot;
        cache_key = f&quot;mutual_count:{':'.join(sorted([user_a, user_b]))}&quot;

        cached = self.follower_count_cache.get(cache_key)
        if cached:
            return int(cached)

        # Compute
        count = len(self.get_mutual_friends(user_a, user_b))

        # Cache for 1 hour (friendships don't change that often)
        self.follower_count_cache.setex(cache_key, 3600, count)

        return count

    def calculate_affinity(self, viewer_id: str, author_id: str) -&gt; float:
        &quot;&quot;&quot;
        Calculate relationship strength for ranking.

        Affinity is a 0-1 score representing how &quot;close&quot; two users are.

        Signals used:
        1. Direct connection (friend/following)
        2. Mutual friends count
        3. Interaction history
        4. Profile visits
        5. Time since last interaction
        &quot;&quot;&quot;
        # Check direct relationship
        is_friend = self._is_friend(viewer_id, author_id)
        is_following = self._is_following(viewer_id, author_id)

        if not is_friend and not is_following:
            return 0.0  # No relationship

        # Get signals from feature store
        mutual_count = self.get_mutual_friends_count(viewer_id, author_id)
        interaction_count = self._get_interaction_count(viewer_id, author_id, days=30)
        days_since_interaction = self._get_days_since_interaction(viewer_id, author_id)

        # Compute affinity score
        # Formula is typically learned from engagement data
        affinity = 0.0

        # Base score for direct connection
        affinity += 0.3 if is_friend else 0.1

        # Mutual friends boost (log scale to avoid domination)
        affinity += 0.2 * math.log1p(mutual_count) / 5.0

        # Interaction recency boost
        affinity += 0.3 * math.log1p(interaction_count) / 4.0

        # Decay based on time since interaction
        recency_decay = 1.0 / (1.0 + days_since_interaction / 7.0)
        affinity += 0.2 * recency_decay

        return min(affinity, 1.0)  # Cap at 1.0
</code></pre>
</div>
<h3 id="graph-partitioning-for-scale-graph-partitioning">Graph Partitioning for Scale {#graph-partitioning}</h3>
<div>
<div class="flow-diagram horizontal">
    <div class="flow-box warning">
        <strong>User A's Shard</strong><br>
        <span>A follows B, C</span>
    </div>
    <div class="flow-box success">
        <strong>User B's Shard</strong><br>
        <span>B followed by A</span>
    </div>
    <div class="flow-box primary">
        <strong>User C's Shard</strong><br>
        <span>C followed by A</span>
    </div>
</div>
<pre><code class="language-python">class GraphPartitioningStrategy:
    &quot;&quot;&quot;
    Partition social graph across multiple shards.

    CHALLENGE: Social graphs are not easily partitionable.
    - Friends are scattered across partitions
    - Any query might need data from multiple partitions

    SOLUTIONS:
    1. User-based sharding (most common)
    2. Geographic sharding (for regional data)
    3. Hybrid with replication of hot edges
    &quot;&quot;&quot;

    # Strategy 1: Consistent hashing by user ID
    def get_partition_for_user(self, user_id: str) -&gt; int:
        &quot;&quot;&quot;
        User-based sharding: All of a user's data on one shard.

        PRO: User's own data is always local
        CON: Friend's data requires cross-shard queries

        In practice: Use caching (TAO) to hide cross-shard latency
        &quot;&quot;&quot;
        hash_value = self._hash(user_id)
        return hash_value % self.NUM_PARTITIONS

    # Strategy 2: Edge colocation
    def get_partition_for_edge(self, source_id: str, dest_id: str) -&gt; int:
        &quot;&quot;&quot;
        Edge colocation: Store edge with source user.

        Query pattern: Usually fetch all edges from one user.
        &quot;Get all friends of user X&quot; is one partition read.
        &quot;Get all users who friended X&quot; might need all partitions.

        TRADE-OFF: Choose based on access pattern.
        Outgoing edges (friends) stored with source.
        Incoming edges (followers) require reverse index.
        &quot;&quot;&quot;
        return self.get_partition_for_user(source_id)

    # Strategy 3: Reverse index for incoming edges
    def add_edge_with_reverse(self, source_id: str, dest_id: str, edge_type: str):
        &quot;&quot;&quot;
        Store edge in two places:
        1. Forward index: (source, type) -&gt; [destinations]
        2. Reverse index: (dest, type) -&gt; [sources]

        Example for &quot;A follows B&quot;:
        Forward: following:A -&gt; [B, ...]   (on A's shard)
        Reverse: followers:B -&gt; [A, ...]   (on B's shard)

        This trades write amplification for read locality.
        &quot;&quot;&quot;
        # Forward edge (user A's outgoing)
        forward_partition = self.get_partition_for_user(source_id)
        self.partitions[forward_partition].add_edge(
            source_id, edge_type, dest_id
        )

        # Reverse edge (user B's incoming)
        reverse_partition = self.get_partition_for_user(dest_id)
        self.partitions[reverse_partition].add_edge(
            dest_id, f&quot;reverse_{edge_type}&quot;, source_id
        )
</code></pre>
<p><strong>Partition Strategy Comparison</strong>:</p>
<table>
<thead>
<tr>
<th>Strategy</th>
<th>Pros</th>
<th>Cons</th>
<th>Best For</th>
</tr>
</thead>
<tbody>
<tr>
<td>User-based</td>
<td>Simple, local user data</td>
<td>Cross-shard for friends</td>
<td>Most social apps</td>
</tr>
<tr>
<td>Edge-based</td>
<td>Locality for edge queries</td>
<td>Complex rebalancing</td>
<td>Graph analytics</td>
</tr>
<tr>
<td>Geographic</td>
<td>Low latency in region</td>
<td>Complex consistency</td>
<td>Global apps</td>
</tr>
<tr>
<td>Hybrid</td>
<td>Balanced trade-offs</td>
<td>Complex to implement</td>
<td>Facebook scale</td>
</tr>
</tbody>
</table>
</div>
</div>
<hr />
<h2 id="edge-cases--failure-modes-edge-cases-failure-modes">Edge Cases &amp; Failure Modes {#edge-cases-failure-modes}</h2>
<div>
<h3 id="celebrity-posting-spike-celebrity-spike">Celebrity Posting Spike {#celebrity-spike}</h3>
<div>
<p><strong>Scenario</strong>: A celebrity with 50M followers posts during a major event (Super Bowl, election night).</p>
<div class="flow-diagram horizontal">
    <div class="flow-box danger">
        <strong>Celebrity Posts</strong>
    </div>
    <div class="flow-arrow">&#8594;</div>
    <div class="flow-box warning">
        <strong>50M Feed Requests</strong><br>
        <span>Within seconds</span>
    </div>
    <div class="flow-arrow">&#8594;</div>
    <div class="flow-box light">
        <strong>Cache Miss Storm</strong>
    </div>
</div>
<p><strong>Problems</strong>:</p>
<ul>
<li>Push model: 50M writes overwhelm fan-out workers</li>
<li>Pull model: Cache not yet warm, DB overloaded</li>
<li>Celebrity post cache: Single hot key, potential hotspot</li>
</ul>
<p><strong>Mitigations</strong>:</p>
<pre><code class="language-python">class CelebrityPostHandler:
    def on_celebrity_post(self, post: Post):
        # 1. Pre-warm cache BEFORE announcing post
        self._warm_celebrity_cache(post)

        # 2. Use multiple cache replicas with consistent hashing
        cache_keys = [f&quot;celeb:{post.author_id}:{i}&quot; for i in range(10)]
        for key in cache_keys:
            self.cache.set(key, post, ttl=300)

        # 3. Rate limit notifications to followers
        self._staged_notification_rollout(post, batch_size=10000)

        # 4. Circuit breaker on fan-out service
        if self.fanout_queue.depth &gt; self.QUEUE_THRESHOLD:
            self._enable_pull_only_mode(post.author_id)
</code></pre>
<p><strong>Key Insight</strong>: For celebrities, switch to pull-only mode during spikes. The content cache handles the read load; don't try to push.</p>
</div>
<h3 id="cache-stampede-scenarios-cache-stampede-scenarios">Cache Stampede Scenarios {#cache-stampede-scenarios}</h3>
<div>
<p><strong>Scenario</strong>: Regional cache cluster fails, 10M users' feed caches are cold.</p>
<p><strong>Problems</strong>:</p>
<ul>
<li>10M simultaneous cache misses</li>
<li>All requests hit central cache, then DB</li>
<li>Cascading failure across tiers</li>
</ul>
<p><strong>Mitigations</strong>:</p>
<pre><code class="language-python">class CacheResilienceHandler:
    def handle_regional_cache_failure(self, region: str):
        # 1. Enable stale serving from backup region
        self._enable_cross_region_reads(region)

        # 2. Implement request coalescing
        # Multiple requests for same key share one computation
        self.enable_request_coalescing()

        # 3. Gradual cache warming (not all at once)
        for user_batch in self._get_active_users_batched(region):
            self._schedule_cache_warmup(user_batch, delay=random.uniform(0, 60))

        # 4. Serve degraded feed (fewer items, no personalization)
        self._enable_degraded_feed_mode(region)
</code></pre>
<p><strong>Degraded Feed Strategy</strong>:</p>
<ul>
<li>Serve top 10 global trending posts (single cache key)</li>
<li>Gradually add personalization as caches warm</li>
<li>Users see &quot;We're catching up on your feed&quot; message</li>
</ul>
</div>
<h3 id="social-graph-inconsistencies-graph-inconsistencies">Social Graph Inconsistencies {#graph-inconsistencies}</h3>
<div>
<p><strong>Scenario</strong>: User A blocks User B, but due to replication lag, B still sees A's posts.</p>
<p><strong>Problems</strong>:</p>
<ul>
<li>Privacy violation</li>
<li>User trust damage</li>
<li>Potential legal issues (GDPR, harassment cases)</li>
</ul>
<p><strong>Mitigations</strong>:</p>
<pre><code class="language-python">class PrivacyEnforcementService:
    def on_block_action(self, blocker_id: str, blocked_id: str):
        # 1. SYNCHRONOUS write to all regions (critical path)
        self._sync_write_block_relationship(blocker_id, blocked_id)

        # 2. Immediately invalidate blocked user's feed cache
        self._invalidate_feed_cache(blocked_id)

        # 3. Add to real-time block list (checked on every feed request)
        self.realtime_blocklist.add(blocker_id, blocked_id)

        # 4. Remove existing content from blocked user's cached feed
        self._scrub_content_from_feed(blocked_id, blocker_id)

    def get_feed_with_privacy_check(self, user_id: str) -&gt; List[Post]:
        feed = self._get_cached_feed(user_id)

        # ALWAYS check real-time blocklist (not cached)
        blocked_by = self.realtime_blocklist.get_blockers(user_id)

        # Filter out posts from anyone who blocked this user
        return [p for p in feed if p.author_id not in blocked_by]
</code></pre>
<p><strong>Key Insight</strong>: Privacy operations are ALWAYS synchronous and bypass cache. Accept higher latency for correctness.</p>
</div>
<h3 id="ranking-model-failures-ranking-failures">Ranking Model Failures {#ranking-failures}</h3>
<div>
<p><strong>Scenario</strong>: ML inference service goes down or returns garbage predictions.</p>
<p><strong>Problems</strong>:</p>
<ul>
<li>No personalization</li>
<li>Random or broken feed ordering</li>
<li>User engagement drops</li>
</ul>
<p><strong>Mitigations</strong>:</p>
<pre><code class="language-python">class RankingFallbackService:
    def rank_with_fallback(self, user_id: str, candidates: List[Post]) -&gt; List[Post]:
        try:
            # Try ML ranking with timeout
            with timeout(50):  # 50ms max
                return self.ml_ranker.rank(user_id, candidates)
        except (TimeoutError, MLServiceError) as e:
            self.metrics.increment('ranking_fallback', tags={'reason': str(e)})
            return self._fallback_ranking(user_id, candidates)

    def _fallback_ranking(self, user_id: str, candidates: List[Post]) -&gt; List[Post]:
        &quot;&quot;&quot;
        Fallback ranking when ML is unavailable.

        Strategy: Use simple heuristics that don't require ML.
        &quot;&quot;&quot;
        scored = []

        for post in candidates:
            score = 0.0

            # Recency (most important fallback signal)
            hours_old = (time.time() - post.created_at) / 3600
            score += 1.0 / (1.0 + hours_old)

            # Engagement velocity (if available)
            if post.like_count &gt; 0:
                score += 0.3 * math.log1p(post.like_count)

            # Direct friend boost
            if self._is_friend(user_id, post.author_id):
                score *= 1.5

            scored.append((post, score))

        scored.sort(key=lambda x: x[1], reverse=True)
        return [post for post, _ in scored]
</code></pre>
<p><strong>Fallback Hierarchy</strong>:</p>
<ol>
<li>Full ML model (default)</li>
<li>Lightweight model (if heavy model fails)</li>
<li>Rule-based scoring (if all ML fails)</li>
<li>Chronological (if all else fails)</li>
</ol>
</div>
<h3 id="data-center-failover-datacenter-failover">Data Center Failover {#datacenter-failover}</h3>
<div>
<p><strong>Scenario</strong>: Entire data center goes offline (power failure, natural disaster).</p>
<div class="flow-diagram">
    <div>
        <div class="flow-box danger">
            <strong>US-EAST (Down)</strong>
        </div>
        <div class="flow-box success">
            <strong>US-WEST (Primary)</strong>
        </div>
        <div class="flow-box primary">
            <strong>EU-WEST (Backup)</strong>
        </div>
    </div>
</div>
<p><strong>Problems</strong>:</p>
<ul>
<li>Users in affected region need to be routed elsewhere</li>
<li>Cross-region latency increases</li>
<li>Replication lag may cause stale data</li>
</ul>
<p><strong>Mitigations</strong>:</p>
<pre><code class="language-python">class DataCenterFailoverService:
    def handle_dc_failure(self, failed_dc: str):
        # 1. Update DNS to route traffic away from failed DC
        self._update_dns_routing(failed_dc, active=False)

        # 2. Identify users assigned to failed DC
        affected_users = self._get_users_by_dc(failed_dc)

        # 3. Pre-warm caches in backup DC for affected users
        backup_dc = self._get_nearest_healthy_dc(failed_dc)
        for user_batch in batch(affected_users, 10000):
            self._warm_user_caches_async(backup_dc, user_batch)

        # 4. Enable cross-region read for consistency
        self._enable_leader_reads_for_affected_users(affected_users)

        # 5. Pause non-critical writes to reduce load
        self._pause_background_jobs(backup_dc)

    def get_feed_during_failover(self, user_id: str) -&gt; List[Post]:
        # Accept slightly stale data during failover
        feed = self._get_feed_from_any_dc(user_id, max_staleness=300)

        if not feed:
            # Return global trending as absolute fallback
            return self._get_trending_posts(limit=20)

        return feed
</code></pre>
<p><strong>Recovery Strategy</strong>:</p>
<ol>
<li>Immediate: Route traffic to healthy DCs</li>
<li>Short-term: Serve degraded/stale content</li>
<li>Medium-term: Warm caches in backup regions</li>
<li>Long-term: Full recovery once DC is back</li>
</ol>
</div>
</div>
<hr />
<h2 id="scaling-strategies-scaling-strategies">Scaling Strategies {#scaling-strategies}</h2>
<div>
<h3 id="horizontal-scaling-patterns-horizontal-scaling">Horizontal Scaling Patterns {#horizontal-scaling}</h3>
<div>
<div class="flow-diagram">
    <div>
        <strong>Scaling Each Layer Independently</strong>
    </div>
    <div>
        <div class="flow-box primary">
            <strong>API Gateway</strong><br>
            <span>Stateless, auto-scale</span>
        </div>
        <div class="flow-box success">
            <strong>Fan-out Workers</strong><br>
            <span>Scale with queue depth</span>
        </div>
        <div class="flow-box warning">
            <strong>Ranking Service</strong><br>
            <span>CPU-bound, GPU optional</span>
        </div>
        <div class="flow-box purple">
            <strong>Cache Cluster</strong><br>
            <span>Memory-bound</span>
        </div>
    </div>
</div>
<pre><code class="language-python">class ScalingController:
    &quot;&quot;&quot;
    Automatic scaling based on service-specific metrics.
    &quot;&quot;&quot;

    SCALING_RULES = {
        'api_gateway': {
            'metric': 'requests_per_second',
            'scale_up_threshold': 10000,
            'scale_down_threshold': 2000,
            'instances_per_10k_rps': 5,
        },
        'fanout_workers': {
            'metric': 'queue_depth',
            'scale_up_threshold': 100000,
            'scale_down_threshold': 10000,
            'instances_per_100k_messages': 10,
        },
        'ranking_service': {
            'metric': 'p99_latency_ms',
            'scale_up_threshold': 80,  # 80ms
            'scale_down_threshold': 30,
            'min_instances': 20,  # Always keep warm
        },
        'cache_cluster': {
            'metric': 'memory_usage_percent',
            'scale_up_threshold': 75,
            'scale_down_threshold': 40,
            'shard_size_gb': 64,
        },
    }

    def evaluate_scaling(self, service: str) -&gt; ScalingDecision:
        rule = self.SCALING_RULES[service]
        current_metric = self.metrics.get(service, rule['metric'])
        current_instances = self.get_instance_count(service)

        if current_metric &gt; rule['scale_up_threshold']:
            return ScalingDecision(
                action='scale_up',
                target_instances=self._calculate_target(service, current_metric)
            )
        elif current_metric &lt; rule['scale_down_threshold']:
            return ScalingDecision(
                action='scale_down',
                target_instances=max(
                    rule.get('min_instances', 2),
                    current_instances - 2
                )
            )

        return ScalingDecision(action='maintain')
</code></pre>
<p><strong>Scaling Triggers by Component</strong>:</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Primary Metric</th>
<th>Secondary Metric</th>
<th>Scale Factor</th>
</tr>
</thead>
<tbody>
<tr>
<td>API Gateway</td>
<td>RPS</td>
<td>Error rate</td>
<td>+5 instances per 10K RPS</td>
</tr>
<tr>
<td>Fan-out Workers</td>
<td>Queue depth</td>
<td>Processing time</td>
<td>+10 per 100K messages</td>
</tr>
<tr>
<td>Ranking Service</td>
<td>p99 latency</td>
<td>CPU usage</td>
<td>+2 when &gt;80ms p99</td>
</tr>
<tr>
<td>Feed Cache</td>
<td>Memory usage</td>
<td>Cache hit rate</td>
<td>+1 shard per 75% memory</td>
</tr>
<tr>
<td>Content Cache</td>
<td>Memory usage</td>
<td>Eviction rate</td>
<td>+1 shard per 10% eviction</td>
</tr>
</tbody>
</table>
</div>
<h3 id="data-partitioning-strategy-data-partitioning">Data Partitioning Strategy {#data-partitioning}</h3>
<div>
<pre><code class="language-python">class DataPartitioningStrategy:
    &quot;&quot;&quot;
    Different data types require different partitioning strategies.
    &quot;&quot;&quot;

    # Feed data: Partition by user_id
    # Reason: A user's feed is always accessed together
    def get_feed_partition(self, user_id: str) -&gt; int:
        return consistent_hash(user_id) % self.NUM_FEED_PARTITIONS

    # Post content: Partition by post_id
    # Reason: Posts are accessed independently, need even distribution
    def get_content_partition(self, post_id: str) -&gt; int:
        return consistent_hash(post_id) % self.NUM_CONTENT_PARTITIONS

    # Social graph: Partition by user_id with edge replication
    # Reason: User's connections accessed together, but need reverse lookups
    def get_graph_partition(self, user_id: str) -&gt; int:
        return consistent_hash(user_id) % self.NUM_GRAPH_PARTITIONS

    # Time-series data (engagement): Partition by time + user
    # Reason: Range queries by time are common
    def get_timeseries_partition(self, user_id: str, timestamp: int) -&gt; int:
        time_bucket = timestamp // (24 * 3600)  # Daily buckets
        return consistent_hash(f&quot;{user_id}:{time_bucket}&quot;) % self.NUM_TS_PARTITIONS
</code></pre>
<p><strong>Partition Sizing Guidelines</strong>:</p>
<table>
<thead>
<tr>
<th>Data Type</th>
<th>Partition Size</th>
<th>Partition Count</th>
<th>Rebalancing Strategy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Feed lists</td>
<td>10GB per shard</td>
<td>100-1000</td>
<td>Consistent hashing</td>
</tr>
<tr>
<td>Post content</td>
<td>100GB per shard</td>
<td>1000-10000</td>
<td>Range-based</td>
</tr>
<tr>
<td>Social graph</td>
<td>50GB per shard</td>
<td>100-500</td>
<td>Hash + replication</td>
</tr>
<tr>
<td>Time-series</td>
<td>50GB per shard</td>
<td>1000+</td>
<td>Time-range partitions</td>
</tr>
</tbody>
</table>
</div>
<h3 id="geographic-distribution-geographic-distribution">Geographic Distribution {#geographic-distribution}</h3>
<div>
<div class="flow-diagram">
    <div>
        <strong>Multi-Region Architecture</strong>
    </div>
    <div>
        <div>
            <strong>US-EAST</strong><br>
            <span>Primary for Americas</span><br>
            <div>
                <span class="flow-box success">Leader DB</span>
            </div>
        </div>
        <div>
            <strong>EU-WEST</strong><br>
            <span>Primary for Europe</span><br>
            <div>
                <span class="flow-box primary">Follower DB</span>
            </div>
        </div>
        <div>
            <strong>AP-SOUTH</strong><br>
            <span>Primary for Asia</span><br>
            <div>
                <span class="flow-box warning">Follower DB</span>
            </div>
        </div>
    </div>
</div>
<pre><code class="language-python">class GeoDistributionService:
    &quot;&quot;&quot;
    Route requests to nearest region while maintaining consistency.
    &quot;&quot;&quot;

    REGIONS = {
        'us-east': {'primary': True, 'serves': ['NA', 'SA']},
        'eu-west': {'primary': False, 'serves': ['EU', 'AF']},
        'ap-south': {'primary': False, 'serves': ['AS', 'OC']},
    }

    def route_read_request(self, user_id: str, user_region: str) -&gt; str:
        &quot;&quot;&quot;
        Route reads to nearest region (latency-optimized).
        &quot;&quot;&quot;
        return self._get_nearest_region(user_region)

    def route_write_request(self, user_id: str, operation: str) -&gt; str:
        &quot;&quot;&quot;
        Route writes based on consistency requirements.
        &quot;&quot;&quot;
        if operation in ['block_user', 'delete_post', 'privacy_change']:
            # Critical operations: write to all regions synchronously
            return 'all_regions'
        else:
            # Normal operations: write to primary, async replicate
            return 'us-east'

    def handle_cross_region_read(self, user_id: str, local_region: str):
        &quot;&quot;&quot;
        Handle case where user's data primary is in different region.
        &quot;&quot;&quot;
        user_primary = self._get_user_primary_region(user_id)

        if user_primary == local_region:
            return self._read_local(user_id)
        else:
            # Check local cache first (may be slightly stale)
            cached = self._read_local_cache(user_id)
            if cached and cached.age_seconds &lt; 60:
                return cached

            # Fallback to cross-region read
            return self._read_from_region(user_id, user_primary)
</code></pre>
<p><strong>Geographic Consistency Model</strong>:</p>
<table>
<thead>
<tr>
<th>Operation</th>
<th>Consistency</th>
<th>Latency</th>
<th>Implementation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Read own feed</td>
<td>Strong</td>
<td>Low</td>
<td>Read from primary region</td>
</tr>
<tr>
<td>Read others' posts</td>
<td>Eventual</td>
<td>Low</td>
<td>Read from local cache</td>
</tr>
<tr>
<td>Post creation</td>
<td>Strong</td>
<td>Medium</td>
<td>Write to primary, async replicate</td>
</tr>
<tr>
<td>Block/Privacy</td>
<td>Strong</td>
<td>High</td>
<td>Synchronous multi-region write</td>
</tr>
</tbody>
</table>
</div>
<h3 id="capacity-planning-capacity-planning">Capacity Planning {#capacity-planning}</h3>
<div>
<pre><code class="language-python">class CapacityPlanner:
    &quot;&quot;&quot;
    Estimate infrastructure requirements based on scale.
    &quot;&quot;&quot;

    def estimate_for_scale(self, daily_active_users: int) -&gt; CapacityEstimate:
        # Feed requests
        feed_requests_per_day = daily_active_users * 20  # 20 feed opens/user/day
        peak_qps = feed_requests_per_day / 86400 * 3  # 3x average for peak

        # Storage
        posts_per_user_per_day = 0.5
        posts_per_day = daily_active_users * posts_per_user_per_day
        post_size_kb = 2  # Average post metadata size

        # Feed cache
        feed_entries_per_user = 500  # Post IDs
        bytes_per_entry = 8
        feed_cache_gb = (daily_active_users * feed_entries_per_user * bytes_per_entry) / 1e9

        # Content cache
        hot_posts = posts_per_day * 7  # Last 7 days
        content_cache_gb = (hot_posts * post_size_kb) / 1e6

        return CapacityEstimate(
            api_servers=math.ceil(peak_qps / 2000),  # 2000 RPS per server
            fanout_workers=math.ceil(posts_per_day / 10000),  # 10K posts/worker/day
            ranking_servers=math.ceil(peak_qps / 500),  # 500 rankings/sec/server
            feed_cache_nodes=math.ceil(feed_cache_gb / 64),  # 64GB per node
            content_cache_nodes=math.ceil(content_cache_gb / 64),
            database_shards=math.ceil(daily_active_users / 10_000_000),  # 10M users/shard
        )
</code></pre>
<p><strong>Capacity Reference Table</strong>:</p>
<table>
<thead>
<tr>
<th>Scale</th>
<th>DAU</th>
<th>Peak QPS</th>
<th>API Servers</th>
<th>Cache Nodes</th>
<th>DB Shards</th>
</tr>
</thead>
<tbody>
<tr>
<td>Startup</td>
<td>100K</td>
<td>500</td>
<td>2</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>Growth</td>
<td>1M</td>
<td>5K</td>
<td>10</td>
<td>8</td>
<td>2</td>
</tr>
<tr>
<td>Scale</td>
<td>10M</td>
<td>50K</td>
<td>50</td>
<td>32</td>
<td>10</td>
</tr>
<tr>
<td>Large</td>
<td>100M</td>
<td>500K</td>
<td>300</td>
<td>200</td>
<td>50</td>
</tr>
<tr>
<td>Facebook</td>
<td>2B</td>
<td>10M</td>
<td>5000+</td>
<td>3000+</td>
<td>500+</td>
</tr>
</tbody>
</table>
</div>
</div>
<hr />
<h2 id="interview-deep-dive-3-level-recursive-questions-interview-deep-dive">Interview Deep Dive: 3-Level Recursive Questions {#interview-deep-dive}</h2>
<div>
<h3 id="section-1-fan-out-strategy-questions-fanout-questions">Section 1: Fan-out Strategy Questions {#fanout-questions}</h3>
<div>
<h4 id="level-1-explain-the-difference-between-fan-out-on-write-and-fan-out-on-read">Level 1: &quot;Explain the difference between fan-out-on-write and fan-out-on-read.&quot;</h4>
<p><strong>Strong Answer</strong>:</p>
<p>&quot;Fan-out-on-write pushes content to followers' pre-computed feeds at write time. When a user posts, we write the post ID to every follower's feed list. This gives O(1) reads but O(N) writes where N is follower count.</p>
<p>Fan-out-on-read computes the feed at read time by fetching posts from all accounts the user follows. This gives O(1) writes but O(M) reads where M is following count.</p>
<p>The key trade-off is write amplification versus read latency. Push is better for read-heavy workloads with bounded follower counts. Pull is better for write-heavy workloads or when users have millions of followers.&quot;</p>
<hr />
<h4 id="level-2-you-mentioned-the-celebrity-problem-with-push-how-exactly-does-the-hybrid-model-handle-a-user-with-10-million-followers-posting">Level 2: &quot;You mentioned the celebrity problem with push. How exactly does the hybrid model handle a user with 10 million followers posting?&quot;</h4>
<p><strong>Strong Answer</strong>:</p>
<p>&quot;For users above a threshold like 10,000 followers, we don't fan out their posts at write time. Instead, we index the post in a celebrity posts store and pull it at read time.</p>
<p>When a user requests their feed, we:</p>
<ol>
<li>Fetch their pre-computed feed from the push pipeline (posts from normal users they follow)</li>
<li>Identify which celebrities they follow</li>
<li>Pull recent posts from those celebrities from the celebrity posts cache</li>
<li>Merge both sets and rank together</li>
</ol>
<p>The celebrity posts cache is user-agnostic, meaning one cache entry serves all 10 million followers. This is why pull actually scales better for celebrities. We might get 10 million reads per post, but they all hit the same cache entry.</p>
<p>For super fans who engage frequently with the celebrity, we might still push to them to improve their experience, but that's a small subset.&quot;</p>
<hr />
<h4 id="level-3-what-happens-if-a-celebritys-cached-posts-expire-and-millions-of-followers-request-their-feed-simultaneously-how-do-you-prevent-thundering-herd">Level 3: &quot;What happens if a celebrity's cached posts expire and millions of followers request their feed simultaneously? How do you prevent thundering herd?&quot;</h4>
<p><strong>Strong Answer</strong>:</p>
<p>&quot;This is the cache stampede problem. Several mitigations work together:</p>
<p><strong>First, probabilistic early refresh</strong>: Before the cache expires, we give some requests a small probability of refreshing it. If TTL is 5 minutes and we're at 4.5 minutes remaining, random requests with 10% probability trigger a background refresh. This spreads the recomputation.</p>
<p><strong>Second, locking with stale serving</strong>: When cache misses, we try to acquire a distributed lock. Only the winner computes. Others either wait briefly or get stale data if available. We use a soft TTL (data is stale but usable) and hard TTL (data must be refreshed).</p>
<p><strong>Third, request coalescing</strong>: At the cache layer, multiple concurrent requests for the same key are collapsed into one. We use a promise or future pattern where the first request creates a promise, and subsequent requests await the same promise.</p>
<p><strong>Fourth, separate freshness tracking</strong>: The content itself has a long TTL, but we track 'fresh_until' separately. This lets us serve 'stale' data while refreshing, following the stale-while-revalidate pattern.</p>
<p>In practice, celebrity posts are so frequently accessed that cache misses are rare. The real scenario is more about handling bursts when a celebrity posts, not cache expiration.&quot;</p>
</div>
<h3 id="section-2-ranking-algorithm-questions-ranking-questions">Section 2: Ranking Algorithm Questions {#ranking-questions}</h3>
<div>
<h4 id="level-1-how-would-you-rank-posts-in-a-news-feed">Level 1: &quot;How would you rank posts in a news feed?&quot;</h4>
<p><strong>Strong Answer</strong>:</p>
<p>&quot;I'd use a multi-stage ranking funnel:</p>
<ol>
<li>
<p><strong>Candidate generation</strong>: Collect posts from followed accounts, filter obvious non-starters like seen content or blocked users. This reduces thousands of candidates to hundreds.</p>
</li>
<li>
<p><strong>Lightweight scoring</strong>: Apply a simple model or heuristic to score candidates. Something like: affinity_with_author * recency_decay * content_type_weight * log(engagement). This reduces to top 50-100.</p>
</li>
<li>
<p><strong>Final ranking</strong>: Apply a full ML model with rich features including user preferences, content embeddings, interaction history, and context. Rank the top candidates.</p>
</li>
<li>
<p><strong>Post-processing</strong>: Apply diversity rules (don't show 5 posts from same person consecutively), business rules (ad placement), and freshness boosts.</p>
</li>
</ol>
<p>For a startup, I'd start with just stage 1 and 2 using hand-tuned weights. ML comes later when we have engagement data to train on.&quot;</p>
<hr />
<h4 id="level-2-you-mentioned-a-multi-stage-funnel-why-not-just-apply-the-full-ml-model-to-all-candidates">Level 2: &quot;You mentioned a multi-stage funnel. Why not just apply the full ML model to all candidates?&quot;</h4>
<p><strong>Strong Answer</strong>:</p>
<p>&quot;Latency and cost constraints make it impractical.</p>
<p>Let's do the math: A user might have 1,000 following and each posted 10 times in the last week. That's 10,000 candidate posts. If our full model takes 10ms per candidate, we'd need 100 seconds just for ranking, which is obviously unacceptable.</p>
<p>The funnel approach exploits the fact that we only need the top 20 posts. We don't need to precisely score the 8,000th best post. So we use cheap signals to discard obviously low-quality candidates early.</p>
<p>Stage 1 uses rules costing microseconds per candidate. Stage 2 uses a lightweight model maybe 0.1ms per candidate, like a two-tower embedding dot product. Stage 3 uses the expensive model but only on 50-100 candidates.</p>
<p>Total: rules on 10,000 (10ms) + lightweight on 500 (50ms) + full model on 50 (500ms) equals roughly 560ms, which is acceptable for pre-computation. For real-time serving, we'd cache the results.</p>
<p>There's also a technique called model distillation where we train a smaller model to mimic the expensive model's outputs. The student model runs faster but captures most of the teacher's accuracy.&quot;</p>
<hr />
<h4 id="level-3-your-ranking-model-needs-features-like-interaction-history-between-viewer-and-author-how-do-you-compute-this-feature-at-serving-time-without-introducing-latency">Level 3: &quot;Your ranking model needs features like 'interaction history between viewer and author'. How do you compute this feature at serving time without introducing latency?&quot;</h4>
<p><strong>Strong Answer</strong>:</p>
<p>&quot;This is where the feature store comes in. We pre-compute features in batch pipelines and store them for real-time lookup.</p>
<p>For interaction history between users A and B, we have a streaming pipeline that processes engagement events. When A likes, comments, or shares B's content, we update a feature in the store:</p>
<pre><code>Key: interaction:{viewer_id}:{author_id}:30d
Value: {likes: 5, comments: 2, shares: 1, last_interaction: timestamp}
</code></pre>
<p>This runs on a stream processing system like Kafka plus Flink. Events flow in, aggregations update, and results go to a low-latency store like Redis or a specialized feature store like Feast or Tecton.</p>
<p>At serving time, fetching this feature is a simple key-value lookup, under 1ms. We batch multiple feature lookups using MGET.</p>
<p>For sparse pairs where A and B rarely interact, we don't store anything. The absence of data is the feature, meaning no recent interactions.</p>
<p>There's a freshness trade-off: streaming updates have seconds of delay, batch updates might be hours old. For critical features like 'did user just block this person', we might have a real-time check. For interaction history, a few minutes of staleness is acceptable.&quot;</p>
</div>
<h3 id="section-3-caching-strategy-questions-caching-questions">Section 3: Caching Strategy Questions {#caching-questions}</h3>
<div>
<h4 id="level-1-how-would-you-cache-the-news-feed">Level 1: &quot;How would you cache the news feed?&quot;</h4>
<p><strong>Strong Answer</strong>:</p>
<p>&quot;I'd use a multi-layer cache strategy:</p>
<p><strong>Layer 1 - Client/CDN</strong>: The device or CDN edge caches the next page of feed. When user scrolls, the next page is already there. TTL is short, maybe 1 minute.</p>
<p><strong>Layer 2 - Feed Cache</strong>: Redis stores the user's pre-computed ranked feed as a sorted set. Key is <code>feed:{user_id}</code>, values are post IDs scored by rank. TTL is 5-15 minutes.</p>
<p><strong>Layer 3 - Content Cache</strong>: Post content is cached separately from feed ordering. Key is <code>post:{post_id}</code>, value is the full post object. TTL is 1 hour since content rarely changes.</p>
<p><strong>Layer 4 - Database</strong>: Source of truth. Only hit on cache misses.</p>
<p>Separating feed ordering from content is crucial. When someone new posts, I might invalidate your feed ordering, but I don't need to invalidate cached post content.&quot;</p>
<hr />
<h4 id="level-2-what-happens-when-a-user-you-follow-unfollows-you-or-blocks-you-how-does-this-affect-the-cache">Level 2: &quot;What happens when a user you follow unfollows you or blocks you? How does this affect the cache?&quot;</h4>
<p><strong>Strong Answer</strong>:</p>
<p>&quot;This is where cache invalidation gets nuanced. Let's break down different scenarios:</p>
<p><strong>User A unfollows User B</strong>: A's feed cache should stop showing B's posts. But immediate invalidation is expensive. Instead, we filter at read time. When fetching A's feed, we also fetch A's current following list. Posts from users not in that list are filtered out. The feed cache naturally refreshes on TTL, and new generations won't include B.</p>
<p><strong>User A blocks User B</strong>: More urgent. B's content should disappear immediately from A's feed. We invalidate A's feed cache and add B to a blocked users cache. At read time, we always filter against the blocked list. We also prevent B from seeing A's content going forward.</p>
<p><strong>User B deletes a post</strong>: We write a tombstone marker and invalidate the post content cache. At feed read time, we filter out tombstones. For push-model feeds, we could asynchronously remove the post ID from affected feeds, but that's expensive. Usually we just let it be filtered at read time.</p>
<p>The principle is: consistency-critical operations like blocks need immediate effect through read-time filtering. Less critical operations like unfollows can be eventually consistent through cache expiration.&quot;</p>
<hr />
<h4 id="level-3-youre-caching-user-specific-feeds-at-facebook-scale-with-2-billion-users-how-do-you-even-fit-this-in-memory-whats-the-storage-strategy">Level 3: &quot;You're caching user-specific feeds. At Facebook scale with 2 billion users, how do you even fit this in memory? What's the storage strategy?&quot;</h4>
<p><strong>Strong Answer</strong>:</p>
<p>&quot;We don't cache everyone's feed. The insight is that most users are not active at any given time.</p>
<p><strong>Active user caching</strong>: We only maintain cached feeds for users who've been active in the last 30 minutes. When a user opens the app, we check for their cached feed. If missing or stale, we compute and cache it. When they close the app, we let the cache expire naturally.</p>
<p><strong>Memory budget math</strong>: Let's say 5% of 2B users are active in a 30-minute window. That's 100M users. Each feed stores 500 post IDs at 8 bytes each, that's 4KB per feed. Total: 400GB. That's achievable with a Redis cluster.</p>
<p><strong>Feed content is separate</strong>: We're only caching post IDs, not content. Post content is cached by post_id, which is user-agnostic. 1 billion posts at 2KB each is 2TB. Still manageable across a cluster.</p>
<p><strong>Tiered storage</strong>: Hot feeds in memory via Redis. Warm feeds on SSD via RocksDB. Cold feeds recomputed on demand. We track access patterns and demote unused feeds.</p>
<p><strong>Compression</strong>: Feed lists compress well since they're sorted integers. Run-length encoding or delta encoding can reduce size by 50-70%.</p>
<p><strong>Geographic distribution</strong>: Users in Europe hit European cache clusters. This reduces cross-region latency and distributes the storage load.</p>
<p>The real scaling secret is that most data is cold. 99.8% cache hit rate means we almost never hit the database. The 0.2% misses are spread across time and easily handled by database capacity.&quot;</p>
</div>
<h3 id="section-4-social-graph-questions-graph-questions">Section 4: Social Graph Questions {#graph-questions}</h3>
<div>
<h4 id="level-1-how-would-you-store-the-social-graph-for-a-facebook-like-application">Level 1: &quot;How would you store the social graph for a Facebook-like application?&quot;</h4>
<p><strong>Strong Answer</strong>:</p>
<p>&quot;The social graph has two main entities: nodes like users, pages, and groups, and edges like friendships, follows, and memberships.</p>
<p>For moderate scale up to 100M users, I'd use PostgreSQL with proper indexing:</p>
<ul>
<li>Users table for nodes</li>
<li>Relationships table for edges with source_id, type, dest_id, and timestamps</li>
<li>Indexes on (source_id, type) for outgoing edge queries</li>
</ul>
<p>For larger scale, I'd use a dedicated graph-aware system. Options include:</p>
<ul>
<li>Neo4j or Amazon Neptune for complex traversals</li>
<li>Redis sorted sets for high-performance simple queries</li>
<li>Custom solution like Facebook's TAO for extreme scale</li>
</ul>
<p>The choice depends on query patterns. If we mostly do one-hop queries like 'get friends', Redis is sufficient and fast. If we need multi-hop traversals like 'friends of friends who also like X', a graph database helps.&quot;</p>
<hr />
<h4 id="level-2-for-the-people-you-may-know-feature-you-need-friends-of-friends-how-do-you-make-that-efficient">Level 2: &quot;For the 'People You May Know' feature, you need friends-of-friends. How do you make that efficient?&quot;</h4>
<p><strong>Strong Answer</strong>:</p>
<p>&quot;Friends-of-friends is a 2-hop traversal. If a user has 500 friends and each friend has 500 friends, we're potentially looking at 250,000 candidates. This is expensive.</p>
<p><strong>Optimization 1 - Limit first hop</strong>: Don't fetch ALL friends. Fetch the 50 friends the user interacts with most. This captures the most relevant social circle.</p>
<p><strong>Optimization 2 - Batch second hop</strong>: Fetch all 50 friends' friend lists in parallel using a connection pool. This turns sequential queries into parallel ones.</p>
<p><strong>Optimization 3 - Pre-computation</strong>: For active users, pre-compute friend-of-friend suggestions daily in a batch job. Store the top 100 suggestions. At serving time, just fetch the pre-computed list.</p>
<p><strong>Optimization 4 - Sampling</strong>: Instead of exact computation, sample. Randomly select 20% of friends and 20% of their friends. With enough samples, we get statistically representative results much faster.</p>
<p><strong>Optimization 5 - Local caching</strong>: Friends lists are relatively stable. Cache them aggressively. The second-hop queries often hit cache.</p>
<p>The key insight is that 'People You May Know' doesn't need to be real-time or exact. A few hours of staleness is fine. Pre-computation during off-peak hours handles most of the work.&quot;</p>
<hr />
<h4 id="level-3-how-do-you-partition-the-social-graph-across-multiple-database-shards-while-maintaining-query-efficiency-for-friendship-lookups">Level 3: &quot;How do you partition the social graph across multiple database shards while maintaining query efficiency for friendship lookups?&quot;</h4>
<p><strong>Strong Answer</strong>:</p>
<p>&quot;Graph partitioning is fundamentally hard because graphs don't partition cleanly. Any edge might cross partition boundaries.</p>
<p><strong>Strategy 1 - User-based sharding</strong>: Partition by user_id. All of User A's outgoing edges are on A's shard. Simple to implement but cross-shard queries needed for 'who follows A' queries.</p>
<p><strong>Strategy 2 - Bidirectional edge storage</strong>: Store each edge twice.</p>
<ul>
<li>Forward: A's shard stores 'A follows B'</li>
<li>Reverse: B's shard stores 'B is followed by A'</li>
</ul>
<p>This trades write amplification for read locality. 'Get who A follows' is one shard. 'Get who follows A' is also one shard. But every follow action writes to two shards.</p>
<p><strong>Strategy 3 - Caching hides partition boundaries</strong>: With 99.8% cache hit rate, cross-shard queries rarely happen. TAO is essentially a distributed cache that hides the complexity of the underlying MySQL shards.</p>
<p><strong>Strategy 4 - Locality-aware placement</strong>: Users who interact frequently should be on the same shard. Run graph clustering algorithms offline to identify communities. Place communities together. This is how Facebook does it with their 'Tao Perf' optimization.</p>
<p><strong>Strategy 5 - Accept some cross-shard queries</strong>: For friend-of-friend, crossing shards is unavoidable. But by batching queries and using parallel fetches, we hide the latency. 50 parallel queries to different shards with 5ms each still complete in 5ms wall-clock time.</p>
<p>The practical answer is: use user-based sharding with bidirectional edges, hide latency with caching, and batch cross-shard queries. This handles 99% of cases. The remaining 1% of complex traversals go to offline processing.&quot;</p>
</div>
</div>
<hr />
<h2 id="cross-referenced-concepts-cross-references">Cross-Referenced Concepts {#cross-references}</h2>
<div>
<h3 id="related-system-design-topics">Related System Design Topics</h3>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Relevance</th>
<th>Link</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Caching</strong></td>
<td>Multi-layer caching for feeds and content</td>
<td><a href="/topics/system-design/caching">[caching]</a></td>
</tr>
<tr>
<td><strong>Message Queues</strong></td>
<td>Async fan-out processing</td>
<td><a href="/topics/system-design/message-queues">[message-queues]</a></td>
</tr>
<tr>
<td><strong>Rate Limiting</strong></td>
<td>Protecting ranking and fan-out services</td>
<td><a href="/topics/system-design/rate-limiting">[rate-limiting]</a></td>
</tr>
<tr>
<td><strong>Consistent Hashing</strong></td>
<td>Graph partitioning, cache distribution</td>
<td><a href="/topics/algorithms/consistent-hashing">[consistent-hashing]</a></td>
</tr>
<tr>
<td><strong>CAP Theorem</strong></td>
<td>Consistency vs availability trade-offs</td>
<td><a href="/topics/system-design/cap-theorem">[cap-theorem]</a></td>
</tr>
<tr>
<td><strong>Load Balancing</strong></td>
<td>Distributing feed requests</td>
<td><a href="/topics/system-design/load-balancing">[load-balancing]</a></td>
</tr>
<tr>
<td><strong>CDN</strong></td>
<td>Edge caching for feed content</td>
<td><a href="/topics/system-design/cdn">[cdn]</a></td>
</tr>
<tr>
<td><strong>Graph Databases</strong></td>
<td>Social graph storage options</td>
<td><a href="/topics/databases/graph-databases">[graph-databases]</a></td>
</tr>
</tbody>
</table>
<h3 id="related-system-architectures">Related System Architectures</h3>
<table>
<thead>
<tr>
<th>System</th>
<th>Shared Patterns</th>
<th>Link</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Twitter Timeline</strong></td>
<td>Fan-out, ranking, real-time updates</td>
<td><a href="/topics/system-architectures/twitter">[twitter]</a></td>
</tr>
<tr>
<td><strong>Instagram Feed</strong></td>
<td>Photo-heavy ranking, stories</td>
<td><a href="/topics/system-architectures/instagram">[instagram]</a></td>
</tr>
<tr>
<td><strong>YouTube Recommendations</strong></td>
<td>ML ranking, candidate generation</td>
<td><a href="/topics/system-architectures/youtube">[youtube]</a></td>
</tr>
<tr>
<td><strong>Notification System</strong></td>
<td>Fan-out, real-time delivery</td>
<td><a href="/topics/system-architectures/notification-system">[notification-system]</a></td>
</tr>
</tbody>
</table>
</div>
<hr />
<h2 id="design-decision-summary-design-summary">Design Decision Summary {#design-summary}</h2>
<div>
<h3 id="when-to-use-each-pattern">When to Use Each Pattern</h3>
<table>
<thead>
<tr>
<th>Scale</th>
<th>Fan-out</th>
<th>Ranking</th>
<th>Cache</th>
<th>Graph Storage</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>&lt; 100K users</strong></td>
<td>Pull only</td>
<td>Chronological or simple scoring</td>
<td>Redis for hot data</td>
<td>PostgreSQL</td>
</tr>
<tr>
<td><strong>100K - 1M</strong></td>
<td>Push for most</td>
<td>Rule-based scoring</td>
<td>Redis cluster</td>
<td>PostgreSQL + Redis</td>
</tr>
<tr>
<td><strong>1M - 100M</strong></td>
<td>Hybrid push/pull</td>
<td>Lightweight ML</td>
<td>Multi-tier cache</td>
<td>Redis + Graph DB</td>
</tr>
<tr>
<td><strong>100M+</strong></td>
<td>Hybrid with dynamic threshold</td>
<td>Full ML pipeline</td>
<td>TAO-like system</td>
<td>Custom distributed</td>
</tr>
</tbody>
</table>
<h3 id="key-trade-offs-to-articulate">Key Trade-offs to Articulate</h3>
<table>
<thead>
<tr>
<th>Trade-off</th>
<th>Choose A When</th>
<th>Choose B When</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Push vs Pull</strong></td>
<td>Bounded follower counts, read-heavy</td>
<td>Celebrity users, write-heavy</td>
</tr>
<tr>
<td><strong>Freshness vs Cost</strong></td>
<td>Users expect real-time</td>
<td>30-60 seconds delay acceptable</td>
</tr>
<tr>
<td><strong>Personalization vs Simplicity</strong></td>
<td>Ad-supported, engagement critical</td>
<td>Startup, building MVP</td>
</tr>
<tr>
<td><strong>Consistency vs Availability</strong></td>
<td>User's own content</td>
<td>Others' content</td>
</tr>
<tr>
<td><strong>Memory vs Compute</strong></td>
<td>Read latency critical</td>
<td>Memory costs prohibitive</td>
</tr>
</tbody>
</table>
<h3 id="red-flags-in-interviews">Red Flags in Interviews</h3>
<table>
<thead>
<tr>
<th>Red Flag</th>
<th>Why It's Wrong</th>
<th>Better Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td>&quot;Always use push model&quot;</td>
<td>Ignores celebrity problem</td>
<td>&quot;Hybrid with threshold tuning&quot;</td>
</tr>
<tr>
<td>&quot;Real-time is required&quot;</td>
<td>Usually not true</td>
<td>&quot;Define freshness SLA first&quot;</td>
</tr>
<tr>
<td>&quot;Graph DB for everything&quot;</td>
<td>Often overkill</td>
<td>&quot;PostgreSQL until 10M+ edges&quot;</td>
</tr>
<tr>
<td>&quot;Full ML ranking from start&quot;</td>
<td>No data to train</td>
<td>&quot;Start with rules, add ML later&quot;</td>
</tr>
<tr>
<td>&quot;Cache everything forever&quot;</td>
<td>Memory and staleness</td>
<td>&quot;Tiered TTL by data type&quot;</td>
</tr>
</tbody>
</table>
</div>
