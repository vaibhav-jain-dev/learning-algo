<h1 id="design-the-twitch-api">Design The Twitch API</h1>
<h2 id="overview">Overview</h2>
<p>A live streaming platform enables creators to broadcast video content to millions of concurrent viewers with real-time chat interaction. The architecture must handle <span><strong>video ingest</strong></span>, <span><strong>real-time transcoding</strong></span>, <span><strong>global CDN delivery</strong></span>, <span><strong>WebSocket-based chat</strong></span>, and <span><strong>personalized recommendations</strong></span>.</p>
<div>
<h4>Core Equation</h4>
<div>
    Streaming Platform = Ingest + Transcode + CDN Delivery + Real-time Chat + VOD Storage + Recommendations
</div>
</div>
<p><strong>Critical Assumption</strong>: The platform assumes viewers have variable network conditions. <span><strong>Adaptive bitrate streaming (ABR)</strong></span> allows seamless quality switching without rebuffering. If network is stable, this complexity adds overhead.</p>
<p><strong>Key Trade-off</strong>: Latency vs. Stability. Lower latency means smaller buffers and more rebuffering risk. Higher latency provides stable playback but reduces interactivity. This trade-off drives most streaming architecture decisions.</p>
<hr />
<h2 id="core-requirements">Core Requirements</h2>
<div>
<h3 id="functional-requirements">Functional Requirements</h3>
<ul>
<li><strong>Live Streaming</strong>: Ingest RTMP, transcode to multiple qualities, deliver via HLS/DASH</li>
<li><strong>Real-time Chat</strong>: Channel chat with sub-second delivery, moderation, emotes</li>
<li><strong>Subscriptions</strong>: Paid subscriber tiers with benefits (emotes, badges, ad-free)</li>
<li><strong>VOD</strong>: Video on demand from past streams with full playback</li>
<li><strong>Clips</strong>: Short highlights extracted from live streams</li>
<li><strong>Discovery</strong>: Browse categories, search, personalized recommendations</li>
</ul>
<h3 id="non-functional-requirements">Non-Functional Requirements</h3>
<ul>
<li><strong>Scale</strong>: 100K+ concurrent streamers, 15M+ concurrent viewers</li>
<li><strong>Latency</strong>: 3-15 seconds stream latency, &lt;100ms chat latency</li>
<li><strong>Availability</strong>: 99.99% for video delivery, 99.9% for chat</li>
<li><strong>Global</strong>: Low-latency access from any geography</li>
</ul>
</div>
<hr />
<h2 id="section-1-high-level-architecture">Section 1: High-Level Architecture</h2>
<h3 id="system-components-overview">System Components Overview</h3>
<div>
<h4>TWITCH STREAMING ARCHITECTURE</h4>
<div>
    <!-- Broadcaster -->
<div>
<div>
<strong>BROADCASTER</strong>
<p>OBS / Streamlabs / Hardware Encoder</p>
<p>RTMP Push to Ingest</p>
</div>
</div>
<div>↓</div>
<pre><code>&lt;!-- Ingest Layer --&gt;
</code></pre>
<div>
<h5>INGEST LAYER (Edge Locations)</h5>
<div>
<div>
<span>US-EAST</span><br>
<span>ingest-iad.twitch.tv</span>
</div>
<div>
<span>US-WEST</span><br>
<span>ingest-lax.twitch.tv</span>
</div>
<div>
<span>EU-WEST</span><br>
<span>ingest-ams.twitch.tv</span>
</div>
<div>
<span>AP-NORTHEAST</span><br>
<span>ingest-tyo.twitch.tv</span>
</div>
</div>
</div>
<div>↓</div>
  <!-- Media Processing -->
<div>
<h5>MEDIA PROCESSING (GPU Clusters)</h5>
<div>
<div>
<strong>Transcoder</strong>
<p>Real-time encoding to multiple quality levels (1080p60, 720p60, 480p30, 360p30, 160p)</p>
</div>
<div>
<strong>Packager</strong>
<p>Segment into HLS (.m3u8 + .ts) with 2-4 second segments for CDN delivery</p>
</div>
</div>
</div>
<div>
<div>↓</div>
<div>↓</div>
</div>
  <!-- Origin + VOD -->
<div>
<div>
<h5>ORIGIN SERVERS</h5>
<p>Live segment cache, playlist generation, CDN origin pull</p>
</div>
<div>
<h5>VOD STORAGE (S3)</h5>
<p>Archived streams, clips, thumbnails, metadata</p>
</div>
</div>
<div>↓</div>
  <!-- CDN Layer -->
<div>
<h5>CDN EDGE LAYER (100+ PoPs Globally)</h5>
<div>
<div>
<span>Edge PoP 1</span>
</div>
<div>
<span>Edge PoP 2</span>
</div>
<div>
<span>Edge PoP 3</span>
</div>
<div>
<span>...</span>
</div>
<div>
<span>Edge PoP N</span>
</div>
</div>
<p>~95% cache hit rate for popular streams</p>
</div>
<div>↓</div>
  <!-- Viewers -->
<div>
<div>
<strong>VIEWERS (Millions)</strong>
<p>HLS playback via native player or hls.js</p>
</div>
</div>
</div>
</div>
<h3 id="related-concepts">Related Concepts</h3>
<p>This architecture leverages <a href="/topic/system-design/cdn">[CDN]</a> for edge caching, <a href="/topic/system-design/message-queues">[message-queues]</a> for chat distribution, <a href="/topic/system-design/load-balancing">[load-balancing]</a> for ingest server selection, and <a href="/topic/system-design/rate-limiting">[rate-limiting]</a> for chat flood protection.</p>
<hr />
<h2 id="section-2-live-streaming-pipeline">Section 2: Live Streaming Pipeline</h2>
<h3 id="deep-mechanics">Deep Mechanics</h3>
<p>The live streaming pipeline transforms a single <span><strong>RTMP stream</strong></span> from the broadcaster into multiple <span><strong>HLS streams</strong></span> at different quality levels, delivered globally through CDN edge servers. The pipeline operates in real-time with strict latency budgets.</p>
<div>
<h4>VIDEO PROCESSING PIPELINE</h4>
<div>
  <!-- Ingest Stage -->
<div>
<h5>STAGE 1: INGEST (from broadcaster)</h5>
<div>
<div>
<p><strong>Protocol:</strong> RTMP (Real-Time Messaging Protocol)</p>
<p><strong>Input:</strong> 1080p60, 6000 kbps, x264/NVENC</p>
<p><strong>Auth:</strong> Stream key validation</p>
</div>
<div>
<span>rtmp://ingest-nyc.twitch.tv/app/</span><br>
<span>live_abc123_streamkey</span>
</div>
</div>
</div>
  <!-- Transcode Stage -->
<div>
<h5>STAGE 2: TRANSCODE (real-time ABR ladder)</h5>
<div>
<div>
<span>1080p60</span><br>
<span>6000 kbps</span><br>
<span>Source</span>
</div>
<div>
<span>720p60</span><br>
<span>3000 kbps</span><br>
<span>High</span>
</div>
<div>
<span>480p30</span><br>
<span>1500 kbps</span><br>
<span>Medium</span>
</div>
<div>
<span>360p30</span><br>
<span>800 kbps</span><br>
<span>Low</span>
</div>
<div>
<span>160p</span><br>
<span>400 kbps</span><br>
<span>Audio-only</span>
</div>
</div>
<p><strong>Note:</strong> Partners get full ladder. Affiliates get limited transcodes. Regular streamers may get source-only.</p>
</div>
  <!-- Package Stage -->
<div>
<h5>STAGE 3: PACKAGE (HLS segmentation)</h5>
<div>
<div>
<p><strong>Format:</strong> HLS (.m3u8 manifest + .ts segments)</p>
<p><strong>Segment Duration:</strong> 2-4 seconds (normal), 1-2s (low-latency)</p>
<p><strong>Latency:</strong> 10-15s normal, 3-5s low-latency mode</p>
</div>
<div>
<span>#EXTM3U</span><br>
<span>#EXT-X-VERSION:3</span><br>
<span>#EXT-X-TARGETDURATION:4</span><br>
<span>#EXTINF:4.000,</span><br>
<span>segment_00001.ts</span><br>
<span>#EXTINF:4.000,</span><br>
<span>segment_00002.ts</span>
</div>
</div>
</div>
</div>
</div>
<h3 id="latency-stack-breakdown">Latency Stack Breakdown</h3>
<div>
<h4>Where Latency Comes From</h4>
<div>
<div>
<div>Encoding</div>
<div>
<span><strong>1-2 seconds</strong> - Broadcaster's encoder (OBS) buffers frames before sending</span>
</div>
</div>
<div>
<div>Network</div>
<div>
<span><strong>0.1-0.5 seconds</strong> - RTMP upload from broadcaster to ingest server</span>
</div>
</div>
<div>
<div>Transcoding</div>
<div>
<span><strong>1-2 seconds</strong> - GPU cluster processes and outputs multiple quality levels</span>
</div>
</div>
<div>
<div>Segmentation</div>
<div>
<span><strong>2-4 seconds</strong> - Must wait for full segment before it can be delivered</span>
</div>
</div>
<div>
<div>CDN</div>
<div>
<span><strong>0.1-0.5 seconds</strong> - Propagation from origin to edge + edge to viewer</span>
</div>
</div>
<div>
<div>Player Buffer</div>
<div>
<span><strong>4-8 seconds</strong> - Player buffers 2-3 segments before playback starts</span>
</div>
</div>
</div>
<div>
<strong>Total:</strong>
<span> Normal mode: 10-15 seconds | Low-latency mode: 3-5 seconds (smaller segments, fewer buffered)</span>
</div>
</div>
<h3 id="live-streaming-interview-questions-3-levels-deep">Live Streaming Interview Questions (3 Levels Deep)</h3>
<div>
<h4>Level 1: Why does Twitch use HLS instead of WebRTC for live streaming?</h4>
<p><strong>Answer:</strong> HLS (HTTP Live Streaming) uses HTTP-based segment delivery, which integrates seamlessly with existing <span>**CDN infrastructure**</span>. A popular streamer with 100K viewers needs each video segment cached and served from edge locations globally. WebRTC is designed for peer-to-peer, low-latency scenarios (video calls) but doesn't have the CDN caching layer needed for broadcast. The economics are clear: serving 100K viewers at 5Mbps via WebRTC from origin = 500 Gbps from your servers. With CDN + HLS, that traffic is distributed across hundreds of edge PoPs with 95%+ cache hit rates.</p>
<div>
<h5>Level 2: How do you achieve sub-5-second latency with HLS when segments are 4 seconds long?</h5>
<p><strong>Answer:</strong> Standard HLS has 10-30 second latency because you wait for: segment encoding + upload + CDN propagation + player buffer (2-3 segments). <span>**Low-Latency HLS (LL-HLS)**</span> attacks each component: (1) <strong>Partial Segments:</strong> Instead of waiting for full 4s segment, emit 200ms "parts" using <span>**Chunked Transfer Encoding**</span>. (2) <strong>Playlist Delta Updates:</strong> Instead of re-fetching entire playlist, server pushes just the delta via HTTP/2 push. (3) <strong>Reduced Buffering:</strong> Player buffers 2-3 parts instead of 2-3 full segments. (4) <strong>Preload Hints:</strong> Playlist includes hints about next parts so player can request them early. Trade-off: Lower latency = more origin requests, less cache efficiency, more rebuffering on poor connections. See [[latency-throughput]](/topic/system-design/latency-throughput) for optimization strategies.</p>
<div>
<h6>Level 3: A streamer in Australia is broadcasting to viewers in Europe. How do you minimize the latency disadvantage compared to local streamers?</h6>
<p><strong>Answer:</strong> Cross-continental streaming faces ~300ms network RTT just for physics. Minimize additional latency through: (1) <strong>Regional Ingest:</strong> Streamer connects to closest ingest server (Sydney). RTMP upload latency minimized. (2) <strong>Internal Backbone:</strong> Transcoded segments travel over optimized internal network (not public internet) from Sydney transcoding cluster to EU origin. Use dedicated fiber/peering arrangements. (3) <strong>Origin Replication:</strong> Push segments to EU origin servers proactively rather than waiting for CDN pull. EU origin serves EU edge PoPs. (4) <strong>Edge Pre-positioning:</strong> For popular streamers, pre-warm major edge PoPs by predicting where viewers will be based on historical data. (5) <strong>Anycast DNS:</strong> Viewers automatically routed to nearest edge PoP regardless of origin location. (6) <strong>Accept the floor:</strong> Physics sets a ~150ms minimum for AU-to-EU. Focus on not adding more. (7) <strong>Measure end-to-end:</strong> Track glass-to-glass latency per region. Alert on regression. Real implementation: Twitch maintains dedicated inter-region backbones and replicates segments to regional origin clusters.</p>
</div>
</div>
</div>
<hr />
<h2 id="section-3-real-time-chat-system">Section 3: Real-Time Chat System</h2>
<h3 id="deep-mechanics-1">Deep Mechanics</h3>
<p>Chat at Twitch scale is a <span><strong>massive fan-out problem</strong></span>: one message from a viewer must reach 100K+ connected clients in near real-time. The architecture uses hierarchical distribution with edge WebSocket servers consuming from a central message bus.</p>
<div>
<h4>CHAT ARCHITECTURE FOR 100K+ VIEWERS</h4>
<div>
  <!-- Chat Ingest -->
<div>
<h5>CHAT INGEST SERVERS</h5>
<div>
<div>
<span>Validate</span><br>
<span>Auth, permissions</span>
</div>
<div>
<span>Rate Limit</span><br>
<span>1 msg/sec/user</span>
</div>
<div>
<span>Moderate</span><br>
<span>AutoMod, filters</span>
</div>
<div>
<span>Enrich</span><br>
<span>Badges, emotes</span>
</div>
</div>
</div>
<div>↓</div>
  <!-- Message Queue -->
<div>
<h5>MESSAGE QUEUE (Kafka)</h5>
<p><strong>Topic:</strong> chat.{channel_id} | <strong>Partitions:</strong> By user_id % N | <strong>Retention:</strong> 2 hours</p>
<p>Provides ordered delivery, durability, and decouples ingest from distribution</p>
</div>
<div>↓</div>
  <!-- Edge WebSocket Servers -->
<div>
<h5>EDGE WEBSOCKET SERVERS</h5>
<div>
<div>
<span>WS Server 1</span><br>
<span>10K connections</span>
</div>
<div>
<span>WS Server 2</span><br>
<span>10K connections</span>
</div>
<div>
<span>WS Server 3</span><br>
<span>10K connections</span>
</div>
<div>
<span>...</span>
</div>
<div>
<span>WS Server N</span><br>
<span>10K connections</span>
</div>
</div>
<p>For 100K viewer stream → ~10 edge servers needed</p>
</div>
<div>↓</div>
  <!-- Viewers -->
<div>
<div>
<strong>VIEWERS (100K+)</strong>
<p>WebSocket connection to nearest edge server</p>
</div>
</div>
</div>
</div>
<h3 id="chat-implementation">Chat Implementation</h3>
<pre><code>                                                      ```python
                                                      class ChatService:
                                                      &quot;&quot;&quot;
                                                      Production chat service with rate limiting and moderation.

                                                      Key Design Decisions:
                                                      - Messages are ephemeral (not persisted long-term)
                                                      - Eventually consistent delivery is acceptable
                                                      - Fan-out happens at edge, not origin
                                                      &quot;&quot;&quot;

                                                      def __init__(self, kafka_producer, redis_client):
                                                      self.kafka = kafka_producer
                                                      self.redis = redis_client

                                                      async def send_message(
                                                      self,
                                                      channel_id: str,
                                                      user_id: str,
                                                      message: str,
                                                      context: ChatContext
                                                      ) -&gt; ChatMessage:
                                                      # Step 1: Rate limit check (1 message/second/user)
                                                      rate_key = f&quot;chat:rate:{channel_id}:{user_id}&quot;
                                                      if not await self.check_rate_limit(rate_key, limit=1, window=1):
                                                      raise RateLimitExceeded(&quot;Slow down! 1 message per second.&quot;)

                                                      # Step 2: Check restrictions (banned, timed out)
                                                      restrictions = await self.get_restrictions(channel_id, user_id)
                                                      if restrictions.is_banned:
                                                      raise UserBanned(restrictions.ban_reason)
                                                      if restrictions.is_timed_out:
                                                      raise UserTimedOut(restrictions.timeout_remaining)

                                                      # Step 3: Moderation pipeline
                                                      moderation_result = await self.moderate_message(
                                                      channel_id, message, context
                                                      )
                                                      if moderation_result.blocked:
                                                      raise MessageBlocked(moderation_result.reason)

                                                      # Step 4: Enrich with badges, emotes
                                                      enriched = ChatMessage(
                                                      id=str(uuid4()),
                                                      channel_id=channel_id,
                                                      user_id=user_id,
                                                      username=context.username,
                                                      message=moderation_result.filtered_message,
                                                      badges=await self.get_user_badges(user_id, channel_id),
                                                      emotes=self.parse_emotes(message),
                                                      timestamp=time.time(),
                                                      color=context.chat_color,
                                                      )

                                                      # Step 5: Publish to Kafka for distribution
                                                      await self.kafka.send(
                                                      topic=f&quot;chat.{channel_id}&quot;,
                                                      key=user_id.encode(),  # Partition by user for ordering
                                                      value=enriched.to_json().encode(),
                                                      )

                                                      return enriched

                                                      async def moderate_message(
                                                      self,
                                                      channel_id: str,
                                                      message: str,
                                                      context: ChatContext
                                                      ) -&gt; ModerationResult:
                                                      &quot;&quot;&quot;
                                                      Multi-layer moderation pipeline:
                                                      1. Channel-specific blocked terms
                                                      2. Global blocked terms
                                                      3. AutoMod ML model
                                                      4. Spam detection
                                                      &quot;&quot;&quot;
                                                      # Check channel-specific filters
                                                      channel_filters = await self.redis.smembers(f&quot;filters:{channel_id}&quot;)
                                                      for term in channel_filters:
                                                      if term.lower() in message.lower():
                                                      return ModerationResult(blocked=True, reason=&quot;blocked_term&quot;)

                                                      # AutoMod check (ML-based content moderation)
                                                      automod_result = await self.automod.check(message, context)
                                                      if automod_result.score &gt; context.automod_threshold:
                                                      # Hold for mod review instead of blocking
                                                      await self.queue_for_review(channel_id, message, automod_result)
                                                      return ModerationResult(blocked=True, reason=&quot;automod_held&quot;)

                                                      # R9K duplicate check (prevents spam)
                                                      if context.r9k_enabled:
                                                      message_hash = hashlib.md5(message.lower().encode()).hexdigest()
                                                      if await self.redis.sismember(f&quot;r9k:{channel_id}&quot;, message_hash):
                                                      return ModerationResult(blocked=True, reason=&quot;duplicate_message&quot;)
                                                      await self.redis.sadd(f&quot;r9k:{channel_id}&quot;, message_hash)
                                                      await self.redis.expire(f&quot;r9k:{channel_id}&quot;, 3600)

                                                      return ModerationResult(blocked=False, filtered_message=message)
                                                      ```
</code></pre>
<h3 id="edge-websocket-server">Edge WebSocket Server</h3>
<pre><code>                                                      ```python
                                                      class EdgeChatServer:
                                                      &quot;&quot;&quot;
                                                      Edge server holding 10K+ WebSocket connections.
                                                      Consumes from Kafka and broadcasts to connected clients.
                                                      &quot;&quot;&quot;

                                                      def __init__(self, kafka_consumer, max_connections: int = 10000):
                                                      self.kafka = kafka_consumer
                                                      self.max_connections = max_connections

                                                      # channel_id -&gt; set of WebSocket connections
                                                      self.subscriptions: Dict[str, Set[WebSocket]] = defaultdict(set)

                                                      # Connection tracking
                                                      self.connections: Dict[str, WebSocket] = {}

                                                      async def handle_connection(self, websocket: WebSocket, user_id: str):
                                                      &quot;&quot;&quot;Handle new WebSocket connection.&quot;&quot;&quot;
                                                      if len(self.connections) &gt;= self.max_connections:
                                                      await websocket.close(code=1013, reason=&quot;Server at capacity&quot;)
                                                      return

                                                      self.connections[user_id] = websocket

                                                      try:
                                                      async for message in websocket:
                                                      await self.handle_client_message(websocket, user_id, message)
                                                      finally:
                                                      self.cleanup_connection(user_id)

                                                      async def handle_client_message(
                                                      self,
                                                      websocket: WebSocket,
                                                      user_id: str,
                                                      message: str
                                                      ):
                                                      &quot;&quot;&quot;Handle JOIN/PART commands from client.&quot;&quot;&quot;
                                                      data = json.loads(message)

                                                      if data[&quot;type&quot;] == &quot;JOIN&quot;:
                                                      channel_id = data[&quot;channel_id&quot;]
                                                      self.subscriptions[channel_id].add(websocket)

                                                      # Send recent messages (last 50)
                                                      recent = await self.get_recent_messages(channel_id, limit=50)
                                                      await websocket.send(json.dumps({
                                                      &quot;type&quot;: &quot;RECENT_MESSAGES&quot;,
                                                      &quot;messages&quot;: recent
                                                      }))

                                                      elif data[&quot;type&quot;] == &quot;PART&quot;:
                                                      channel_id = data[&quot;channel_id&quot;]
                                                      self.subscriptions[channel_id].discard(websocket)

                                                      async def consume_messages(self):
                                                      &quot;&quot;&quot;
                                                      Consume from Kafka and broadcast to subscribed connections.
                                                      This is the core fan-out loop.
                                                      &quot;&quot;&quot;
                                                      async for record in self.kafka:
                                                      channel_id = record.topic.split(&quot;.&quot;)[-1]  # chat.{channel_id}
                                                      message = json.loads(record.value)

                                                      # Get all WebSocket connections subscribed to this channel
                                                      subscribers = self.subscriptions.get(channel_id, set())

                                                      if not subscribers:
                                                      continue

                                                      # Broadcast to all subscribers concurrently
                                                      payload = json.dumps({
                                                      &quot;type&quot;: &quot;MESSAGE&quot;,
                                                      &quot;data&quot;: message
                                                      })

                                                      # Use gather for concurrent sends, handle failures gracefully
                                                      await asyncio.gather(
                                                      *[self.safe_send(ws, payload) for ws in subscribers],
                                                      return_exceptions=True
                                                      )

                                                      async def safe_send(self, websocket: WebSocket, payload: str):
                                                      &quot;&quot;&quot;Send with error handling - remove dead connections.&quot;&quot;&quot;
                                                      try:
                                                      await asyncio.wait_for(websocket.send(payload), timeout=5.0)
                                                      except (ConnectionClosed, asyncio.TimeoutError):
                                                      # Connection dead - will be cleaned up
                                                      pass
                                                      ```
</code></pre>
<h3 id="chat-interview-questions-3-levels-deep">Chat Interview Questions (3 Levels Deep)</h3>
<div>
<h4>Level 1: How do you scale chat for 100K+ concurrent viewers in a single channel?</h4>
<p><strong>Answer:</strong> The core challenge is <span>**fan-out**</span>: one message needs to reach 100K connected clients. You can't have one server holding 100K WebSocket connections. The architecture is hierarchical: (1) <strong>Chat Ingest Layer</strong> receives messages, validates, rate-limits, applies moderation. (2) <strong>Message Queue (Kafka)</strong> provides topic per channel, ordered delivery, handles burst writes. (3) <strong>Edge WebSocket Servers</strong> each hold ~10K connections, consume from Kafka, broadcast to connected users. For 100K viewers, you need ~10 edge servers. Key insight: chat is <span>**eventually consistent**</span> - if message delivery is delayed 100ms between edge servers, nobody notices.</p>
<div>
<h5>Level 2: How do you handle a chat message storm during a viral moment (streamer wins tournament)?</h5>
<p><strong>Answer:</strong> Viral moments cause chat to spike 10-100x normal volume. Multiple mitigation layers: (1) <strong>Slow Mode:</strong> Force N seconds between messages per user - reduces eligible senders. (2) <strong>Sub-only Mode:</strong> Only subscribers can chat - dramatic reduction in volume. (3) <strong>Client-side Throttling:</strong> Player shows subset of messages to prevent UI lag. Client randomly samples messages above threshold. (4) <strong>R9K Mode:</strong> Block duplicate/similar messages - prevents copy-paste spam. (5) <strong>Graceful Degradation:</strong> Under extreme load, delay messages rather than drop them. Queue depth alerts trigger auto-slow-mode. (6) <strong>Edge Server Scaling:</strong> Auto-scale WebSocket servers based on connection count and CPU. Use [[rate-limiting]](/topic/system-design/rate-limiting) patterns at multiple layers.</p>
<div>
<h6>Level 3: A user is connected to Edge Server A but sends a message. How does that message reach users connected to Edge Server B, and how do you ensure ordering?</h6>
<p><strong>Answer:</strong> This requires understanding the full message flow and consistency model. (1) <strong>Message Path:</strong> User on Edge A sends message -> Chat Ingest Server (not Edge A!) validates and writes to Kafka -> All Edge Servers (A, B, C...) consume from Kafka and broadcast. Edge servers are <em>read-only distributors</em>, not message originators. (2) <strong>Ordering Guarantee:</strong> Kafka provides <span>**partition-level ordering**</span>. We partition by user_id, so messages from same user are ordered. Messages from different users may be reordered between edges - this is acceptable for chat. (3) <strong>Cross-Edge Consistency:</strong> Edge A and Edge B consume from the same Kafka topic but may be at different offsets (one is behind). This means User X on Edge A might see a message before User Y on Edge B. Acceptable delay is ~100ms. (4) <strong>Total Ordering Alternative:</strong> If strict ordering required (e.g., auction), use single partition or sequence numbers. But this limits throughput. (5) <strong>Edge Case - Message Dedup:</strong> If Kafka consumer restarts, it may re-deliver messages. Edge servers must deduplicate by message ID to prevent duplicates. Use bloom filter or small LRU cache of recent message IDs.</p>
</div>
</div>
</div>
<hr />
<h2 id="section-4-vod-storage-and-clips">Section 4: VOD Storage and Clips</h2>
<h3 id="deep-mechanics-2">Deep Mechanics</h3>
<p><span><strong>VOD (Video on Demand)</strong></span> stores past broadcasts for later viewing. Unlike live streaming where segments are ephemeral, VOD requires long-term storage with efficient retrieval. <span><strong>Clips</strong></span> are short highlights extracted from live or VOD content.</p>
<div>
<h4>VOD STORAGE ARCHITECTURE</h4>
<div>
  <!-- Live Pipeline -->
<div>
<div>
<h5>LIVE PIPELINE</h5>
<p>During stream, segments are cached in memory/SSD for low-latency delivery</p>
</div>
<div>→</div>
<div>
<h5>VOD ARCHIVE</h5>
<p>After stream ends, segments are archived to object storage (S3)</p>
</div>
</div>
  <!-- Storage Tiers -->
<div>
<h5>TIERED STORAGE</h5>
<div>
<div>
<span>HOT (0-7 days)</span><br>
<span>S3 Standard</span><br>
<span>$0.023/GB/month</span>
<span>Most replays happen in first week</span>
</div>
<div>
<span>WARM (7-60 days)</span><br>
<span>S3 Standard-IA</span><br>
<span>$0.0125/GB/month</span>
<span>Infrequent access, fast retrieval</span>
</div>
<div>
<span>COLD (60+ days)</span><br>
<span>S3 Glacier</span><br>
<span>$0.004/GB/month</span>
<span>Archive, retrieval takes minutes</span>
</div>
</div>
</div>
  <!-- VOD Structure -->
<div>
<h5>VOD DATA STRUCTURE</h5>
<div>
<div>
<span>s3://twitch-vods/</span><br>
<span>  ├── {channel_id}/</span><br>
<span>  │   ├── {vod_id}/</span><br>
<span>  │   │   ├── manifest.m3u8</span><br>
<span>  │   │   ├── 1080p60/</span><br>
<span>  │   │   │   ├── 00001.ts</span><br>
<span>  │   │   │   ├── 00002.ts</span><br>
<span>  │   │   │   └── ...</span><br>
<span>  │   │   ├── 720p60/</span><br>
<span>  │   │   ├── thumbnails/</span><br>
<span>  │   │   └── metadata.json</span>
</div>
<div>
<p><strong>Typical VOD Size:</strong></p>
<p>4-hour stream @ 1080p60 = ~50GB</p>
<p>With all quality levels = ~100GB</p>
<p><strong>Retention:</strong></p>
<p>Partners: 60 days</p>
<p>Turbo/Prime: 60 days</p>
<p>Regular: 14 days</p>
</div>
</div>
</div>
</div>
</div>
<h3 id="clip-creation-system">Clip Creation System</h3>
<pre><code>                                                                                      ```go
                                                                                      // Real-time clip extraction from live stream or VOD
                                                                                      type ClipService struct {
                                                                                      segmentStore  SegmentStore     // In-memory/SSD cache of recent segments
                                                                                      vodStore      VODStore         // S3 for archived content
                                                                                      processQueue  *WorkQueue       // Async clip processing
                                                                                      metadata      MetadataStore    // PostgreSQL for clip metadata
                                                                                      }

                                                                                      func (c *ClipService) CreateClip(req ClipRequest) (*Clip, error) {
                                                                                      // Validate: clips are 5-60 seconds
                                                                                      if req.Duration &lt; 5 || req.Duration &gt; 60 {
                                                                                      return nil, ErrInvalidDuration
                                                                                      }

                                                                                      // Calculate time range
                                                                                      // If from live stream, use current time - offset
                                                                                      // If from VOD, use VOD timestamp
                                                                                      var startTime, endTime time.Time
                                                                                      if req.Source == SourceLive {
                                                                                      endTime = time.Now().Add(-time.Duration(req.Delay) * time.Second)
                                                                                      startTime = endTime.Add(-time.Duration(req.Duration) * time.Second)
                                                                                      } else {
                                                                                      startTime = req.VODTimestamp
                                                                                      endTime = startTime.Add(time.Duration(req.Duration) * time.Second)
                                                                                      }

                                                                                      // Get segments from appropriate store
                                                                                      var segments []*Segment
                                                                                      var err error

                                                                                      if req.Source == SourceLive {
                                                                                      // Recent segments in memory/SSD (kept for ~2 hours)
                                                                                      segments, err = c.segmentStore.GetRange(req.ChannelID, startTime, endTime)
                                                                                      } else {
                                                                                      // Fetch from S3
                                                                                      segments, err = c.vodStore.GetSegments(req.VODID, startTime, endTime)
                                                                                      }

                                                                                      if err != nil {
                                                                                      return nil, fmt.Errorf(&quot;failed to get segments: %w&quot;, err)
                                                                                      }

                                                                                      // Create clip record
                                                                                      clip := &amp;Clip{
                                                                                      ID:           uuid.New().String(),
                                                                                      ChannelID:    req.ChannelID,
                                                                                      CreatorID:    req.UserID,
                                                                                      Title:        req.Title,
                                                                                      Duration:     req.Duration,
                                                                                      SourceType:   req.Source,
                                                                                      Status:       ClipStatusProcessing,
                                                                                      CreatedAt:    time.Now(),
                                                                                      }

                                                                                      // Save metadata immediately (clip is &quot;created&quot; but processing)
                                                                                      if err := c.metadata.SaveClip(clip); err != nil {
                                                                                      return nil, err
                                                                                      }

                                                                                      // Queue async processing (combine segments, generate thumbnail)
                                                                                      c.processQueue.Push(&amp;ClipJob{
                                                                                      Clip:     clip,
                                                                                      Segments: segments,
                                                                                      })

                                                                                      // Return immediately - client polls for completion
                                                                                      return clip, nil
                                                                                      }

                                                                                      func (c *ClipService) ProcessClip(job *ClipJob) error {
                                                                                      // Step 1: Combine segments into single video file
                                                                                      // Uses FFmpeg to concatenate and trim to exact timestamps
                                                                                      combinedPath, err := c.combineSegments(job.Segments, job.Clip.Duration)
                                                                                      if err != nil {
                                                                                      return c.markFailed(job.Clip.ID, err)
                                                                                      }

                                                                                      // Step 2: Generate thumbnail at middle of clip
                                                                                      thumbnailPath, err := c.generateThumbnail(combinedPath)
                                                                                      if err != nil {
                                                                                      return c.markFailed(job.Clip.ID, err)
                                                                                      }

                                                                                      // Step 3: Upload to permanent storage
                                                                                      videoURL, err := c.vodStore.Upload(combinedPath, fmt.Sprintf(&quot;clips/%s/video.mp4&quot;, job.Clip.ID))
                                                                                      if err != nil {
                                                                                      return c.markFailed(job.Clip.ID, err)
                                                                                      }

                                                                                      thumbURL, err := c.vodStore.Upload(thumbnailPath, fmt.Sprintf(&quot;clips/%s/thumb.jpg&quot;, job.Clip.ID))
                                                                                      if err != nil {
                                                                                      return c.markFailed(job.Clip.ID, err)
                                                                                      }

                                                                                      // Step 4: Update metadata
                                                                                      return c.metadata.UpdateClip(job.Clip.ID, map[string]interface{}{
                                                                                      &quot;status&quot;:        ClipStatusReady,
                                                                                      &quot;video_url&quot;:     videoURL,
                                                                                      &quot;thumbnail_url&quot;: thumbURL,
                                                                                      &quot;processed_at&quot;:  time.Now(),
                                                                                      })
                                                                                      }
                                                                                      ```
</code></pre>
<h3 id="vod-interview-questions-3-levels-deep">VOD Interview Questions (3 Levels Deep)</h3>
<div>
<h4>Level 1: How do you efficiently store VODs when a popular streamer broadcasts 8 hours daily?</h4>
<p><strong>Answer:</strong> An 8-hour stream at 1080p60 with all quality levels is ~200GB. For a platform with 100K daily streams, that's petabytes per day. The solution is <span>**tiered storage**</span>: (1) <strong>Hot tier (S3 Standard)</strong> for first 7 days - most VOD views happen in this window. (2) <strong>Warm tier (S3 Standard-IA)</strong> for days 7-60 - cheaper, retrieval still fast. (3) <strong>Cold tier (S3 Glacier)</strong> for 60+ days - very cheap, minutes to retrieve. (4) <strong>Retention limits:</strong> Regular users get 14 days, Partners get 60 days. Automatic deletion prevents unbounded growth. (5) <strong>Transcoding on demand:</strong> Only store source quality long-term; regenerate lower qualities on-demand for cold VODs.</p>
<div>
<h5>Level 2: A user wants to clip a moment from 30 seconds ago in a live stream. How do you serve this instantly?</h5>
<p><strong>Answer:</strong> Clips require access to recent stream segments. Architecture: (1) <strong>Segment Buffer:</strong> During live stream, keep last 2 hours of segments in memory/SSD cache on origin servers. This is the "clip window." (2) <strong>Instant Clip Creation:</strong> When user clips, we already have segments cached - no need to fetch from cold storage. (3) <strong>Async Processing:</strong> Return clip ID immediately. Background worker combines segments, generates thumbnail, uploads to S3. Client polls for completion (typically 5-10 seconds). (4) <strong>Pre-generated Thumbnails:</strong> Generate thumbnails every 10 seconds during live stream. Clip can use nearest pre-generated thumbnail while final one is processing. See [[storage]](/topic/system-design/storage) for tiered caching strategies.</p>
<div>
<h6>Level 3: How do you handle seeking to any timestamp in a 10-hour VOD without downloading the entire file?</h6>
<p><strong>Answer:</strong> This is where <span>**HLS segment indexing**</span> shines. (1) <strong>Manifest Structure:</strong> The .m3u8 playlist contains entries for every segment with duration. Player can calculate which segment contains target timestamp. (2) <strong>Byte-Range Requests:</strong> Segments are stored as discrete files. Player requests only the needed segment(s) via HTTP Range requests. (3) <strong>Keyframe Index:</strong> For frame-accurate seeking, we need an index of <span>**I-frames (keyframes)**</span>. HLS #EXT-X-I-FRAME-STREAM-INF provides this. Without keyframe, must decode from previous keyframe. (4) <strong>Segment Granularity:</strong> 4-second segments mean worst-case 4 seconds of "overshoot" when seeking. For precise seeking, use smaller segments or byte-range within segments. (5) <strong>Thumbnail Scrubbing:</strong> Generate sprite sheets (grid of thumbnails) at 10-second intervals. As user scrubs, show thumbnail instantly while actual video loads. (6) <strong>Prefetching:</strong> Once user starts watching, prefetch next N segments. If they seek forward, likely segments already cached. (7) <strong>CDN Caching:</strong> Popular VOD segments are cached at edge. Seeking within popular VODs is fast because segments are edge-cached.</p>
</div>
</div>
</div>
<hr />
<h2 id="section-5-transcoding-pipeline">Section 5: Transcoding Pipeline</h2>
<h3 id="deep-mechanics-3">Deep Mechanics</h3>
<p><span><strong>Real-time transcoding</strong></span> is the most computationally intensive component. It converts the broadcaster's single input stream into multiple quality levels (ABR ladder) with strict latency requirements - processing must be faster than real-time.</p>
<div>
<h4>TRANSCODING INFRASTRUCTURE</h4>
<div>
  <!-- GPU Cluster -->
<div>
<h5>GPU TRANSCODING CLUSTER</h5>
<div>
<div>
<span>NVIDIA T4</span><br>
<span>8-12 streams/GPU</span><br>
<span>NVENC encoder</span>
</div>
<div>
<span>NVIDIA T4</span><br>
<span>8-12 streams/GPU</span><br>
<span>NVENC encoder</span>
</div>
<div>
<span>NVIDIA T4</span><br>
<span>8-12 streams/GPU</span><br>
<span>NVENC encoder</span>
</div>
<div>
<span>...</span>
</div>
</div>
<p><strong>Scale:</strong> 100K concurrent streams × 5 outputs = 500K parallel encoding jobs</p>
</div>
  <!-- Transcoding Tiers -->
<div>
<h5>TRANSCODING TIERS (Business Logic)</h5>
<div>
<div>
<span>PARTNERS</span><br>
<span>Full ABR ladder</span>
<span>1080p, 720p, 480p, 360p, 160p</span><br>
<span>Always guaranteed</span>
</div>
<div>
<span>AFFILIATES</span><br>
<span>Limited ladder</span>
<span>Source + 720p + 480p</span><br>
<span>When capacity available</span>
</div>
<div>
<span>REGULAR</span><br>
<span>Source only</span>
<span>No transcoding</span><br>
<span>Viewers need bandwidth</span>
</div>
</div>
</div>
  <!-- Quality Ladder -->
<div>
<h5>ABR LADDER ENCODING SETTINGS</h5>
<div>
<table>
<tr>
<th>Quality</th>
<th>Resolution</th>
<th>Bitrate</th>
<th>Frame Rate</th>
<th>Keyframe</th>
</tr>
<tr>
<td>Source</td>
<td>1920×1080</td>
<td>6000 kbps</td>
<td>60 fps</td>
<td>2 sec</td>
</tr>
<tr>
<td>720p60</td>
<td>1280×720</td>
<td>3000 kbps</td>
<td>60 fps</td>
<td>2 sec</td>
</tr>
<tr>
<td>480p30</td>
<td>852×480</td>
<td>1500 kbps</td>
<td>30 fps</td>
<td>2 sec</td>
</tr>
<tr>
<td>360p30</td>
<td>640×360</td>
<td>800 kbps</td>
<td>30 fps</td>
<td>2 sec</td>
</tr>
<tr>
<td>160p (audio)</td>
<td>284×160</td>
<td>400 kbps</td>
<td>30 fps</td>
<td>2 sec</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<h3 id="transcoding-implementation">Transcoding Implementation</h3>
<pre><code>                                                                                                              ```python
                                                                                                              class TranscodingPipeline:
                                                                                                              &quot;&quot;&quot;
                                                                                                              Real-time transcoding pipeline using GPU acceleration.

                                                                                                              Performance Requirements:
                                                                                                              - Must encode faster than real-time (1 second of video in &lt; 1 second)
                                                                                                              - Target: 0.3-0.5 seconds processing per second of content
                                                                                                              - GPU memory: ~500MB per stream per output quality
                                                                                                              &quot;&quot;&quot;

                                                                                                              def __init__(self, gpu_cluster: GPUCluster, config: TranscodeConfig):
                                                                                                              self.gpu_cluster = gpu_cluster
                                                                                                              self.config = config
                                                                                                              self.active_streams: Dict[str, TranscodeSession] = {}

                                                                                                              async def start_stream(
                                                                                                              self,
                                                                                                              channel_id: str,
                                                                                                              input_stream: RTMPStream,
                                                                                                              tier: StreamerTier
                                                                                                              ) -&gt; TranscodeSession:
                                                                                                              # Determine quality ladder based on tier
                                                                                                              qualities = self.get_quality_ladder(tier)

                                                                                                              # Allocate GPU resources
                                                                                                              gpu_allocation = await self.gpu_cluster.allocate(
                                                                                                              stream_id=channel_id,
                                                                                                              outputs=len(qualities),
                                                                                                              priority=tier.priority,
                                                                                                              )

                                                                                                              if not gpu_allocation:
                                                                                                              if tier == StreamerTier.PARTNER:
                                                                                                              # Partners are guaranteed - wait for resources
                                                                                                              gpu_allocation = await self.gpu_cluster.allocate_with_preemption(
                                                                                                              stream_id=channel_id,
                                                                                                              outputs=len(qualities),
                                                                                                              )
                                                                                                              else:
                                                                                                              # Non-partners get source-only when capacity constrained
                                                                                                              return self.create_passthrough_session(channel_id, input_stream)

                                                                                                              # Create FFmpeg pipeline
                                                                                                              session = TranscodeSession(
                                                                                                              channel_id=channel_id,
                                                                                                              input_stream=input_stream,
                                                                                                              outputs=qualities,
                                                                                                              gpu_id=gpu_allocation.gpu_id,
                                                                                                              )

                                                                                                              # Start encoding pipeline
                                                                                                              await session.start()
                                                                                                              self.active_streams[channel_id] = session

                                                                                                              return session

                                                                                                              def get_quality_ladder(self, tier: StreamerTier) -&gt; List[QualityProfile]:
                                                                                                              &quot;&quot;&quot;Get ABR ladder based on streamer tier.&quot;&quot;&quot;
                                                                                                              if tier == StreamerTier.PARTNER:
                                                                                                              return [
                                                                                                              QualityProfile(&quot;1080p60&quot;, 1920, 1080, 60, 6000),
                                                                                                              QualityProfile(&quot;720p60&quot;, 1280, 720, 60, 3000),
                                                                                                              QualityProfile(&quot;480p30&quot;, 852, 480, 30, 1500),
                                                                                                              QualityProfile(&quot;360p30&quot;, 640, 360, 30, 800),
                                                                                                              QualityProfile(&quot;160p30&quot;, 284, 160, 30, 400),
                                                                                                              ]
                                                                                                              elif tier == StreamerTier.AFFILIATE:
                                                                                                              return [
                                                                                                              QualityProfile(&quot;source&quot;, 1920, 1080, 60, 6000),
                                                                                                              QualityProfile(&quot;720p60&quot;, 1280, 720, 60, 3000),
                                                                                                              QualityProfile(&quot;480p30&quot;, 852, 480, 30, 1500),
                                                                                                              ]
                                                                                                              else:
                                                                                                              # Regular streamers - source only (no transcoding)
                                                                                                              return [QualityProfile(&quot;source&quot;, 1920, 1080, 60, 6000)]


                                                                                                              class TranscodeSession:
                                                                                                              &quot;&quot;&quot;
                                                                                                              Active transcoding session using FFmpeg with NVENC.
                                                                                                              &quot;&quot;&quot;

                                                                                                              async def start(self):
                                                                                                              &quot;&quot;&quot;Start FFmpeg transcoding pipeline.&quot;&quot;&quot;
                                                                                                              # Build FFmpeg command for multi-output encoding
                                                                                                              cmd = self.build_ffmpeg_command()

                                                                                                              # Start process
                                                                                                              self.process = await asyncio.create_subprocess_exec(
                                                                                                              *cmd,
                                                                                                              stdin=asyncio.subprocess.PIPE,
                                                                                                              stdout=asyncio.subprocess.PIPE,
                                                                                                              stderr=asyncio.subprocess.PIPE,
                                                                                                              )

                                                                                                              # Start monitoring task
                                                                                                              asyncio.create_task(self.monitor_health())

                                                                                                              def build_ffmpeg_command(self) -&gt; List[str]:
                                                                                                              &quot;&quot;&quot;
                                                                                                              Build FFmpeg command for GPU-accelerated multi-output encoding.

                                                                                                              Key flags:
                                                                                                              - -hwaccel cuda: Use GPU for decoding
                                                                                                              - -c:v h264_nvenc: Use NVIDIA hardware encoder
                                                                                                              - -preset p4: Balance quality/speed (p1=fastest, p7=slowest)
                                                                                                              - -g 120: Keyframe every 2 seconds at 60fps
                                                                                                              - -sc_threshold 0: Disable scene change detection for consistent segments
                                                                                                              &quot;&quot;&quot;
                                                                                                              cmd = [
                                                                                                              &quot;ffmpeg&quot;,
                                                                                                              &quot;-hwaccel&quot;, &quot;cuda&quot;,
                                                                                                              &quot;-hwaccel_device&quot;, str(self.gpu_id),
                                                                                                              &quot;-i&quot;, self.input_url,
                                                                                                              ]

                                                                                                              for output in self.outputs:
                                                                                                              cmd.extend([
                                                                                                              &quot;-map&quot;, &quot;0:v&quot;, &quot;-map&quot;, &quot;0:a&quot;,
                                                                                                              &quot;-c:v&quot;, &quot;h264_nvenc&quot;,
                                                                                                              &quot;-preset&quot;, &quot;p4&quot;,
                                                                                                              &quot;-b:v&quot;, f&quot;{output.bitrate}k&quot;,
                                                                                                              &quot;-maxrate&quot;, f&quot;{int(output.bitrate * 1.5)}k&quot;,
                                                                                                              &quot;-bufsize&quot;, f&quot;{output.bitrate * 2}k&quot;,
                                                                                                              &quot;-vf&quot;, f&quot;scale={output.width}:{output.height}&quot;,
                                                                                                              &quot;-r&quot;, str(output.fps),
                                                                                                              &quot;-g&quot;, str(output.fps * 2),  # Keyframe interval
                                                                                                              &quot;-sc_threshold&quot;, &quot;0&quot;,
                                                                                                              &quot;-c:a&quot;, &quot;aac&quot;,
                                                                                                              &quot;-b:a&quot;, &quot;128k&quot;,
                                                                                                              &quot;-f&quot;, &quot;hls&quot;,
                                                                                                              &quot;-hls_time&quot;, &quot;4&quot;,
                                                                                                              &quot;-hls_list_size&quot;, &quot;5&quot;,
                                                                                                              &quot;-hls_flags&quot;, &quot;delete_segments&quot;,
                                                                                                              f&quot;{self.output_path}/{output.name}/playlist.m3u8&quot;
                                                                                                              ])

                                                                                                              return cmd
                                                                                                              ```
</code></pre>
<h3 id="transcoding-interview-questions-3-levels-deep">Transcoding Interview Questions (3 Levels Deep)</h3>
<div>
<h4>Level 1: Why does Twitch need to transcode streams? Can't viewers just watch the source quality?</h4>
<p><strong>Answer:</strong> <span>**Adaptive Bitrate (ABR)**</span> streaming is essential because viewers have varying network conditions. A viewer on mobile data can't sustain 6 Mbps for 1080p60. Without transcoding, that viewer either rebuffers constantly or can't watch at all. Transcoding creates multiple quality options (720p, 480p, 360p) so the player can automatically switch based on available bandwidth. Additionally, transcoding normalizes the input - streamers use different encoders, bitrates, and settings. Transcoding ensures consistent output format for CDN delivery.</p>
<div>
<h5>Level 2: Transcoding is expensive. How does Twitch decide which streams get transcoding and which don't?</h5>
<p><strong>Answer:</strong> Transcoding economics drive a tiered approach: (1) <strong>Partners:</strong> Guaranteed full quality ladder (5 outputs) regardless of capacity. They generate revenue through ads and subs, justifying cost. (2) <strong>Affiliates:</strong> Get transcoding when capacity available. If GPU cluster is at 80% capacity, new affiliates may only get source + 1-2 qualities. (3) <strong>Regular streamers:</strong> Source-only (passthrough) - no transcoding. Most regular streams have <10 viewers who likely have good internet. (4) <strong>Dynamic allocation:</strong> System monitors GPU utilization. When capacity frees up (streams end), affiliates waiting for transcoding get upgraded. When a partner goes live, lower-priority streams may be downgraded. See [[resource-allocation]](/topic/system-design/distributed-systems) for scheduling patterns.</p>
<div>
<h6>Level 3: The transcoding cluster is at 95% capacity and a popular partner is about to go live. How do you ensure they get resources without degrading other partners?</h6>
<p><strong>Answer:</strong> This requires sophisticated <span>**resource management with preemption**</span>: (1) <strong>Capacity Reservation:</strong> Reserve 10-15% of cluster capacity for partner burst. This "reserved" capacity can be used by affiliates but is preemptible. (2) <strong>Preemption Priority:</strong> When partner needs resources: first preempt affiliate streams above their minimum tier (reduce 3 outputs to 2), then preempt regular passthrough streams if needed. Never preempt partners. (3) <strong>Graceful Degradation:</strong> Don't instantly drop quality - give stream 30 seconds to wind down, let viewers' players adapt. (4) <strong>Predictive Scaling:</strong> If scheduled events (esports, big streamers' schedules), pre-scale GPU capacity. Use historical data to predict peak demand. (5) <strong>Multi-Region:</strong> Spread transcoding across regions. If US-East is constrained, route some streams to US-West transcoding (acceptable latency for output to CDN). (6) <strong>Quality Reduction vs Stream Drop:</strong> Never drop a stream entirely - instead reduce quality ladder. A partner with 3 outputs is better than a dropped stream. (7) <strong>Cost-Based Spillover:</strong> In extreme cases, use cloud GPU instances (AWS g4dn) as overflow. More expensive but prevents degradation.</p>
</div>
</div>
</div>
<hr />
<h2 id="section-6-recommendation-engine">Section 6: Recommendation Engine</h2>
<h3 id="deep-mechanics-4">Deep Mechanics</h3>
<p>The <span><strong>recommendation engine</strong></span> personalizes content discovery, helping viewers find streams they'll enjoy. Unlike Netflix (catalog of fixed content), Twitch recommendations must account for real-time factors: who is currently live, viewer count trends, and stream recency.</p>
<div>
<h4>RECOMMENDATION SYSTEM ARCHITECTURE</h4>
<div>
  <!-- Data Sources -->
<div>
<h5>DATA SOURCES</h5>
<div>
<div>
<span>Watch History</span><br>
<span>Channels, duration, recency</span>
</div>
<div>
<span>Follows/Subs</span><br>
<span>Explicit preferences</span>
</div>
<div>
<span>Interactions</span><br>
<span>Chat, clips, raids</span>
</div>
<div>
<span>Real-time</span><br>
<span>Live status, viewers, trends</span>
</div>
</div>
</div>
  <!-- Model Pipeline -->
<div>
<div>
<h5>CANDIDATE GENERATION</h5>
<p>Retrieve 1000s of potentially relevant streams using lightweight models (embedding similarity, collaborative filtering)</p>
</div>
<div>
<h5>RANKING</h5>
<p>Deep neural network scores candidates on predicted engagement (watch time, follows, interactions)</p>
</div>
<div>
<h5>FILTERING & DIVERSITY</h5>
<p>Remove duplicates, apply business rules, ensure category diversity, inject exploration</p>
</div>
</div>
  <!-- Signals -->
<div>
<h5>RANKING SIGNALS (Feature Engineering)</h5>
<div>
<div>
<strong>User-Stream Affinity</strong>
<ul>
<li>Cosine similarity of user/streamer embeddings</li>
<li>Shared viewers (collaborative signal)</li>
<li>Category match to watch history</li>
<li>Language preference match</li>
</ul>
</div>
<div>
<strong>Stream Quality Signals</strong>
<ul>
<li>Current viewer count (social proof)</li>
<li>Viewer trend (growing vs declining)</li>
<li>Chat activity rate</li>
<li>Stream duration (just started vs 8 hours in)</li>
</ul>
</div>
<div>
<strong>Contextual Features</strong>
<ul>
<li>Time of day / day of week</li>
<li>User's current session length</li>
<li>Recently watched in session</li>
<li>Device type (mobile prefers shorter content)</li>
</ul>
</div>
<div>
<strong>Business Rules</strong>
<ul>
<li>Partner boost (contractual requirements)</li>
<li>New streamer boost (growth program)</li>
<li>Promoted content (sponsored)</li>
<li>Mature content filter</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<h3 id="recommendation-implementation">Recommendation Implementation</h3>
<pre><code>                                                                                                                      ```python
                                                                                                                      class RecommendationEngine:
                                                                                                                      &quot;&quot;&quot;
                                                                                                                      Two-stage recommendation pipeline: Candidate Generation + Ranking.

                                                                                                                      Architecture follows YouTube's deep learning recommendations paper
                                                                                                                      adapted for live streaming constraints.
                                                                                                                      &quot;&quot;&quot;

                                                                                                                      def __init__(
                                                                                                                      self,
                                                                                                                      embedding_store: EmbeddingStore,
                                                                                                                      ranking_model: RankingModel,
                                                                                                                      feature_store: FeatureStore,
                                                                                                                      ):
                                                                                                                      self.embeddings = embedding_store
                                                                                                                      self.ranker = ranking_model
                                                                                                                      self.features = feature_store

                                                                                                                      async def get_recommendations(
                                                                                                                      self,
                                                                                                                      user_id: str,
                                                                                                                      context: RecommendationContext,
                                                                                                                      limit: int = 20,
                                                                                                                      ) -&gt; List[RecommendedStream]:
                                                                                                                      # Stage 1: Candidate Generation (fast, retrieves 1000s)
                                                                                                                      candidates = await self.generate_candidates(user_id, context)

                                                                                                                      # Stage 2: Ranking (slower, scores 100s)
                                                                                                                      ranked = await self.rank_candidates(user_id, candidates, context)

                                                                                                                      # Stage 3: Post-processing
                                                                                                                      final = self.apply_business_rules(ranked, context)
                                                                                                                      final = self.ensure_diversity(final)

                                                                                                                      return final[:limit]

                                                                                                                      async def generate_candidates(
                                                                                                                      self,
                                                                                                                      user_id: str,
                                                                                                                      context: RecommendationContext,
                                                                                                                      ) -&gt; List[CandidateStream]:
                                                                                                                      &quot;&quot;&quot;
                                                                                                                      Retrieve candidate streams using multiple strategies.
                                                                                                                      Goal: High recall (don't miss good streams), speed (&lt; 50ms).
                                                                                                                      &quot;&quot;&quot;
                                                                                                                      candidates = set()

                                                                                                                      # Strategy 1: Followed channels that are live
                                                                                                                      followed_live = await self.get_followed_live(user_id)
                                                                                                                      candidates.update(followed_live)

                                                                                                                      # Strategy 2: Similar to recently watched (embedding similarity)
                                                                                                                      user_embedding = await self.embeddings.get_user(user_id)
                                                                                                                      similar_streams = await self.embeddings.nearest_neighbors(
                                                                                                                      user_embedding,
                                                                                                                      k=500,
                                                                                                                      filter=&quot;is_live=true&quot;
                                                                                                                      )
                                                                                                                      candidates.update(similar_streams)

                                                                                                                      # Strategy 3: Popular in user's preferred categories
                                                                                                                      preferred_categories = await self.features.get_user_categories(user_id)
                                                                                                                      for category in preferred_categories[:5]:
                                                                                                                      popular_in_cat = await self.get_popular_in_category(category, limit=100)
                                                                                                                      candidates.update(popular_in_cat)

                                                                                                                      # Strategy 4: Collaborative filtering (viewers like you also watched)
                                                                                                                      cf_candidates = await self.collaborative_filter(user_id, limit=200)
                                                                                                                      candidates.update(cf_candidates)

                                                                                                                      # Strategy 5: Global trending (ensures fresh content)
                                                                                                                      trending = await self.get_trending(limit=100)
                                                                                                                      candidates.update(trending)

                                                                                                                      return list(candidates)

                                                                                                                      async def rank_candidates(
                                                                                                                      self,
                                                                                                                      user_id: str,
                                                                                                                      candidates: List[CandidateStream],
                                                                                                                      context: RecommendationContext,
                                                                                                                      ) -&gt; List[ScoredStream]:
                                                                                                                      &quot;&quot;&quot;
                                                                                                                      Score candidates using deep ranking model.
                                                                                                                      Model predicts: P(user watches for 5+ minutes | stream, context)
                                                                                                                      &quot;&quot;&quot;
                                                                                                                      # Fetch features for all candidates (batch for efficiency)
                                                                                                                      user_features = await self.features.get_user_features(user_id)
                                                                                                                      stream_features = await self.features.get_batch_stream_features(
                                                                                                                      [c.stream_id for c in candidates]
                                                                                                                      )

                                                                                                                      # Build feature vectors
                                                                                                                      feature_vectors = []
                                                                                                                      for candidate in candidates:
                                                                                                                      sf = stream_features[candidate.stream_id]

                                                                                                                      features = {
                                                                                                                      # User-stream affinity
                                                                                                                      &quot;embedding_similarity&quot;: self.cosine_sim(
                                                                                                                      user_features[&quot;embedding&quot;], sf[&quot;embedding&quot;]
                                                                                                                      ),
                                                                                                                      &quot;shared_viewers_count&quot;: sf[&quot;shared_viewers&quot;].get(user_id, 0),
                                                                                                                      &quot;category_match&quot;: int(sf[&quot;category&quot;] in user_features[&quot;preferred_categories&quot;]),
                                                                                                                      &quot;language_match&quot;: int(sf[&quot;language&quot;] == user_features[&quot;language&quot;]),

                                                                                                                      # Stream quality
                                                                                                                      &quot;viewer_count&quot;: sf[&quot;viewer_count&quot;],
                                                                                                                      &quot;viewer_count_log&quot;: math.log1p(sf[&quot;viewer_count&quot;]),
                                                                                                                      &quot;viewer_trend&quot;: sf[&quot;viewer_5min_delta&quot;],
                                                                                                                      &quot;chat_messages_per_minute&quot;: sf[&quot;chat_rate&quot;],
                                                                                                                      &quot;stream_duration_minutes&quot;: sf[&quot;duration_minutes&quot;],

                                                                                                                      # Historical interaction
                                                                                                                      &quot;times_watched_channel&quot;: user_features[&quot;channel_watches&quot;].get(
                                                                                                                      sf[&quot;channel_id&quot;], 0
                                                                                                                      ),
                                                                                                                      &quot;days_since_last_watch&quot;: user_features[&quot;channel_recency&quot;].get(
                                                                                                                      sf[&quot;channel_id&quot;], 999
                                                                                                                      ),
                                                                                                                      &quot;is_following&quot;: int(sf[&quot;channel_id&quot;] in user_features[&quot;follows&quot;]),
                                                                                                                      &quot;is_subscribed&quot;: int(sf[&quot;channel_id&quot;] in user_features[&quot;subs&quot;]),

                                                                                                                      # Context
                                                                                                                      &quot;hour_of_day&quot;: context.hour,
                                                                                                                      &quot;day_of_week&quot;: context.day_of_week,
                                                                                                                      &quot;session_watch_count&quot;: context.streams_watched_this_session,
                                                                                                                      &quot;is_mobile&quot;: int(context.device_type == &quot;mobile&quot;),
                                                                                                                      }

                                                                                                                      feature_vectors.append(features)

                                                                                                                      # Batch prediction
                                                                                                                      scores = await self.ranker.predict_batch(feature_vectors)

                                                                                                                      # Combine candidates with scores
                                                                                                                      scored = [
                                                                                                                      ScoredStream(stream=c, score=s)
                                                                                                                      for c, s in zip(candidates, scores)
                                                                                                                      ]

                                                                                                                      return sorted(scored, key=lambda x: x.score, reverse=True)

                                                                                                                      def ensure_diversity(
                                                                                                                      self,
                                                                                                                      streams: List[ScoredStream],
                                                                                                                      max_same_category: int = 3,
                                                                                                                      max_same_game: int = 2,
                                                                                                                      ) -&gt; List[ScoredStream]:
                                                                                                                      &quot;&quot;&quot;
                                                                                                                      Ensure recommendation diversity using MMR-like approach.
                                                                                                                      Prevent &quot;filter bubble&quot; of same category/game.
                                                                                                                      &quot;&quot;&quot;
                                                                                                                      result = []
                                                                                                                      category_counts = defaultdict(int)
                                                                                                                      game_counts = defaultdict(int)

                                                                                                                      for stream in streams:
                                                                                                                      cat = stream.category
                                                                                                                      game = stream.game_id

                                                                                                                      # Skip if category/game over-represented
                                                                                                                      if category_counts[cat] &gt;= max_same_category:
                                                                                                                      continue
                                                                                                                      if game and game_counts[game] &gt;= max_same_game:
                                                                                                                      continue

                                                                                                                      result.append(stream)
                                                                                                                      category_counts[cat] += 1
                                                                                                                      if game:
                                                                                                                      game_counts[game] += 1

                                                                                                                      return result
                                                                                                                      ```
</code></pre>
<h3 id="real-time-features-pipeline">Real-Time Features Pipeline</h3>
<pre><code>                                                                                                                      ```python
                                                                                                                      class RealTimeFeatureService:
                                                                                                                      &quot;&quot;&quot;
                                                                                                                      Computes and serves real-time features for recommendations.

                                                                                                                      Challenge: Features like &quot;viewer count&quot; change every second.
                                                                                                                      Solution: Streaming computation with windowed aggregations.
                                                                                                                      &quot;&quot;&quot;

                                                                                                                      def __init__(self, kafka_consumer, redis_client, flink_cluster):
                                                                                                                      self.kafka = kafka_consumer
                                                                                                                      self.redis = redis_client
                                                                                                                      self.flink = flink_cluster

                                                                                                                      async def start_feature_computation(self):
                                                                                                                      &quot;&quot;&quot;
                                                                                                                      Flink job for streaming feature computation.

                                                                                                                      Input events:
                                                                                                                      - stream_start, stream_end
                                                                                                                      - viewer_join, viewer_leave
                                                                                                                      - chat_message
                                                                                                                      - follow, subscribe, raid
                                                                                                                      &quot;&quot;&quot;
                                                                                                                      job = self.flink.create_job(&quot;recommendation_features&quot;)

                                                                                                                      # Viewer count: count distinct viewers per stream
                                                                                                                      job.add_operator(
                                                                                                                      name=&quot;viewer_count&quot;,
                                                                                                                      input=&quot;viewer_events&quot;,
                                                                                                                      operation=CountDistinct(
                                                                                                                      key=&quot;stream_id&quot;,
                                                                                                                      value=&quot;user_id&quot;,
                                                                                                                      window=TumblingWindow(seconds=10),
                                                                                                                      ),
                                                                                                                      output=&quot;stream_viewer_counts&quot;
                                                                                                                      )

                                                                                                                      # Viewer trend: compare current count to 5 minutes ago
                                                                                                                      job.add_operator(
                                                                                                                      name=&quot;viewer_trend&quot;,
                                                                                                                      input=&quot;stream_viewer_counts&quot;,
                                                                                                                      operation=WindowDelta(
                                                                                                                      key=&quot;stream_id&quot;,
                                                                                                                      current_window=10,
                                                                                                                      compare_window=300,  # 5 minutes
                                                                                                                      ),
                                                                                                                      output=&quot;stream_viewer_trends&quot;
                                                                                                                      )

                                                                                                                      # Chat rate: messages per minute
                                                                                                                      job.add_operator(
                                                                                                                      name=&quot;chat_rate&quot;,
                                                                                                                      input=&quot;chat_events&quot;,
                                                                                                                      operation=Rate(
                                                                                                                      key=&quot;channel_id&quot;,
                                                                                                                      window=SlidingWindow(minutes=1),
                                                                                                                      ),
                                                                                                                      output=&quot;channel_chat_rates&quot;
                                                                                                                      )

                                                                                                                      # Write to Redis for serving
                                                                                                                      job.add_sink(
                                                                                                                      inputs=[&quot;stream_viewer_counts&quot;, &quot;stream_viewer_trends&quot;, &quot;channel_chat_rates&quot;],
                                                                                                                      sink=RedisSink(
                                                                                                                      self.redis,
                                                                                                                      key_pattern=&quot;features:{stream_id}&quot;,
                                                                                                                      ttl_seconds=30,
                                                                                                                      )
                                                                                                                      )

                                                                                                                      await job.start()
                                                                                                                      ```
</code></pre>
<h3 id="recommendation-interview-questions-3-levels-deep">Recommendation Interview Questions (3 Levels Deep)</h3>
<div>
<h4>Level 1: How does Twitch's recommendation system differ from Netflix's?</h4>
<p><strong>Answer:</strong> The key difference is <span>**real-time inventory**</span>. Netflix has a fixed catalog - recommendations can be precomputed and cached. Twitch's "catalog" changes constantly: streams go live/offline, viewer counts fluctuate, chat activity varies. This means: (1) Recommendations must incorporate real-time signals (who is live now, trending streams). (2) Cannot fully precompute - must generate on-request with fresh data. (3) "Quality" is temporal - a stream that was great 2 hours ago might be boring now. (4) Cold start for new streams - need to surface new content without historical engagement data.</p>
<div>
<h5>Level 2: A new streamer with zero followers goes live. How do you give them a chance to be discovered?</h5>
<p><strong>Answer:</strong> This is the <span>**cold start problem**</span> for content. Strategies: (1) <strong>Exploration injection:</strong> Reserve 5-10% of recommendation slots for exploration. Randomly sample new/low-viewer streams to surface them. (2) <strong>Category-based seeding:</strong> If user watches Minecraft, new Minecraft streamers get boosted in their feed. Category match is a strong signal even without history. (3) <strong>Engagement velocity:</strong> Track early signals like follower rate, chat engagement relative to viewer count. High engagement rate suggests quality even with low absolute numbers. (4) <strong>Raid/host boost:</strong> If established streamer raids a small channel, that's a strong endorsement signal - boost discovery. (5) <strong>New streamer program:</strong> Explicit boost for first N streams or first 30 days. Related: [[machine-learning]](/topic/system-design/ml-systems) exploration-exploitation tradeoffs.</p>
<div>
<h6>Level 3: How do you prevent the recommendation system from creating a "rich get richer" effect where top streamers dominate all recommendations?</h6>
<p><strong>Answer:</strong> <span>**Popularity bias**</span> is real and requires explicit mitigation: (1) <strong>Log-transform viewer counts:</strong> Use log(viewers) instead of raw count as a feature. Difference between 100 and 1000 viewers is more meaningful than 100K vs 101K. (2) <strong>Calibration by category:</strong> A 500-viewer speedrunner is "popular" for that category; normalize by category distribution. (3) <strong>Diversity constraints:</strong> Max 2-3 streams from same popularity tier per page. Force mix of large, medium, small streams. (4) <strong>Personalization over popularity:</strong> Weight user-specific affinity signals (embedding similarity, watch history) more than global popularity. A user who loves niche content should see niche streams. (5) <strong>Position-aware training:</strong> Train ranking model on data that accounts for position bias - clicks on position 1 aren't more "valuable" than position 10. Use techniques like IPW (Inverse Propensity Weighting). (6) <strong>A/B test for ecosystem health:</strong> Track not just click-through rate but metrics like "unique streamers watched per user" and "new streamers reaching 100 concurrent viewers." Optimize for healthy ecosystem, not just engagement. (7) <strong>Editorial curation:</strong> Human-curated "Rising Stars" or "Hidden Gems" sections that surface quality small streamers identified by content team.</p>
</div>
</div>
</div>
<hr />
<h2 id="section-7-scale-phases">Section 7: Scale Phases</h2>
<h3 id="phase-1-starting-phase-mvp">Phase 1: Starting Phase (MVP)</h3>
<div>
<div>
<p><strong>Assumptions:</strong><br />
- <strong>Streamers</strong>: 100 - 1,000 concurrent<br />
- <strong>Viewers</strong>: 10K - 100K concurrent<br />
- <strong>Chat messages</strong>: 10K - 100K/minute<br />
- <strong>Budget</strong>: $10,000 - $50,000/month</p>
<p><strong>Architecture Decision: Use Managed Services</strong></p>
<div>
<table>
<thead>
<tr>
<th>Component</th>
<th>Managed Service</th>
<th>Monthly Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Ingest + Transcode</strong></td>
<td>AWS IVS or Mux</td>
<td>~$2,000-5,000</td>
</tr>
<tr>
<td><strong>Video Delivery</strong></td>
<td>Included with IVS/Mux</td>
<td>-</td>
</tr>
<tr>
<td><strong>Chat</strong></td>
<td>Ably or PubNub</td>
<td>~$500-1,000</td>
</tr>
<tr>
<td><strong>Database</strong></td>
<td>Supabase or PlanetScale</td>
<td>~$100-500</td>
</tr>
<tr>
<td><strong>Hosting</strong></td>
<td>Vercel/Railway</td>
<td>~$100-500</td>
</tr>
</tbody>
</table>
</div>
<pre><code>                                                                                                                          ```python
                                                                                                                          # Phase 1: Simple architecture using managed services
                                                                                                                          class StreamService:
                                                                                                                          def __init__(self, ivs_client, chat_service):
                                                                                                                          self.ivs = ivs_client  # AWS IVS handles ingest + transcode + CDN
                                                                                                                          self.chat = chat_service  # Ably handles WebSocket scale

                                                                                                                          def create_stream(self, channel_id: str):
                                                                                                                          # AWS IVS creates everything - ingest, transcode, CDN
                                                                                                                          channel = self.ivs.create_channel(
                                                                                                                          name=channel_id,
                                                                                                                          type=&quot;STANDARD&quot;,  # Includes transcoding
                                                                                                                          latency_mode=&quot;LOW&quot;,  # 3-5 second latency
                                                                                                                          )

                                                                                                                          return {
                                                                                                                          &quot;ingest_endpoint&quot;: channel.ingest_endpoint,
                                                                                                                          &quot;stream_key&quot;: channel.stream_key,
                                                                                                                          &quot;playback_url&quot;: channel.playback_url,
                                                                                                                          }

                                                                                                                          class ChatService:
                                                                                                                          def __init__(self, ably_client):
                                                                                                                          self.ably = ably_client

                                                                                                                          def send_message(self, channel_id: str, user_id: str, message: str):
                                                                                                                          # Ably handles fan-out, no need to manage WebSocket servers
                                                                                                                          channel = self.ably.channels.get(f&quot;chat:{channel_id}&quot;)
                                                                                                                          channel.publish(&quot;message&quot;, {
                                                                                                                          &quot;user_id&quot;: user_id,
                                                                                                                          &quot;message&quot;: message,
                                                                                                                          &quot;timestamp&quot;: time.time(),
                                                                                                                          })
                                                                                                                          ```
</code></pre>
</div>
</div>
<h3 id="phase-2-growth-phase">Phase 2: Growth Phase</h3>
<div>
<div>
<p><strong>Assumptions:</strong><br />
- <strong>Streamers</strong>: 10K concurrent<br />
- <strong>Viewers</strong>: 1M concurrent<br />
- <strong>Chat messages</strong>: 10M/minute<br />
- <strong>Budget</strong>: $100,000 - $500,000/month</p>
<p><strong>Architecture Decision: Hybrid (Managed + Custom)</strong></p>
<p>At this scale, managed service costs become significant. Start building custom components for highest-cost items while keeping others managed.</p>
<div>
<table>
<thead>
<tr>
<th>Component</th>
<th>Approach</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Ingest</strong></td>
<td>Custom RTMP servers</td>
<td>Control, cost at scale</td>
</tr>
<tr>
<td><strong>Transcode</strong></td>
<td>Custom GPU cluster</td>
<td>Biggest cost driver</td>
</tr>
<tr>
<td><strong>CDN</strong></td>
<td>CloudFront or Fastly</td>
<td>CDN is commodity, don't build</td>
</tr>
<tr>
<td><strong>Chat</strong></td>
<td>Custom Kafka + WebSocket</td>
<td>Scale requires custom</td>
</tr>
<tr>
<td><strong>Recommendations</strong></td>
<td>Custom ML pipeline</td>
<td>Differentiation</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<h3 id="phase-3-twitch-scale">Phase 3: Twitch Scale</h3>
<div>
<div>
<p><strong>Assumptions:</strong><br />
- <strong>Concurrent streamers</strong>: 100K+<br />
- <strong>Concurrent viewers</strong>: 15M+ peak<br />
- <strong>Chat messages</strong>: 100M+/minute<br />
- <strong>Video bandwidth</strong>: 100+ Tbps</p>
<p><strong>Architecture Decision: Fully Custom</strong></p>
<p>At Twitch scale, nearly everything is custom-built for cost and control.</p>
<div>
<table>
<thead>
<tr>
<th>Component</th>
<th>Twitch Approach</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Ingest</strong></td>
<td>50+ global ingest PoPs, custom RTMP extensions</td>
</tr>
<tr>
<td><strong>Transcode</strong></td>
<td>Custom GPU clusters with NVENC, tiered allocation</td>
</tr>
<tr>
<td><strong>CDN</strong></td>
<td>Custom CDN + multi-CDN for redundancy</td>
</tr>
<tr>
<td><strong>Chat</strong></td>
<td>IRC-based protocol, custom fan-out infrastructure</td>
</tr>
<tr>
<td><strong>VOD</strong></td>
<td>Custom tiered storage with S3 + Glacier</td>
</tr>
<tr>
<td><strong>Recommendations</strong></td>
<td>Custom ML platform with real-time features</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Why Custom CDN at This Scale:</strong></p>
<p>At 100+ Tbps, CDN costs are $0.01-0.02/GB. That's $30M+/year in CDN fees. Building custom CDN with owned/leased infrastructure becomes economically viable.</p>
</div>
</div>
<hr />
<h2 id="trade-off-analysis">Trade-off Analysis</h2>
<div>
<h3 id="core-trade-offs-in-live-streaming">Core Trade-offs in Live Streaming</h3>
<table>
<tr>
<th>Trade-off</th>
<th>Option A</th>
<th>Option B</th>
<th>Twitch's Choice</th>
</tr>
<tr>
<td>Latency vs. Stability</td>
<td>Low latency (3-5s), more rebuffering</td>
<td>Higher latency (15-30s), stable</td>
<td>User-selectable, default stable</td>
</tr>
<tr>
<td>Transcode Cost vs. Quality</td>
<td>Transcode all (expensive)</td>
<td>Partners only (cheaper)</td>
<td>Tiered by streamer level</td>
</tr>
<tr>
<td>Chat Consistency vs. Perf</td>
<td>Strong ordering (slower)</td>
<td>Eventually consistent (faster)</td>
<td>Eventually consistent</td>
</tr>
<tr>
<td>Recommendations</td>
<td>Popularity-based (simple)</td>
<td>Personalized (complex)</td>
<td>Personalized with diversity</td>
</tr>
<tr>
<td>VOD Retention</td>
<td>Keep forever (expensive)</td>
<td>Aggressive deletion (cheap)</td>
<td>Tiered by user level</td>
</tr>
</table>
</div>
<hr />
<h2 id="interview-tips">Interview Tips</h2>
<div>
<h3 id="red-flags-in-your-answer">Red Flags in Your Answer</h3>
<div>
<pre><code>                                                                                                                          - **&quot;Use WebRTC for the main broadcast stream&quot;** - Shows lack of understanding of scale requirements
                                                                                                                          - **&quot;Store chat messages in SQL database&quot;** - Missed the real-time/ephemeral nature of chat
                                                                                                                          - **&quot;Build custom CDN from scratch&quot;** - Unless you're at Twitch scale, this is over-engineering
                                                                                                                          - **&quot;Single WebSocket server for all users&quot;** - Doesn't understand connection limits (~65K per server)
                                                                                                                          - **&quot;Transcode to all qualities for every streamer&quot;** - Ignores cost constraints
                                                                                                                          - **&quot;Same architecture for 100 viewers and 100K viewers&quot;** - Missing scale awareness
</code></pre>
</div>
<h3 id="impressive-statements">Impressive Statements</h3>
<div>
<pre><code>                                                                                                                          - **&quot;For &lt; 100 concurrent streamers, I'd use AWS IVS rather than building transcoding infrastructure&quot;** - Shows pragmatism
                                                                                                                          - **&quot;The median stream has very few viewers, so optimize for the common case while handling viral moments&quot;** - Demonstrates workload understanding
                                                                                                                          - **&quot;Chat can be eventually consistent because viewers don't notice 100ms delay between edge servers&quot;** - Shows consistency trade-off awareness
                                                                                                                          - **&quot;HLS latency comes from segment duration + encoding + CDN + buffer - each can be optimized independently&quot;** - Shows deep latency stack knowledge
                                                                                                                          - **&quot;CDN caching converts O(n) bandwidth into O(1) for popular content&quot;** - Demonstrates CDN value understanding
                                                                                                                          - **&quot;Transcoding is where Twitch differentiates - partners get priority access to limited GPU resources&quot;** - Shows business/technical connection
</code></pre>
</div>
<h3 id="30-second-summary-for-interviews">30-Second Summary for Interviews</h3>
<blockquote>
<p>&quot;A streaming platform has five core systems: <strong>Ingest</strong> (receive RTMP from broadcasters), <strong>Transcode</strong> (convert to multiple qualities in real-time using GPUs), <strong>Delivery</strong> (HLS segments via CDN), <strong>Chat</strong> (WebSocket fan-out with Kafka), and <strong>Recommendations</strong> (two-stage ML pipeline with real-time features). The key insight is that video scales via CDN caching - popular streams are served from edge, making cost-per-viewer approach zero. Chat scales via hierarchical fan-out. Start with managed services (AWS IVS) and only build custom when you hit scale limits or cost thresholds.&quot;</p>
</blockquote>
</div>
<hr />
<h2 id="related-topics">Related Topics</h2>
<pre><code>                                                                                                                      - [[cdn]](/topic/system-design/cdn) - Edge caching and delivery networks
                                                                                                                      - [[message-queues]](/topic/system-design/message-queues) - Kafka for chat distribution
                                                                                                                      - [[rate-limiting]](/topic/system-design/rate-limiting) - Chat flood protection
                                                                                                                      - [[caching]](/topic/system-design/caching) - Segment and metadata caching
                                                                                                                      - [[load-balancing]](/topic/system-design/load-balancing) - Ingest server selection
                                                                                                                      - [[storage]](/topic/system-design/storage) - VOD tiered storage patterns
                                                                                                                      - [[concurrency-patterns]](/topic/system-design/concurrency-patterns) - WebSocket server design
                                                                                                                      - [[latency-throughput]](/topic/system-design/latency-throughput) - Streaming latency optimization
</code></pre>
